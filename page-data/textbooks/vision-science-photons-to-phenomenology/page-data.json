{"componentChunkName":"component---src-templates-post-template-tsx","path":"/textbooks/vision-science-photons-to-phenomenology/","result":{"data":{"markdownRemark":{"html":"<blockquote class=\"blockquote\">\n  <p>There is no red or green. All the colors we see are artificial; they're constructs of the mind. This beautiful rainbow of colors you see here on these felt-pens is, in reality, different shades of gray.</p>\n  <p class=\"blockquote-footer\">Peter Cawdron, Anomaly</p>\n</blockquote>\n<h1 id=\"part-i-foundations\" style=\"position:relative;\"><a href=\"#part-i-foundations\" aria-label=\"part i foundations permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Part I: Foundations</h1>\n<h2 id=\"chapter-1-an-introduction-to-vision-science\" style=\"position:relative;\"><a href=\"#chapter-1-an-introduction-to-vision-science\" aria-label=\"chapter 1 an introduction to vision science permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 1: An Introduction to Vision Science</h2>\n<ul>\n<li>Most of us take for granted our ability to see the world around us.</li>\n<li>Yet when viewed critically, visual perception is so incredibly complex that it almost seems like a miracle.</li>\n<li>How do we quickly and effortlessly perceive a meaningful 3D scene from the 2D pattern of light that enters our eyes?</li>\n<li>This is the fundamental question of vision.</li>\n<li>Three domains of vision\n<ul>\n<li>Phenomena of visual perception\n<ul>\n<li>E.g. Colors, shape, size, distance.</li>\n</ul>\n</li>\n<li>Nature of optical information\n<ul>\n<li>E.g. Wavelength, intensity, distance.</li>\n</ul>\n</li>\n<li>Physiology of the visual nervous system\n<ul>\n<li>E.g. Retina, visual cortex, neurons.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Understanding all three domains and their relationships is required to explain vision.</li>\n</ul>\n<p><strong>Section 1.1: Visual Perception</strong></p>\n<ul>\n<li>This chapter covers: the nature of visual perception from an evolutionary perspective, the nature of optical information from surface light reflected into the eyes, and lastly the physiology of the nervous system that underlies our ability to see.</li>\n<li>Although the eyes are important to see, large parts of the brain are just as crucial.</li>\n<li>What we learn here forms the scaffolding for information in later chapters.</li>\n<li>Visual perception: the process of acquiring knowledge about environmental objects and events by extracting information from the light they emit or reflect.</li>\n<li>Notes on this definition\n<ul>\n<li>The acquisition of knowledge is what separates an eye from a camera. Cameras don’t know anything about the scenes they record and don’t act on visual information.</li>\n<li>Perception isn’t just about an observer’s visual experiences but also the objects and events in the environment.</li>\n<li>Optical information is the foundation of all vision and comes from the way physical surfaces interact with light.</li>\n<li>The difference between emitting and reflecting light is that emitted light only comes from a source, while reflected light uses light from an emitted source.</li>\n</ul>\n</li>\n<li>What is visual perception for?</li>\n<li>Vision evolved to aid in the survival and successful reproduction of organisms. And not just seeing, but hearing, touching, tasting, and smelling all participate in this goal.</li>\n<li>Not all species rely as heavily on vision as we do, but vision is important to us because it provides spatially accurate information from a distance.</li>\n<li>E.g. Bats don’t rely heavily on vision but instead use echolocation to gather information.</li>\n<li>Sound and smell can provide information from an even greater distance, but they’re rarely as accurate as vision is for identifying and location objects.</li>\n<li>Taste and touch provide the most direct information about objects but they can’t provide information from far distances.</li>\n<li>For evolution, visual perception is only useful if it’s reasonably accurate otherwise it would have never evolved to such an accurate level in humans.</li>\n<li>Light is an enormously rich source of environmental information and human vision exploits it to a high degree.</li>\n<li>Veridical perception: perception that’s consistent with the actual state of the environment.</li>\n<li>But is vision a clear window into reality?</li>\n<li>No, and exceptions include adaptation, afterimages, visual illusions, ambiguous figures, and hallucinations.</li>\n<li>Adaptation\n<ul>\n<li>Visual experience becomes less intense with prolonged exposure to a stimulus.</li>\n<li>E.g. If you go outside at night, you first only see a few dim stars but after a few minutes, you start to see more stars and brighter stars. The stars don’t emit more light as you continue gazing, but your visual system becomes more sensitive to the light they do emit.</li>\n<li>What changes over time is our visual system and not the environment.</li>\n</ul>\n</li>\n<li>Afterimage\n<ul>\n<li>Strong visual stimuli sometimes leave an impression after the stimuli has disappeared.</li>\n<li>E.g. A blinding camera flash is followed by a dark spot afterimage where the flash retinally was.</li>\n<li>This is clearly not veridical perception because the afterimage lingers long after the physical flash is gone.</li>\n</ul>\n</li>\n<li>Illusions\n<ul>\n<li>Stimuli that fool our visual system into making perceptual errors.</li>\n<li>Visual illusions support the fact that perception is fallible and can’t be considered a clear window into reality.</li>\n<li>The reality that vision provides (perception) must be a construction by the visual system from the way it processes light information.</li>\n<li>The nature of this construction implies certain hidden assumptions, assumptions that we aren’t conscious of and when untrue result in illusions.</li>\n<li>It’s easy to get carried away by illusions and claim that vision is grossly inaccurate and unreliable, but this is a mistake.</li>\n<li>Vision is only as accurate as it needs to be for evolutionary purposes.</li>\n<li>Later we’ll consider the possibility that the perceptual errors produced by illusions may actually be harmless side effects of the same processes that produce veridical perception under normal circumstances.</li>\n<li>The point is that illusions prove that perception isn’t just a simple registration of objective reality.</li>\n</ul>\n</li>\n<li>Vision must be an interpretive process that somehow transforms complex, moving, 2D patterns of light at the backs of our eyes into stable perceptions of 3D objects in 3D space.</li>\n<li>We conclude that the objects we perceive are actually interpretations based on the structure of images rather than direct registrations of physical reality.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 668px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/50b2895001b12ac9918217b9d4860137/74866/figure1-1-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 126%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAIAAAC+dZmEAAAACXBIWXMAAAsTAAALEwEAmpwYAAADr0lEQVR42lVU32sdRRTev0EkPktf7Zv61BJslSKlWpA+2JciphRBoZQ+5MVCMAaTtOmNSe7uzv7+Mbs7OzM7s5ubmyvBgv5lfjO7uerlMMydOd853/nOmXW0FqMpLZbLxf7Bb1tbTxYXwy+7Oy9ePL+6Wh0c7j9+/O3Ll88vV4tOca2lsgaIo5RUSsC6jq9Wy1evfv5g4728SD795OP797/86+93e7/u3vvi3tb3361WF5wz+Hdwtuasd1Lyi+ViZ2dnY+P92fHbmzc/evTomz/fXfm+e3iwH4Z+PyibSUrFZccN+DqS7DpxuVpub2/fuPFhw+pbt24/fPj16o/Lzc3bP/34w53PNoGfwB3ALVZnQpp48vx8CALy+s2h1t3Jycnb2dFwPnz14MGzZ08/v3uH0vz/4NaRnZAmLfAd46xhDa0oCfy8SKu6Uhp3QgiOn+67f2vsuJDtlFmpbgwJP4Soa8oQp2WIqHvVD33fQ2K+BndrsCGjDbjvO8bqNItZW1NaSimQreVNlqUNAwv538yGtq0WzM0RruEEzlmeEuK53pwLRquSBCSKo7ZthGg7NRU8gdfxtMYNb5oa1Ra0oCDfNoy3URKXtBCSS9mukSNYrK3vFZo0d90kTaIkycuSIk7L8iJHLXBAZoyDQcrW1CynAvjUM62KIk8yIHMSEgNuGtf3fN+DHKytQFtYpJDMwWyYYNKAcU3BtiqDkCRpHEUhZBOdRPOquhipjkhowUXjjIOJeqA+hK3R5rqM4qAosiSJytKUKmzo64QjElY70yu5lg17zPBiMQyD7gcNXuOh3fB1ZnA2mdFJa+aJIQSOqpoiv6VAgczyZDY7CgIff5FwzdlmBtI+TgsWaO/p6e95nsZxGIYEIQA+PTuJUQIt0TmUZq1iLXXG2VJq/CRICJblMQDwKMq8aetxbNIsgRYtZzDMH5AWPI6nkno0baoQYGX8mv5cgcLe3u6bo9eue4YpsOBqAhthemV10gA0DS5q+BHf8i+SwXwDBGAo1WY2nEdzyjKrrRIoDytaDZ6omZZFmhqqjJlwwOC8birIVtV53RQwB0mOj2dGnigoaZ6mMb47kCpOQs+fo+FhRMZNkkbwwfxEEQEqCH0H3hDDJx5GIk0j358bZBz6xA0jrP7Z/BQtwCE8obkVLyaBRwLXgR9iIBKugY+MUzK3gbHClRDfJvSxt6sHIkYUgEHG81yEyLIEIkWGVRBEAHlgWFUlnrEdG1Mtxp5WeVGmMFrl/wCwp7j1r3X3nQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 1.1.5\"\n        title=\"\"\n        src=\"/CR4-DL/static/50b2895001b12ac9918217b9d4860137/74866/figure1-1-5.png\"\n        srcset=\"/CR4-DL/static/50b2895001b12ac9918217b9d4860137/63868/figure1-1-5.png 250w,\n/CR4-DL/static/50b2895001b12ac9918217b9d4860137/0b533/figure1-1-5.png 500w,\n/CR4-DL/static/50b2895001b12ac9918217b9d4860137/74866/figure1-1-5.png 668w\"\n        sizes=\"(max-width: 668px) 100vw, 668px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Two facts about ambiguous figures\n<ul>\n<li>The two interpretations are mutually exclusive and can’t occur simultaneously; only one model fits the sensory data at any one time.</li>\n<li>Both interpretations form a multistable perception where both possibilities switch back and forth as you continue looking at them. It seems that both interpretations compete where the winner is perceived until it gets “tired”.</li>\n</ul>\n</li>\n<li>If perception were a true reflection of reality, then no ambiguous figures could exist.</li>\n<li>This suggests that something more complex and creative is occurring in vision beyond the information just in light.</li>\n<li>Our current hypothesis is that the brain constructs a model of what environmental situation might have produced the observed pattern of sensory stimulation.</li>\n<li>Perception matches the constructed model rather than the actual sensory stimulation, and sometimes these models are wrong or that multiple models are equally plausible.</li>\n<li>This isn’t to say though that the models are made-up or are fiction.</li>\n<li>Perceptual models must be closely coupled to the sensory information of the world and provide reasonably accurate interpretations of this information to be evolutionary useful.</li>\n<li>Illusions show us that our models are sometimes inaccurate and ambiguous figures show us that our models are sometimes identical, but these are under rare and artificial circumstances.</li>\n<li>Our everyday experience tells us that our perceptual models are usually both accurate and unique.</li>\n<li>Another piece of evidence for the model-constructive view of visual perception is that we complete surfaces we can’t see.</li>\n<li>E.g. If one object partially blocks another object, we understand that the blocked object is still complete and whole.</li>\n<li>Almost nothing we see is visible in its entirety, yet almost everything is perceived as whole and complete.</li>\n<li>Self-occluded surfaces: surfaces of an object that are entirely hidden from view by its own visible surfaces.</li>\n<li>E.g. You can’t see the back of a person if you’re seeing their face.</li>\n<li>Impossible objects: 2D line drawings that seem like 3D objects but are physically impossible.</li>\n<li>The existence of impossible objects further show how our perceptions are internal constructions of a hypothesized external reality.</li>\n<li>Why did perception evolve to form models?</li>\n<li>At some level, the answer must be that the models were more useful than the raw images for evolution.</li>\n<li>E.g. Models help to predict the future and are invariant to viewpoint changes such as movement and lighting.</li>\n<li>Models take more time and effort to construct but once built are easier to maintain and use.</li>\n<li>E.g. It’s useful to know the current position of a moving object, but it’s more useful to know its current direction and speed to predict its trajectory.</li>\n<li>An important note is that the process of extrapolation must work faster than the predicted event to be useful, which may be why perceptual predictions are generated so quickly.</li>\n<li>Classifying objects is useful because it allows us to respond in appropriate ways given the vast amount of information we have stored from previous experiences with similar objects.</li>\n<li>Classification makes handling new objects easier because objects in the same class share many properties and behaviors.</li>\n<li>E.g. Things that are alive move and behave differently than things that are dead.</li>\n<li>However, classifying objects is difficult and an unsolved problem.</li>\n<li>There are also cognitive constraints, top-down goals, plans, and expectations that influence perception.</li>\n<li>This makes sense because the purpose of perception is to meet the needs of the organism using the opportunities afforded by the environment.</li>\n<li>When an object has your attention, you become conscious of its detailed properties.</li>\n<li>But attending to an object doesn’t have to mean moving your eyes to fixate on it, even though it usually means that.</li>\n<li>E.g. Staring at something but thinking about something else or not looking at something but thinking about it.</li>\n<li>Many high-level aspects of perception seem to be fully conscious whereas most lower levels don’t seem to be accessible, modifiable, or conscious.</li>\n</ul>\n<p><strong>Section 1.2: Optical Information</strong></p>\n<ul>\n<li>Visual perception depends on three ingredients: light, surfaces that reflect light, and the visual system of an observer that can detect light.</li>\n<li>Without any of these ingredients, visual perception doesn’t occur.</li>\n<li>We study each of these ingredients for their basic facts. This section covers how light interacts with surfaces.</li>\n<li>Light that enters our eyes must somehow carry information about the environment, information that we can use for survival and reproduction.</li>\n<li>Optics: the branch of physics that studies light.</li>\n<li>Luminance: the number of photons falling on a given surface per unit of time.</li>\n<li>The luminance of light covaries to some degree with its perceived brightness, but the relation is far from simple.</li>\n<li>Illumination: the lighting conditions in the environment.</li>\n<li>E.g. A point-source illuminant like the Sun or a diffuse-source illuminant like the Sun over some clouds.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 801px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/6fe849eec13b9e20f59880bbb71480ff/2ad15/figure1-2-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 85.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAACd0lEQVR42i2TWW7cMBBEdc74FI7hT8MO7DhfMQIjGc9oo1aukkgtM87x8jgKQVBcurqrq1uJtbrrO23UMDjLwWhjlLVGyn6e/TwH60wcVvd9k+enw+H3+/uvXvbjOCTOWal6TM+XLY7zum3L+byC92Fa1qVpmsPhz8fxcMqOx9NHlqdpdsqLfABMsGkamraxzi7rvCzzzFxnrVUInpu2a0VV5kVWirIoyyzPhShEJSLYGO39CFJpBWcfQpjBL7APc/B+4mmcJl4NWbnBDXGCnPwUI4/TQJ4gMdrBywptC4swe2MNdprPjh1BjqzjNCYuvo2kB9t1W+G5smwrAa4phGF0l8tlO2+Xz0vUJW7OGOMoido6GHqlJUqgEHF88BCBNhPCrDglbVScI51AWKglWkspO+Ig4O3tbVVV9jq4JYXJ+6Is0Kxu6oeHh7e3nxSpbqlAjVgxsg/QHinY17u7p29Pn38hucIqChY81jDHNU/PL89VUxmHQhIhAffj5KCB0f39/ZebG1K9gi1gSHJvB1sK8fL95fHpsW5rZWiMPuasdYfa5Iw2WZ69/njFEZLgGCQSEERb40ZHTFIgrLZaKon8MTJNhtrMdVvQFS/Ig06At20FLLXCmlDMvR3icaedpidiMkmMNqKf0jSVShEWUl3fw1mIah8ox1oUZcwZzd1VAH4P7JTSNHrTtuhMtZ1zpEdX8ta2UfN9Qztzn5RlKkTZy/hj4UzHH+h/J7G99lPsxH2lPPsNpSWpJE2PRZFTyVN6YoUhFKpa9Awpq7raj1Ul8jzjiDGTGxgkXVcTlilithHZdZFXJNZ3VxfxCke4Zk9doLC3wD9AGplztGVTsQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 1.2.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/6fe849eec13b9e20f59880bbb71480ff/2ad15/figure1-2-1.png\"\n        srcset=\"/CR4-DL/static/6fe849eec13b9e20f59880bbb71480ff/63868/figure1-2-1.png 250w,\n/CR4-DL/static/6fe849eec13b9e20f59880bbb71480ff/0b533/figure1-2-1.png 500w,\n/CR4-DL/static/6fe849eec13b9e20f59880bbb71480ff/2ad15/figure1-2-1.png 801w\"\n        sizes=\"(max-width: 801px) 100vw, 801px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Having more light sources complicates the optical structure of the environment.</li>\n<li>Three ways light can interact with surfaces\n<ul>\n<li>Transmission</li>\n<li>Absorption</li>\n<li>Reflection</li>\n</ul>\n</li>\n<li>These photon-surface interactions are what ultimately provide vision with information about surfaces, except for transparent surfaces.</li>\n<li>Of the three types of interactions, reflection is the most important for vision because reflected light is changed by its interaction with the surface and it can be captured by the eye.</li>\n<li>Review of specular (mirror-like) and matte surfaces. Real surfaces fall somewhere between the two.</li>\n<li>Some light bounces in almost every direction off almost every surface in the environment, so every surface acts as a secondary light source.</li>\n<li>We can go further and introduce third-order and higher-order reflections.</li>\n<li>The richness of natural scenes depends on capturing these higher-order reflections, these complex interactions of ambient light with the structure of the physical environment.</li>\n<li>Ambient optical array (AOA): all the light coming towards a given point of observation.</li>\n<li>The light in the AOA is complex but follows laws that provide information vision can exploit.</li>\n<li>Vision is the process of reverse inference, of determining the arrangement of surfaces that must exist in the environment to have structured the AOA in just that way.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 978px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/6fd6a74a231bafd8ac0c9f7d2b39c710/914c7/figure1-2-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 91.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsTAAALEwEAmpwYAAACXUlEQVR42k2TiY6jQAxE+fdZ7RyrSTgDhLs5+oKQ7Bfu6yYZrWShhna5ymUTKCO1UcZqsxrrwhpr/Bcl1bzd7DSLLE+neXz8vd8fO7Hfb9ttve1roIxelAQAzIWljCKUlkL0vA2iT7IYsIPt2/1+I0BS14E1TNbAvG4OT9W2a67XctssecCiJBTj4DHb8QS8bibQDmYPWsDU46Isi6q+UkLKGQlNW5PqMS6oTtp28+Cn4CetY4aNnudlWuRsVw3+BbCHYJDrpoOjWyfbmoOZpyu/2a5v8IxsXndH6K58DhBNBPYlG3BVV5f8UjcVhP3QRulZjD3gRU70PE2CWuhnKG46Vh7MDs8hjqO8uJDB3bwcPvXWGxnFYVEWvoXFy2QiS+CtZsjoWd9+vYVRSKt8oe30ksB5nN8/fp/OJ6kWpRYNUi8Eo1LaaTAY9n36zi4Z4vuhI2+cBJrtZq5V+fn18fXns+1qGjY/4Cfz03MUumBJsAdadILfH3htDqvJ8Su0KD0H+j8kVUiiW22kmPo4jQbRAeA7FqLloPEr6MFOttE/06qbOk2TQQwsRhifAZOKfhrph36cRmauDmb1Ah/kZLx/vLNeMAAI4xMDk0oiHsNwBNuxE8OUeYJff5U1TdskWVIUOfyQs6HAuOUQhucwOkOutfvhnsz0/NM2JIQYxTgOKKJPTNrvG5Nv2qrrGq0l/49ndoYHVVN1QycmQQzApnEcxTSNMAPGOQ5IZdQ0QmY3tG3Xdn07DF0Qp0mSpXmZF9fiUuR5kZfXkmCfyCZy3wVGFKW7IifNUtahLPN/sI65GqfYTyIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 1.2.5\"\n        title=\"\"\n        src=\"/CR4-DL/static/6fd6a74a231bafd8ac0c9f7d2b39c710/914c7/figure1-2-5.png\"\n        srcset=\"/CR4-DL/static/6fd6a74a231bafd8ac0c9f7d2b39c710/63868/figure1-2-5.png 250w,\n/CR4-DL/static/6fd6a74a231bafd8ac0c9f7d2b39c710/0b533/figure1-2-5.png 500w,\n/CR4-DL/static/6fd6a74a231bafd8ac0c9f7d2b39c710/914c7/figure1-2-5.png 978w\"\n        sizes=\"(max-width: 978px) 100vw, 978px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The eyes only sample a directional (front-facing) subset of the AOA and there is a different AOA at every point in the environment.</li>\n<li>If we move, we extend the static AOA into a dynamic AOA.</li>\n<li>The static AOA is characterized by a pattern of light converging at the observation point, while the dynamic AOA is characterized by the optic flow of light over time.</li>\n<li>The dynamic AOA is important because it provides more information and allows us to perceive depth, the shapes of moving objects, and our own trajectory as we move.</li>\n<li>If vision provides accurate information about the external world, then there must be a consistent relationship (laws) between the geometry of environmental surfaces and the light that enters the eye at a specific observation point. And there are such laws.</li>\n<li>Distal stimulus: the object in the external world.</li>\n<li>Proximal stimulus: the object’s optical image at the back of the eye.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 670px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/9a93a853b3e59c68f2a372a6c4d1434a/d67fd/figure1-2-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 120%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAIAAAB1KUohAAAACXBIWXMAAAsTAAALEwEAmpwYAAADJklEQVR42j2U6XbaMBCF/djpvzSnbaBtQjCQYLzJtrzJ+wLt4/WTRHOYY2RZd+bqzpWc221dr8uyztM8LuuSZmlVV8PYj9MwzuM46ejHXsfQ2ej61oZjkUReyHkB0ctcqkYRbdd2g4XxHIZpHMbBJur4Da1jkdcbf9O6Lut1bdoG2LzMLG37run0KwOCXKSYDCPw98qUpSCEp3mCGOQvgR+EflkVTddWqv4MkYjTx7tIE7hocNs1u92OWdfdx3EcxeHp/fT49PS8eb74F6opVZcA2wbA9tfPx6evh9MR8g5sKZUXOVFWJZVVUzP48M6kkHlmBNMbhkKt1Nk7v+3fgjAgqa7ctCoWsXe5ICBb0PIMWhK9VaNZKrP9wU2zrDGbhwJEGDhIhQB+4Hueh1S3P1croQ2IzOsSJ2LvuiJJGGvN/8ddMDDojLwUNA0HuSA4YGIkpsmqzdgg9UYcYFVdJqkAhjGQgCf8SUE6KSX7180zSAjXjbr33IJZN03Dn783U39ZDBdaQHG9574ji6YAlesKEvwdfL0u2Ga9zqYf5eHoer5Hup5SOKRtIJ+XhXs8oLAsJJ6h21DQ4Nmsgy2N+Th/fP/xDTwzhDUprcIwD18e3IO72WzwD7lscQcBqGA1Y7eZzKgGT7ql9etoHr1ssBDtmRZNXhkJtMO0mNPA3qAKnoCF7TCEbVfBIwSvVNKaqXowztFgvoHkPNV1RXRmt7ZDk8FjezTH+ZWq8Emvz5YO7KlbisiZTA8H11hSmnSTVbgbhu12+/vl5XX3ijfNTH8HszckgWqtKtpDh/G2RX46id0S0pjfErZ3g0Op4+nI0fMuZzgXZY5hkC1JE85dlstIRJwZkJg0jELUxuoMODOOXpQI/VlmIokZ87koiiiKMHMUx0EURnEkROyHQch5FTERRSFPDU6zhG88i7KkJkvNJHXTSMSfgCS1MxErYwAWjEKw5TKxZTPDiidULS8/COCvsyQCn4VhwIWRJMKBep5LprIslaasTQHMZiGY8fwLB56CLKAysnNJOQB4MWVjuoVavNrLwJ5Q7MUrTzqsWmXvIwSuVfUPSYu0RS0GaSUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 1.2.6\"\n        title=\"\"\n        src=\"/CR4-DL/static/9a93a853b3e59c68f2a372a6c4d1434a/d67fd/figure1-2-6.png\"\n        srcset=\"/CR4-DL/static/9a93a853b3e59c68f2a372a6c4d1434a/63868/figure1-2-6.png 250w,\n/CR4-DL/static/9a93a853b3e59c68f2a372a6c4d1434a/0b533/figure1-2-6.png 500w,\n/CR4-DL/static/9a93a853b3e59c68f2a372a6c4d1434a/d67fd/figure1-2-6.png 670w\"\n        sizes=\"(max-width: 670px) 100vw, 670px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Visual angle: the number of degrees occupied by the proximal stimulus.</li>\n<li>It’s important to know that the image on the retina has only two spatial dimensions rather than three, so vital spatial information has been lost going from the real 3D world to the eye’s 2D image of it.</li>\n<li>E.g. Depth information is lost from the projection of the 3D world onto a 2D surface.</li>\n<li>To perceive the world in three dimensions, we must somehow recover depth information from the 2D image. This may be possible, however, because the projection from a 3D to 2D image is highly lawful and consistent.</li>\n<li>Projective geometry: the mathematical study of how a higher-dimensional space is mapped onto a lower-dimensional space.</li>\n<li>E.g. In dynamic vision, the projection is from the 4D structure of space-time onto the 3D space of optic flow that unfolds over time on the 2D surface at the retina.</li>\n<li>No notes on pinhole camera, perspective projection, convex lens.</li>\n<li>One issue with using a pinhole camera is that too large of an aperture makes the image blurry. This can be solved by using a convex lens that provides a virtual pinhole at its focal point, which is what our eyes do.</li>\n<li>No notes on perspective versus orthographic projection. Orthographic projection can be thought of as a special case of perspective projection when the distance is infinite between object and focal point.</li>\n<li>The early stages of visual perception try to solve the inverse problem.</li>\n<li>Inverse problem: how to go from optical images of a scene to knowledge of the objects that gave rise to them.</li>\n<li>There’s no easy way to solve the inverse problem because going from the 2D image to the 3D environment has many possible solutions.</li>\n<li>Each point in the image could map into an infinite number of points in the environment.</li>\n<li>Sensory data doesn’t provide enough information to obtain a specific solution, so it’s surprising how our brains manage to come up with the correct solution most of the time.</li>\n<li>We know 3D perception is possible because the human visual system can do it.</li>\n<li>The dominant theory explaining our 3D perception ability argues that the ability is possible because the visual system makes a lot of highly plausible assumptions about the nature and conditions of the environment.</li>\n<li>These assumptions constrain the inverse problem enough to make it solvable most of the time.</li>\n<li>Thus, vision is a heuristic process.</li>\n</ul>\n<p><strong>Section 1.3: Visual Systems</strong></p>\n<ul>\n<li>Once we’ve mastered some of the basic facts about the hardware of the visual system, it’s anatomy or physical structure, we can then turn to what it does, its physiological or biological function.</li>\n<li>Vision occurs when light from external sources is reflected from surfaces of objects and enters the eye.</li>\n<li>The visual system consists of both eyes and the brain areas that process information from the eyes.</li>\n<li>The fact that both eye and brain are required for vision means that a person with normal eyes but damaged visual brain might be as blind as a person with a normal brain but damaged eyes.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/c76f8a1a190f784772248293e6fa115a/cdef6/figure1-3-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAACh0lEQVR42k1T204aURSdf7H2zfTVvreNiS9FTUpKLMJDvTzUpIomWhEQAQERTWGY+wyX4cy5zBnwB7sOWNvMCpk57LXXmrX3aBGdApRFjFMumIh5MPRb7WapdF2t3rTbzX6/Nxz6s5mcPyfzuQRwnyQC0CIaUUaXzFgK3/ca9RoI3cduq9W8u2tUKmV0cRxbJmI2l8lsgURIybUlk3GlCX1U12q39Xrt4aHjuo7t2L3ebxze37dtx5rNkwU5RqM4ZtqSuSCL8WQEtXK59GZlZWdn2/M9y7b8wPd8t9lqomMYjmFbxko2jqn2V1YpT0n4bXf3/fp6Ov3l+PiHaRqO63Q697reHwx0mH96eoQsKuP4H5mCzHg0CcfpdPrd2lr1tlq8ujorFA4O9lOfP6dSqXwuVzg96XY7qIwlFzEVItJe0wJADgIfOo16vVIu5/P5/f3vCI9ztreX3d7aajRqCyYTgnJOtP+jhm3TMjCqbvchl8ttpVKHhwe/Li+LxeLm5ubb1dWTk5/z5xk8coWpGtXrhEEOpxNKiWEMbm4q8JzNZjOZr5lM5tOHjxsbG0hBSqHqOWEsfHnnJTmihERT17WjaBpLhoQKhdPz87PLi4ujw6NSqQhB5LwgTxWZqCV5EWeMQhmuEOZwFDiOdX1dxGuXyiWMsK/3KCNcUNSjO4nGsE2oYi7FGRPgj/EIIxgpGmFPLcvECbYAviCzMBgSMn61jQwVsDpogb/VDkmOffQ8dzIZ4VEmMUJdDBUhE0pDzbBML/CHoyEQ4BoFo7G6wXoREiJCrBc+jOUJ4AcKnue4rqX19b5hGvgdGAMso4nLMpGqPtAnyifIXqDIvuL4rguaYtqua/0BgxYdb1uFjIkAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 1.3.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/c76f8a1a190f784772248293e6fa115a/00d43/figure1-3-1.png\"\n        srcset=\"/CR4-DL/static/c76f8a1a190f784772248293e6fa115a/63868/figure1-3-1.png 250w,\n/CR4-DL/static/c76f8a1a190f784772248293e6fa115a/0b533/figure1-3-1.png 500w,\n/CR4-DL/static/c76f8a1a190f784772248293e6fa115a/00d43/figure1-3-1.png 1000w,\n/CR4-DL/static/c76f8a1a190f784772248293e6fa115a/cdef6/figure1-3-1.png 1163w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Blindsight: blindness due to damage to parts of the visual cortex.</li>\n<li>E.g. Blind spots in the visual field, describing faces but unable to identify them, or seeing stationary scenes but can’t perceive motion.</li>\n<li>We shouldn’t take for granted the simple facts about our eyes.</li>\n<li>E.g. Some species can’t move their eyes but must instead move their entire head. Some species have their eyes placed on the sides rather than the front.</li>\n<li>Eye placement reflects the tradeoff between accuracy of depth perception and coverage of the visual world.</li>\n<li>E.g. Eyes placed in the front have excellent depth perception but poor coverage, while eyes placed on the sides have poor depth perception but excellent coverage.</li>\n<li>No notes on eye anatomy such as cornea, aqueous humor, pupil, and iris.</li>\n<li>Interestingly, pupil size also changes in response to psychological factors.</li>\n<li>E.g. Positive emotional reactions and mental effort.</li>\n<li>We have no conscious control over the mechanisms that control pupil size.</li>\n<li>After the optics of the eye have done their job, the next important function of the eye is to convert light into neural activity.</li>\n<li>Review of neuron, dendrite, graded potential, cell body, action potential, axon, neurotransmitter, synapse, photoreceptor, rod, cone, horizontal, bipolar, amacrine, ganglion cells, and fovea.</li>\n<li>Our ability to adapt to dark environments doesn’t smoothly increase over time as cones and rods have different adaption curves.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 815px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/2825d161da07590dbbca26d65539f41b/ef916/figure1-3-12.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 83.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAACaklEQVR42j2TiXLbIBRF9efNJG0S243taAOhBS1IQrvT9AN7kGaaecMQwV2479kbpjEr8kIX19v10/fLqsp1keZZmmVlVc7z5Gpxta7zMPRSxlWl53lclskb5lFICR6WWApKJpKKRVwU+TSN7t5+FXDXGWPqpqk4ejxWb5wnPwwSlbDJ8lwmiR2HaZm3r217bOu2rCzbcoDb1nRd25g6TRMHPgRRBvN+eq+a+gBXdVXVdd3UvIh9b3u4OqBdW5ZaSMG/DixVoquSzc/XX6fL2Q8CbqssDaMwisLr/RaEAT4pUgBec6qS7VDGaqF1P1hyIrbb/fb69oadYRwoO1A9GJAUym3bQMRzHDiKoyMwNMu6Ml1rWnP7vJ/OJ5KHdNnWx5+v77/fXd/ZwUIBZLe9B4a4LsuP68fzy/OPpyeQTWswguGP2xU7fuAD0KVOaWKekcWyLh7Ene3r1iRKcUwSIpFBFEQiro1LK4wjlaZ07vL7win917pYV1o/eq3tqW6whIxhcoYLt+RTm6YoNY/nO4Yb0xABT53p+zRO8+TZaXRFMMQz0aURJA9BLUmV2KcFPP4BL06QaZspZDxaAj05ETgb1FqixYvtEdz3HWtvLa12UR/l/ox3ulzez+dQ8FxKoMYjozimxYREkSX6NIJxIEJWl4uUPN7DklAJsGP1GQwRQxFEIf1jyA/zFB75yErBCJGHjkxVzI9BKYgi8swzZsvdThV74Y4S16Eihxc6QoERCqccRBGySeqI2YCUuxS3xa6sdiS9hELvE8JPKC9yb+8qbQ3jvcNYwIu/22MEXFSDPWbh2NOU/x//ARkYeOrTPdI4AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 1.3.12\"\n        title=\"\"\n        src=\"/CR4-DL/static/2825d161da07590dbbca26d65539f41b/ef916/figure1-3-12.png\"\n        srcset=\"/CR4-DL/static/2825d161da07590dbbca26d65539f41b/63868/figure1-3-12.png 250w,\n/CR4-DL/static/2825d161da07590dbbca26d65539f41b/0b533/figure1-3-12.png 500w,\n/CR4-DL/static/2825d161da07590dbbca26d65539f41b/ef916/figure1-3-12.png 815w\"\n        sizes=\"(max-width: 815px) 100vw, 815px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Peculiarities of retinal design\n<ul>\n<li>The photoreceptive layer is the last layer of the retina but it would make more sense for it to be the first layer since it’s the one capturing light.</li>\n<li>Rods and cones appear to be pointing backwards with their light-sensitive segment the furthest from the incoming light. This might be because the enzymes for pigment regeneration are opaque and can’t be in front of the photoreceptors.</li>\n<li>Blood vessels block incoming light but our brain adapts to and ignores them.</li>\n<li>The optic disk or blind spot is where all of the ganglion cell axons leave the eye and it has no photoreceptors. We don’t experience blindness here except under special circumstances.</li>\n<li>The photoreceptors of vertebrates actually respond to light by decreasing synaptic activity. Invertebrate eyes work in the opposite and more intuitive way by increasing their output for increases in light intensity.</li>\n</ul>\n</li>\n<li>Review of the optic chiasm (where the fibers from the nasal side cross), superior colliculus, lateral geniculate nucleus (LGN), occipital cortex, and localization.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 911px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/9017f3813f75e5e854199252f919107d/636c2/figure1-3-15.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 110.80000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAIAAABPIytRAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADsklEQVR42lVU604aWxid5+l5gzYx8cepwcRIaFX0h3qUWE8wgje8AbUoVAHFyt0ZYIC5z+y5j3hqH+6smbEmJV/IZthrr/Xttb6hPM9Gvbz812jUP3x4nznO/Hx59jxnMnmaTDwsHNe2HduyTdNCGYapE4MEpVOuazmO9evXz6Wlxb/evVtZWbEdy7KMTretEW3y7Lmu7bg4wsERJv4wTd00/DIMSiMK02Nohvn498eZmZnV1dVH+rFWuzs9O+31GPADEyIt2wrBb3jKsg1REgbsIBqNTk9PR6PzN+UbmqEbzYYg8IHs1wrAPgyCw6IMQ/OenC7dTafTxVLx+/X3VCpVrpSLxaIsy6B5g4XfIS0xjABsEtezW+1WKp06ODwAcnd3d2tr66H+AH6NqK7nGNitEyh/a/s3cwBmGDqRSJydnaH/0XhUb9R1Q1dUudlq3tXuQO56Lp7glD/Auq7KigQnSqVSPp/P5XJgHo7YyfNTf9A/yhxtJjZPTk4I0UCuairRdV95KBtgUeKJrkESx3PpvfTi4iJko2HsRjvZXDYej5+engiioKiKrCgKjgiZIcF2zKBsWZFxYWtra7FPsY3Nf6Ail8+BORKJHB8f8zzXaDRG3FiSJRVCQmZVleE2yNkhi0u6Kl4lEpsw/PDwoFAonJ+fY72xsXH/4x5C2p22JMu6aWo68cGapmiaitxBNgwXJXFvfw9IXuARAfyFh/v7+3Nzc9lsFmDUYMgCTxGiDIcDXhhDNmC9fg/kOzs7mczR7d1tpVJpNptXxcvCZaF2X/uy/QUp6na71dsqCP2Q6LqGxGMAcO2hVcgmPB+iC/rx4eEHrqpardI0jfAtLCzgFDQPfko34IGqmwR45KHVavX7PY4bl8s33wrfoAJRxw1ha6fbAQzzs7a+BguQX8oIfFY1xbR0hAF3BhKQo+Hrm2t4nkwmL68uoRNpQX5xc/PR+fX19YuLC98qzIY/hhgbx0aSwFCpVkBVr9fReXInORuJhJOHSE9NTcXjS/mvedQrGEgUAogZRpjADx4Qwh6cEovFOI5zPQ/6P3/+NDs7i9jQPeYPMCKlBTEMBsDE1u1/t5eXl0ulIjoKJ1klpN3tDMcj32fL0hFsyLYdP/2o3y8N03ty4VwjGBKEGXmQFNmPJ9FkVRVlicIMCCIPEmQT0cEiCLAsiCIkYI0F7o9lWQSjxw56MKPfZ/r9ActSvMDhjYFsCX6eRFiKD36OuXE4BhzP4yE41VdCrBRBlgRJ/B+PvhwP5P0kDQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 1.3.15\"\n        title=\"\"\n        src=\"/CR4-DL/static/9017f3813f75e5e854199252f919107d/636c2/figure1-3-15.png\"\n        srcset=\"/CR4-DL/static/9017f3813f75e5e854199252f919107d/63868/figure1-3-15.png 250w,\n/CR4-DL/static/9017f3813f75e5e854199252f919107d/0b533/figure1-3-15.png 500w,\n/CR4-DL/static/9017f3813f75e5e854199252f919107d/636c2/figure1-3-15.png 911w\"\n        sizes=\"(max-width: 911px) 100vw, 911px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>We still don’t know the extent to which functions are localized, but the evidence increasingly supports the view that it is.</li>\n<li>The mapping from retina to striate cortex is topographical.</li>\n<li>E.g. Nearby regions on the retina project to nearby regions in the striate cortex.</li>\n<li>Cortical magnification factor: how the fovea receives a much greater representation in the cortex than the periphery.</li>\n<li>This factor means that we have more detailed spatial information about objects in the central region of the retina, and not that the perception of space is distorted so that objects in the center appear bigger.</li>\n<li>A brain area is marked as “visual” if it follows the topographic organization of the retina.</li>\n<li>The visual cortex is a patchwork quilt of small maps that code for different aspects of retinal stimulation.</li>\n<li>E.g. Brightness, color, motion, depth, texture, and form.</li>\n<li>Review of the ventral “what” visual pathway and dorsal “where” visual pathway.</li>\n<li>Visual agnosia: deficits in identifying objects by sight.</li>\n<li>E.g. Some patients can’t recognize a person from their looks, but can recognize them by their voice. This isn’t due to the inability to see as such patients can describe the faces they see quite precisely.</li>\n<li>Unilateral neglect: the inability to attend to objects in the opposite half of the visual field of the brain damaged site.</li>\n<li>Much of the visual cortex is hidden within the folds of the cortex.</li>\n<li>Review of the primary visual cortex (V1) and the six layers of the cortex.</li>\n<li>Physiological pathways hypothesis: that there are separate neural pathways for processing information about different visual properties.</li>\n<li>E.g. Color, shape, depth, and motion.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 812px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/0596c27cd7306ee209bd568fc82afe82/63ec5/figure1-3-22.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 104.80000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAIAAADJt1n/AAAACXBIWXMAAAsTAAALEwEAmpwYAAACuElEQVR42l2UiXKjMBBE/fsbx+s4PgADkhAIdHHY+cF9QGLHWzVVFjI9PdPTw0bIIrumRZnneUbwWGkpZVlVUleqaXSMvu/9OEbOWitjaq1lDG4Y4oara56lWSJkKVVJokorIYo0vWRZwr88WmumqW+a+nw+5UVmjOaeLBupZCmK0+mTAE+WS3Iuy4IqzuejUqLvQ+z9MEZda9LleUpRIfppGjbWWes605lKV3VTc25MY1oTglvv+z6OU084b9u25r4fIsgZ7IL30Yc++BgVOZXsrCULnJ1r286oSpq20TWp9f0+3G4j8QQv+OC8g/OaX4uy0JosAhUB0P/Hxx4t6rqihXHsV+QLmPoB50UOXi8tAEaYJLm8vf1Js3QWqW1iDNNvZh98gDl4wGCu14xRwZkX19rUlL3SIi+wcRrGBzOcK3iN2EfvnXOd99YzzLHnjNqMCsFX5BO8cj7wHJAdzrLM8USlJPNDrRjd/T5X+xv/DXa/wFKp4+lIqwx2t3tHLfA+WKVK51tKQLMn2P8wRwYWnJAC8Ol8EqLcvm/3+x2CO9/hTXqZbq/Mj5jNFOnYGtOoSpERt6ATfqLtuez/BaPqb6kC1gFP+lV2lLO2xWe8M+Kq+3i7T485rcz+hRnBKknDaZKgE0M6X04sjHMtslPU8OPNlznP5HN46jwc9uwJo979fd9u3yolbGfwDPtHLU97xiHGoY9zwREZAeMHQi8B3ixrzEp/fU33+0Tlq72JTZJeSlHOI8XPUrRtgw0bU3edqdcDbVvjliXprGGxZiFsy3ptjqfPw+cBfLashFISe8/BV0Xwc0UC1rvWFbblwyBlweNcl642rAGDlbOTBCkgz4tiXQ9+yYXPCWYOhs8GXcyLqyStbbIFvPwzv0E+IdmEjETc8xKvVigmynklh4DDh8Vh4zT8A9wOK3sEz70bAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 1.3.22\"\n        title=\"\"\n        src=\"/CR4-DL/static/0596c27cd7306ee209bd568fc82afe82/63ec5/figure1-3-22.png\"\n        srcset=\"/CR4-DL/static/0596c27cd7306ee209bd568fc82afe82/63868/figure1-3-22.png 250w,\n/CR4-DL/static/0596c27cd7306ee209bd568fc82afe82/0b533/figure1-3-22.png 500w,\n/CR4-DL/static/0596c27cd7306ee209bd568fc82afe82/63ec5/figure1-3-22.png 812w\"\n        sizes=\"(max-width: 812px) 100vw, 812px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The nature of visual processing in higher level areas of cortex is less clear than in area V1.</li>\n</ul>\n<h2 id=\"chapter-2-theoretical-approaches-to-vision\" style=\"position:relative;\"><a href=\"#chapter-2-theoretical-approaches-to-vision\" aria-label=\"chapter 2 theoretical approaches to vision permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 2: Theoretical Approaches to Vision</h2>\n<ul>\n<li>Theory: an integrated set of statements/hypotheses about the underlying mechanisms or principles that organizes and explains known, unknown, and predicted facts.</li>\n<li>We’ll examine many theories but one common feature among all of them is that they all ultimately prove to be inadequate.</li>\n<li>However, we can learn from wrong theories by trying to improve them and they may contain pieces of the truth.</li>\n<li>The answers to all possible experimental questions wouldn’t lead to proper scientific understanding because that’s just a list of facts.</li>\n<li>Proper understanding requires a theory that integrates old facts and predicts new facts.</li>\n<li>A theory relates facts to each other and allows them to be derived from a small and consistent set of underlying assumptions.</li>\n</ul>\n<p><strong>Section 2.1: Classical Theories of Vision</strong></p>\n<ul>\n<li>Why do things look the way they do?</li>\n<li>Four issues of visual perception\n<ul>\n<li>Environment versus organism.\n<ul>\n<li>Things look the way they do because that’s what they are and that’s how our visual nervous system evolved.</li>\n</ul>\n</li>\n<li>Empiricism versus nativism.\n<ul>\n<li>Empiricism argues that things look the way they do because we’ve learned to see them that way.</li>\n<li>Nativism argues that things look the way they do because we were born to see them that way.</li>\n</ul>\n</li>\n<li>Atomism versus holism.\n<ul>\n<li>Atomism argues that things look the way they do because each small piece of the visual field appears that way versus the whole visual field.</li>\n<li>Holism argues that how one part of the field appears will be strongly affected by how other parts of the field appear.</li>\n</ul>\n</li>\n<li>Introspection versus behavior.\n<ul>\n<li>Should a perceptual theory be derived from phenomenological observations of one’s own conscious experience or from objective measurements of human performance?</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Different theories take different stances on these four issues.</li>\n<li>Structuralism\n<ul>\n<li>Perception comes from when basic sensory atoms evoke memories of other sensory atoms.</li>\n<li>Sensory atom: a primitive, indivisible element of experience in a given sense modality.</li>\n<li>Rapid and unconscious associative memory processes were thought to underlie perception.</li>\n<li>Trained introspection was used to discover sensory atoms.</li>\n</ul>\n</li>\n<li>Gestaltism\n<ul>\n<li>Rejected nearly everything about structuralism and argued that perceptions had their own intrinsic structure as wholes that couldn’t be reduced down to parts.</li>\n<li>The simple concatenation of parts rarely captured the perceived structure of the whole.</li>\n<li>How does the structure of the whole emerge from its subparts?</li>\n<li>Psychophysiological isomorphism: perceptual experiences are structured the same as their underlying brain events.</li>\n<li>E.g. For the opponent theory of color, the opponent color pairs (red/green, blue/yellow, and black/white) should match the opponent structure in the neural events underlying color perception.</li>\n</ul>\n</li>\n<li>Ecological optics\n<ul>\n<li>Proposed that perception could be better understood by analyzing the structure of the organism’s environment; it’s ecology.</li>\n<li>This theory is about the informational basis of perception in the environment rather than its mechanistic basis in the brain.</li>\n<li>How does the world structure light in an ambient optic array (AOA) such that people perceive the environment by sampling information?</li>\n<li>E.g. Texture gradient provides information about surface tilt, light source direction, etc.</li>\n<li>Perceiving as the active exploration of the environment.</li>\n<li>However, the additional information from the temporal dimension doesn’t fully solve the inverse problem of vision because it doesn’t present a unique solution.</li>\n</ul>\n</li>\n<li>Constructivism\n<ul>\n<li>Combines many of the three approaches described above.</li>\n<li>Global percepts are constructed from local information.</li>\n<li>Unconscious inference: that the inverse problem can be solved by using assumptions with retinal images to reach perceptual conclusions about the environment.</li>\n<li>Vision is a probabilistic process where it computes the interpretation with the highest probability given the retinal stimulation.</li>\n<li>E.g. A circle behind a square has a higher probability of occurring than a three-quarter circle matching exactly the edges of the square.</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 844px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/0fb6ac61f19da6d5c7c3c7f97e01287c/33e10/figure2-1-12.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96.80000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC5UlEQVR42lWSS0/bQBSFLdEV3UB/B13RBZuKDVkgSgv9A3RTdkVU4lFE1XaTBAgUCHH8GtvxM7bnZSewoX+ux2MQIF1Znpn73XPm2JqUTEouS1VSVSnKSuiG/vvPr+5JF8vJtBKSiYfTxzbJNCEYCgshOBOcC045Z1L0znvHP4/39vc3Nj6laUKFwD56VCeQekNrxnBFNlUwljMaROG3nZ2lpaX5+bmT0245neaMcfkcLh5g8UhiPMiC0endbf9msLj4bvPz5jjPCs4L5UhK8QIWEr7hiqniFFM4w6BqMimnEzmp0JvRAkPpM9tQ0WoSV5XY5jXMGGzTWqfmHUKScQbDWKJ4DbMnWJYYxcM4tIkdhEFOi3GRK+eYQnNK1bNABDZxwihQ9umTMuDT3tnB4UGn24F/OEzzhqf1VVV+aNg72G932tUE94bBXCmrqHsX5z+Oji4u/wbhKEqigtNxoS7JlT6jZ+e9/cODy6vLwaAfhD54puA6vSRN4jRtt9tvFxaWl9+7PgEwViEBhpcoiS3HJp7baq18WFu9vS0hrjXpId77f/dft7dfzcy8mZvrdNu8lLg8sEylgJwxaKXVej07u7a2WlW8oOOHP4whAym8kb++/nHryxY+bJpnYJpqeKR9ow92v+8q27SGVfR1egwfuyqjOEL+sJq+5FFplgVxOL2ruChAUpZpRZGBLGiWoyPPwijMccR53f0SRsVpnOdJQ1I21q6vr66uLk3LQBiIxA8DfE/bcUzbxNK0LcO2dNO40XXTRmDENHXXw5nujxzNIeizPN83LBN9IMHow6HjkiE2HNt2ycAYDi1TNwzdHBIX/YZlG8S1NN93HXRAzCWYYuIFFjARspYJO8CwDx47mHsz6NuOOTQG0Nc8l3gecV3ij3y0giS+5/oeLBCXONCBceI0MLzoeh+yzRO2cUHbsgzHsWwoQ80lURym4yRJojiJkxSVhEmELMNoFEZ+kuLFj2L/P0+HxLLSU05pAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 2.1.12\"\n        title=\"\"\n        src=\"/CR4-DL/static/0fb6ac61f19da6d5c7c3c7f97e01287c/33e10/figure2-1-12.png\"\n        srcset=\"/CR4-DL/static/0fb6ac61f19da6d5c7c3c7f97e01287c/63868/figure2-1-12.png 250w,\n/CR4-DL/static/0fb6ac61f19da6d5c7c3c7f97e01287c/0b533/figure2-1-12.png 500w,\n/CR4-DL/static/0fb6ac61f19da6d5c7c3c7f97e01287c/33e10/figure2-1-12.png 844w\"\n        sizes=\"(max-width: 844px) 100vw, 844px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Heuristic interpretation process: the visual system makes inferences about the most likely environmental conditions that could have produced the image.</li>\n<li>It’s a heuristic because it uses probabilistic rules that are usually, but not always, true. If the assumptions are false, they lead to wrong conclusions in the form of visual illusions.</li>\n<li>Perception matches reality when the assumptions are true and become illusory when they are false.</li>\n<li>The more likely the assumptions are true, the more likely the perception is correct.</li>\n<li>The evolutionary utility of vision is maximized by using the most probable assumptions, assumptions that we cover in the rest of the book.</li>\n<li>How exactly does the visual system go beyond the observed information to solve the inverse problem in plausible and effective ways?</li>\n<li>E.g. The visual system assumes that large-scale edges of indoor environments, such as floors, walls, and ceilings, are either aligned with gravitational vertical or are perpendicular to it.</li>\n<li>Illusory perceptions usually occur under highly artificial circumstances because these assumptions tend to be true under normal viewing conditions. However, they also reveal the existence of the assumptions.</li>\n<li>There’s a close interdependence between reality, illusion, and hidden assumptions.</li>\n<li>In terms of implementation, connectionist networks can reach perceptual conclusions based partly on incoming sensory data and partly on additional assumptions embodied by the pattern of interconnections among its neuronlike elements.</li>\n</ul>\n<p><strong>Section 2.2: A Brief History of Information Processing</strong></p>\n<ul>\n<li>Three important developments in the 1950s to 1960s\n<ul>\n<li>Use of computer simulations to model cognitive processes</li>\n<li>Application of information processing ideas to psychology</li>\n<li>Emergence of the idea that the brain is a biological processor of information</li>\n</ul>\n</li>\n<li>No notes on computer vision, the invention of computers, Turing machines, block worlds, neural networks, perceptrons, Hebb’s cell assemblies, and parallel distributed processing (PDP).</li>\n<li>It turns out to be unbelievably hard to get computers to “see” even simple things.</li>\n<li>Luminance edges: changes in the amount of light falling on two adjacent regions of an image.</li>\n<li>The luminance structure in 2D images provides information about the structure of surfaces and objects in 3D space.</li>\n<li>No notes on behaviorism, Broadbent’s filter theory of auditory attention, iconic memory.</li>\n<li>Broadbent was among the first to propose a psychological theory specifying the temporal structure of information processing events.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/a42bc1dc8375015cb69881129f7f08a3/966c1/figure2-2-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.599999999999994%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAABH0lEQVR42jWQCY7CMAxFc3sEIxVRCs3aPauzFS44LsxEjuXIeT/OJ8YZG5wHHyDEFFKGhDlhhlJirVnrjdK+bW+M9dbqWkspKeeEXaKds8GHCB5Qwy/rsunNe4daqPJ678MwNE1za2/Xa4PdUhFNMcWYgFjwCENOalDt/d5cj2WdRYl5npx3m9bYQkwphcdSMsJHpEDcB04lU0rPP5fz5Xw6naZ57vv+0XWUMYSNtXWvECH/k3C8HIiPwUeIOWmr1TgywZWSAAHHq3veX3Xd1ra9P57P7tEZY/74w5RAxmWa12XZVm2NscaHwzn04TNhwlobjTyOPc0Tfue4gIGmBEsoZ5RzzFwKISUX/BtSSRzsW6jPwlpI3IJxzgQbRvkLu0MMtHIWTSAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 2.2.5\"\n        title=\"\"\n        src=\"/CR4-DL/static/a42bc1dc8375015cb69881129f7f08a3/00d43/figure2-2-5.png\"\n        srcset=\"/CR4-DL/static/a42bc1dc8375015cb69881129f7f08a3/63868/figure2-2-5.png 250w,\n/CR4-DL/static/a42bc1dc8375015cb69881129f7f08a3/0b533/figure2-2-5.png 500w,\n/CR4-DL/static/a42bc1dc8375015cb69881129f7f08a3/00d43/figure2-2-5.png 1000w,\n/CR4-DL/static/a42bc1dc8375015cb69881129f7f08a3/aa440/figure2-2-5.png 1500w,\n/CR4-DL/static/a42bc1dc8375015cb69881129f7f08a3/966c1/figure2-2-5.png 1694w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Few neuroscientists take the simple analogy of neural spikes as digital code seriously anymore because there are many striking differences between brains and digital computers.</li>\n<li>But the idea that the brain is doing some kind of information processing is now almost universally believed.</li>\n<li>No notes on lesion experiments, electrical brain stimulation, single-cell recording, Hubel and Wiesel’s work on receptive fields, autoradiography (using a radioactive substance to track the firing of neurons), brain imaging techniques (CT, PET, MRI, and fMRI).</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/fede8e4be83b3d3aa0d468239158615a/eaf69/figure2-2-10.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.199999999999996%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAAsTAAALEwEAmpwYAAABuElEQVR42kWRb0/aUBTG+TZI1lqoqNBggpDMVmayhpfUqFlTDL6accMX8M5Nt8Qldm60tPR/b+9tce4j7gGzLXl6cm7b33POuaeUMUpzygqWZVkUh17gLbxFGIVRFPqBj4RkZK00QyRpSpK8YCzPGCOlNcxw/vL1bjqd6Lp+cnpydnaqaQNZlg8OXiPB0TCM6+vxzacbP/BWMPsLszWsqm87nY4kNZvNxtHRG2DIt7frPM+Vy2We40Sx1mjsnp8Pn34tKSX0H0xZpmna8bGm6+9QZzg0Li/f395+vv92Px5/RDvdbnd3Z2djo9zv959/PwP+Xzkl6WFPUWS51ztst9utVmt/v418Mp2gT89bwEhRFFEUB4PBCn5pm9AMMydpIklSpVKpVoVarQoJgsBxr/D3aDSybQsWpmkODePiYrR8WmZom5ISSFbkhBJVVTHuliiC4Xle2NyE6vWtvb3W1YcrXDW2YFmzOImLZYE7ynNaclwHKwnCwPO9ueu4CzyO484Rg9DHJzCu60ZxBGFzCUnjNI6TCCo9mA/m4/fHnz9m1uxFlm3b85VQBEPBFIIFIrwgvF8pjv4AcmEOC7uHc5AAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 2.2.10\"\n        title=\"\"\n        src=\"/CR4-DL/static/fede8e4be83b3d3aa0d468239158615a/00d43/figure2-2-10.png\"\n        srcset=\"/CR4-DL/static/fede8e4be83b3d3aa0d468239158615a/63868/figure2-2-10.png 250w,\n/CR4-DL/static/fede8e4be83b3d3aa0d468239158615a/0b533/figure2-2-10.png 500w,\n/CR4-DL/static/fede8e4be83b3d3aa0d468239158615a/00d43/figure2-2-10.png 1000w,\n/CR4-DL/static/fede8e4be83b3d3aa0d468239158615a/aa440/figure2-2-10.png 1500w,\n/CR4-DL/static/fede8e4be83b3d3aa0d468239158615a/eaf69/figure2-2-10.png 1701w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><strong>Section 2.3: Information Processing Theory</strong></p>\n<ul>\n<li>The information processing paradigm is a way of thinking about the human mind as a computational process.</li>\n<li>No notes on Thomas Kuhn’s scientific paradigm and the brain-as-a-computer metaphor.</li>\n<li>E.g. Minds as “software” and brains as “hardware”.</li>\n<li>Review of David Marr’s three levels of description: computational (the system’s goals), algorithmic (how a computation is executed), and implementational (algorithm embodied as a physical process).</li>\n<li>Three assumptions of information processing\n<ul>\n<li>Informational description\n<ul>\n<li>Mental events can be described as informational events consisting of three parts: input information, operation, and output information.</li>\n<li>Information events are diagrammed as a black box in an information flow diagram.</li>\n</ul>\n</li>\n<li>Recursive decomposition\n<ul>\n<li>Any complex informational event at one level can be specified at a lower level by decomposing it into component informational events and a flow diagram.</li>\n<li>E.g. A black box can be defined by a number of smaller black boxes inside it, plus a description of how they’re interconnected.</li>\n<li>This process can be recursively performed again and again for each black box.</li>\n<li>Recursive decomposition only works if the system is actually structured as a hierarchy.</li>\n</ul>\n</li>\n<li>Physical embodiment\n<ul>\n<li>Information and operations are entities in the abstract domain, while representations and processes are entities in the physical world.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Representation: a physical entity that carries information about something.</li>\n<li>Process: a physical transformation that changes one representation into the next.</li>\n<li>A representational system includes two similar but distinct worlds: the represented world (external) and the representing world (internal).</li>\n<li>How does an internal world represent an external world?</li>\n<li>One method is by having the internal representation mirror the structure of the external world. Thus there’s a map from external objects to internal representations.</li>\n<li>A standard tradeoff in representing information is that the more information is stored directly, the less must be inferred by additional processing and vice versa.</li>\n<li>Without representations, processes would have nothing to work on. And without processes, no work would get done.</li>\n<li>Processes make implicit information in the input representation explicit. However, processes can’t create information from thin air.</li>\n<li>All the information must be available either in the optical structure projected to the retina or from internal knowledge of the viewer.</li>\n<li>E.g. The boundaries between retinal regions projected from different surfaces and how surfaces are combined to form objects.</li>\n<li>Visual perception thus combines external and internal information to make meaningful facts about the environment available to the organism; it’s a form of inference.</li>\n<li>E.g. An image with a set of lines converging towards a vanishing point, plus the convergence assumption (lines converging to a point on the horizon are parallel), lets the visual system conclude that the converging lines are actually parallel lines.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 812px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/1498eeaa0b1d2d250efa18f46a068cb7/63ec5/figure2-3-9.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 88.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsTAAALEwEAmpwYAAACnklEQVR42kVT2W7bMBDU9wZ9aR6CAoGBIGiA2o6L2JZIipcoiZeow3a/sEM5aYUFTROc3dmZZeGj90OoeS2UkFpKKZSSSiuEaU1jmsboNbBpuq7VWlnXX67LvExFGIKPgUuBTUxxGNM4jTnmvE5zDtxDLMt0uSwxhr7v/oEjKkutEm7P0z2meboDPmGXGTDE9XYd0tBbVL5ksE9DHAfCSEUI47yWQoNga9qubfOKH4NSXd7gXycEN6ZZwXPhh5jmabvbPTx8e3r68fy8eX39udvt9vt9WZXv74f3w+/tbvv29vby8rLZbB6/PzLGbn9uK+00DNN4Ks+4/vFxPJ5OZVXVPH+mbWoutG7qmlFKz2V5PB6RF5qB/3/wmVQZBuZ1LaQQUhpAjdZZ9kZp2CDwccFPpxNkX8EzBEPPidSsoqwWAvdUA2OAbZ13utG5f/QNq/K5BmecfPYcs2CJZZ/VisnRWwttrLNKKed83ts+C9Z38P9L7S9wLbhUOO2yyJ1xzmLvfE7R9T1y4YPmPnipJJJ+gsMKpoxZ79CeCw4cfXCojXW1qhtSAin0BzxSgUIGz1OBkQK4wdyZBilRCXh0i+LeW4Cd9zFGVMZs4RzMQwzLZbkLlkcSlcEnTSNawnhB5XFKqAzyEQVtj0MIgqmEhd19PHNljPEycykx3lwIJjgowCrcgEfICMEhGzqGi21+GBosbrcr8MW5KuFQa/uKEgO90Jbt75qjsvfOZvGgWR9CgGBox64B8sX+cPi13RLGMGRwGDUJpRjMsjzzTEVQSrASpG4NmqKM4v1mOloVcBhUac0IoyCPv8gC57LzEqkErsr8WjReDSHV/XAdXlNkJK9LUlWUoiWQQWTCweP1pYQnHu97BPZpxBmefkhj+gvuCKXrjaobjgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 2.3.9\"\n        title=\"\"\n        src=\"/CR4-DL/static/1498eeaa0b1d2d250efa18f46a068cb7/63ec5/figure2-3-9.png\"\n        srcset=\"/CR4-DL/static/1498eeaa0b1d2d250efa18f46a068cb7/63868/figure2-3-9.png 250w,\n/CR4-DL/static/1498eeaa0b1d2d250efa18f46a068cb7/0b533/figure2-3-9.png 500w,\n/CR4-DL/static/1498eeaa0b1d2d250efa18f46a068cb7/63ec5/figure2-3-9.png 812w\"\n        sizes=\"(max-width: 812px) 100vw, 812px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>No notes on deductive and inductive inference.</li>\n<li>Most inferences in visual processing are inductive because they’re not guaranteed to be true.</li>\n<li>E.g. The assumption that converging lines are parallel isn’t always true, but only probabilistically true. It’s false for the converging but nonparallel sides of a trapezoid.</li>\n<li>If the assumption is false for the current situation, then the conclusion may not be valid.</li>\n<li>This is how inferential theories of vision account for the existence and nature of many visual illusions.</li>\n<li>E.g. The Ponzo illusion may be due to the unconscious misapplication of the convergence assumption.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 808px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/85e4c17a5c0de4f4e4a579df5642cfc6/3534c/figure2-3-10.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 74%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB0klEQVR42lWTC3ajMAxF2f8GOpkB/MOAAf+N2V6f7Cank/PSWraunizIcN/5o/pePE8JwZ6ncfbEGjul6b/kmodyp1xSjymjpK7dbNgxZo8pIPzktLTyhttBB3Jf39n5S6+LdRYljtOUWkqDgYHrQjh8mK6USdu+Oe99IGGdS0ZL+Y2VH/cyNCB2BkIeDM1h4E+t1nohdhYL8L/g0uBOpkhqMKxCDHctd707sxtTftk2mLyHlEKMP8KW827bd5Bo25jTOl+f57wuhCj0C6bRDB8yBk+2ZgcPOMSEhn0ISE05m+P4wHT/7hyCa/JoG3fb9rWNlHquD74IChbHeWB4dBEa1nvaOYeUPPWck28T7lel09r+09+akRdCfe671W2PugzzPErJ13XZthUf9IxR72a/7IUnjNuic+vcZS1N3dvLnpAPuJEbXq+v158vlJimcWazUkqvWkihlOScMc6EVFKpmTGcIlTLorUWQqhFDpzPsOWcT9PEBWVywddtBckFWzRhfVNKidZQCDQyUW2Qko3jX8aaiZJaL+M4LXrBhpAcsFrgLOYZxf+hipBSUwmJI3KGJG0KGDLO0TM6hwOXAu82nmd7BSNmGaKP7V3CryXn+A3cdmJloJghEQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 2.3.10\"\n        title=\"\"\n        src=\"/CR4-DL/static/85e4c17a5c0de4f4e4a579df5642cfc6/3534c/figure2-3-10.png\"\n        srcset=\"/CR4-DL/static/85e4c17a5c0de4f4e4a579df5642cfc6/63868/figure2-3-10.png 250w,\n/CR4-DL/static/85e4c17a5c0de4f4e4a579df5642cfc6/0b533/figure2-3-10.png 500w,\n/CR4-DL/static/85e4c17a5c0de4f4e4a579df5642cfc6/3534c/figure2-3-10.png 808w\"\n        sizes=\"(max-width: 808px) 100vw, 808px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>An important function of perceptual learning is to collect the most appropriate hidden assumptions so that veridical perception is achieved as often as possible.</li>\n<li>However, in the case of conflicting assumptions, how does the visual system determine which inference is correct?</li>\n<li>E.g. The lines in Figure 2.3.9 can be perceived to lie at the same distance (same plane) or be perceived as receding into the distance. The former assumption supports the conclusion that the converging lines aren’t parallel in depth, while the latter assumption supports that the lines are parallel.</li>\n<li>If different assumptions lead to different conclusions, one can’t conclude anything using both assumptions.</li>\n<li>So, one assumption must be selected to the exclusion of other assumptions. This isn’t ideal though as we now need a competitive framework.</li>\n<li>Alternatives to competition\n<ul>\n<li>Soft constraints: informational restrictions that may be overridden.</li>\n<li>Fuzzy logic: statements can have different degrees of truth.</li>\n<li>Probabilistic inference: uses Bayes’ theorem to select the assumption.</li>\n</ul>\n</li>\n<li>One argument against the inferential perspective of vision is that visual illusions occur in very odd and artificial conditions that would rarely, if ever, show up naturally.</li>\n<li>Yet, the phenomena do occur and require an explanation; we can’t just ignore evidence.</li>\n<li>This book takes the stance that a perceptual theory must account for all phenomena of visual perception, whether natural or not.</li>\n<li>Inference-based vision is preferable to direct perception because it can explain both natural and unnatural phenomena.</li>\n<li>We’ll also adopt the stance that perception involves some form of inductive inference carried out by computations in neural networks.</li>\n<li>Review of top-down (hypothesis-driven) and bottom-up (data-driven) processes where the retinal image is at the bottom and subsequent interpretations are farther along the visual pathway.</li>\n<li>Our intuition suggests that vision is a bottom-up process and while that may be true for the early processing stages, this can’t be true for all of visual perception.</li>\n<li>E.g. Perception of the present state produces expectations about the future. These are top-down components because higher-level interpretation aren’t observed.</li>\n</ul>\n<p><strong>Section 2.4: Four Stages of Visual Perception</strong></p>\n<ul>\n<li>Four stages\n<ul>\n<li>Image-based</li>\n<li>Surface-based</li>\n<li>Object-based</li>\n<li>Category-based</li>\n</ul>\n</li>\n<li>The complete set of firing rates in all receptors of both eyes and their organization is the first representation of optical information within the visual system.</li>\n<li>The representation is complicated by the uneven distribution of receptors and the four different kinds of receptors.</li>\n<li>We can simplify this by assuming the retina captures light in a x-y grid pattern.</li>\n<li>E.g. Pixels or picture elements.</li>\n<li>Pixel: the primitive indivisible unit of visual information that represents an intensity value at the location.</li>\n<li>The coordinate system of the retinal image is presumed to be tied to the structure of the retina.</li>\n<li>E.g. Center of coordinate system is the fovea.</li>\n<li>Past the retinal stage is the image-based stage.</li>\n<li>E.g. Local edges and lines, matching images in the left and right eyes.</li>\n<li>The luminance edges detected in an image aren’t the same edges that people typically perceive.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 805px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/51d99ebbeeae6fa9db6838b976f1364c/c946b/figure2-4-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 80.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAACTklEQVR42iWT2WLjIAxF/f8v0yTTpJ14ZQcveMFx+ndzcF2FgtBydSUKrUXT1D5466yx1jrHigghYowppbbrvr6+tNFK81nEmGwglSy8t/3Qv3/er+O1pX1Ftm1LCaNxmthrbYiYlfue9n0/Xjt/x4tkRQh+S9uyLiGEMcYej3mOy6KNYbusmzIWz3lZCGGdD31PCFyccwV4vXf3++Pjz8fj8V3VrVSG/FQwjuOybUJp3C7X2+fn4/msbrf7syzfP0d2tlY3fG1HSdfrXwxPf40mZ96oWZZVfb3eqqap6mx5uVxBALRc8zgNzvspRgXUOLdCujBUVU0VZPZQMoyswBmmeU0J5P0wEKZwVk8x14aqk2qYYt20SmlynM7J+uBCr41jjwzj1NRNZlvKwmg5TeSboWSal3nN3CzrCnICYQ3XpMU687TvcY5QcLwPwBbGyBhHSKSYTqhOSCFJbDgQEmdqISghCEfX/j1LrgjYtm0mDNj0BnqAxDpOOK1cj3GhTnIShS0FI1CzrivQGJtCdM0wAjD+os2ybbBykodzAiSdn7M+zfSXodvT69jzkDinlZLG6HMwHcCy0AdrcVi3xBHAnJkWafJshj7ArvOueD6/67oSgkKoxJ0DnOcxX4fehwAxlGCyzp8Txij6YRgopSjL78fjXtW5/1lqJoFmtfxQsX3ylWVdt3S+rLIZTcJeaVVoJdq24YkIdAq2BcKONYeocxNwIABBueC1ARi2MmE8ya5jOiQPkf/4Y/HriWTr85j7KGTLrMKQknjO8/wfUI91Rc1RfY0AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 2.4.4\"\n        title=\"\"\n        src=\"/CR4-DL/static/51d99ebbeeae6fa9db6838b976f1364c/c946b/figure2-4-4.png\"\n        srcset=\"/CR4-DL/static/51d99ebbeeae6fa9db6838b976f1364c/63868/figure2-4-4.png 250w,\n/CR4-DL/static/51d99ebbeeae6fa9db6838b976f1364c/0b533/figure2-4-4.png 500w,\n/CR4-DL/static/51d99ebbeeae6fa9db6838b976f1364c/c946b/figure2-4-4.png 805w\"\n        sizes=\"(max-width: 805px) 100vw, 805px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>No notes on Marr’s raw and full primal sketch.</li>\n<li>Common underlying structure of image-based representations\n<ul>\n<li>Image-based primitives</li>\n<li>2D geometry</li>\n<li>Retinal reference frame</li>\n</ul>\n</li>\n<li>The second stage is the surface-based stage and it deals with recovering the properties of visible surfaces that might have produced the features discovered in the image-based stage.</li>\n<li>One distinguishing feature of the surface-based stage is that it represents information in three dimensions.</li>\n<li>It doesn’t represent all surfaces, only the visible ones, and can’t be computed from only retinal images because it uses additional assumptions.</li>\n<li>Properties of surface-based representations\n<ul>\n<li>Surface primitives: local patches of 2D surface at some slant located at some distance.</li>\n<li>3D geometry</li>\n<li>Viewer-centered reference frame: surfaces are represented in a 3D coordinate system based on the viewer.</li>\n</ul>\n</li>\n<li>The representation of surface is constructed from several image-based features.</li>\n<li>E.g. Stereopsis, motion parallax, shading and shadows, texture, size, shape, and occlusion.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 671px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/7ae17cff478477c04f342d09032beb19/d0e73/figure2-4-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 124%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAIAAAC+dZmEAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2klEQVR42k2U63KjMAxG8/4/d9qddpNws40NmGsgz7dHEklLPdRDfKRPN1+OY9v3x3Hs3jV1XXn+NXUj24rFf3scn5uaH6/Xf/Ocn89937eLwhtw24a+78dxHIY+5yHnns2QB56+7+TVd+OYMTFNBj8E1rV771OMnEgp2uq6pEsehTGdQ/DLMj+fB4jAj8eKGe8bAGDzY6vXNaj7lFLO2Tu3rovCxwXSYAILIaQutTE6CbEuy7Ii7KbxPnSd2MJzG8Jywvsb3jgdkT3019uVPepNLU4xei8KdJERoluW5Xgeu8LLti3AMNfbjRR//v1EBQ6d8/JChXNFWbKZphHZFrMkbNvmdZ3gxXOKzjuk4g2dOCE94nCa2GGOsKuqnOcJzT/wCuwEJl0470ibJkxrlvOYiZ8WiLFlM08vGHJefjzjB9vKSHkNlvK6hoDxiRbep2yBZ4HpKSssG6uRJkweTBAwdUYLG4Ml2wYjG7XpBedXY+lGEs5Hrf/gf8PLkud5BC7rMkp5UlkWOJNuwXN/Rk5Q1jxvWGS/4LWoija25JOsEjatzor6eKlWYz1HCTTbL1jWOpcVnqMNQJDCMFSVUg1WMGpRkHOqfWZb4ZG6K9wiW3LepXd72+r0CxnAEIcV3kjYSM7wjFqtc/cbtoTxHnO2mIEZjBNWzROVRGGShpbBSK9hVBPnPJpsIsIzmh+PjQ6TIhMGjSwp4i+2li32tIQ1qT1srD1POAQmscGqC03XJ64OKZhMdW9CkGBDrj1HOB2et01m8fL19fnx8aco7rfbFW+V3l30OfmTsQo2VOf1hhIV1drAXLyXegBXdclIwQDXHG7qoiyolmgOvlETKIdECyb5fqnlwig5BwaMq0YPBVUBxmTzKySnOajXq1gU2LkKwd/fX/wAjAQTKYOtsFO1Xq/eVpuH9ZZdl2gm0rpqIwZ9oeZpGOB7cedWofLmk3mWG6pL5Gxdl/8R29sVR0Q62QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 2.4.5\"\n        title=\"\"\n        src=\"/CR4-DL/static/7ae17cff478477c04f342d09032beb19/d0e73/figure2-4-5.png\"\n        srcset=\"/CR4-DL/static/7ae17cff478477c04f342d09032beb19/63868/figure2-4-5.png 250w,\n/CR4-DL/static/7ae17cff478477c04f342d09032beb19/0b533/figure2-4-5.png 500w,\n/CR4-DL/static/7ae17cff478477c04f342d09032beb19/d0e73/figure2-4-5.png 671w\"\n        sizes=\"(max-width: 671px) 100vw, 671px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The fact that we have expectations about partially and completely hidden surfaces suggests that there’s some form of true 3D representation that includes occluded surfaces.</li>\n<li>The object-based stage is when visual representations truly include 3D information.</li>\n<li>However, to achieve this representation, the visual system has to make further hidden assumptions because now the inferences include information about unseen surfaces or parts of surfaces.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 676px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/4b71339a3f3dd1a6eae7e6c3939f4182/9bb7a/figure2-4-7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 77.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+UlEQVR42m2T6XLbMAyE9e5p+t9pU8vWSYrifVNS+nhd2ZbTZurZ4dgcfFgQgKuy5FzuSimnmOLEJmOtNgZSRimttNGUUaGEj95654Jz3uKslvUJ7wLMOS9LyVCBUl5SLIUrJZUM0e/koeoW9CBTySECnlEOUtBpGimd5tmnBFgoFWJwwbv9BOw/4ZRzzNnHQCfWtt3Ly2vTttem+fb6nQmhjJFShhRdjDt8U3VUeJSdE6G0H4a6vlyuzbm+nE5vl6YdyMSl8jn9Dz54vBntuWv7/QGNhEitYT7DOecniSxV/tfZB4/24sF93wup5lm8n88+xVkqPPsGx08Yzs/K8cXdYGPN6XRqu/791/ntx090G85CyQN+6AGXA4YzCl7WxXknlUbBSBdLRs1Cq6/w33Mq62Kdg+0tV8HPsiyYQlrKY1Rf4HVbt48NVsu2lm2FszEGbfMhOh+sxyphpIFLqawBjJlDWAcMtcLe4HPvE9YTt9ZZgVCNxdSzkFxwxmbGuXZWO6egfXm1cabquu5c12yeu75HjJACi9UPPaEEl1yIYRww+XEk3TAMlLR91w190zb4C1RwQBBjDHH71YSFpHDGiW5jyNDEGNYGWNO19eWK1NfmSiZaIQgmyE339ON+khEewwhHgmkTQrCkI6HaGgwMjxf7yYUSfwA9yFY3yxvnHAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 2.4.7\"\n        title=\"\"\n        src=\"/CR4-DL/static/4b71339a3f3dd1a6eae7e6c3939f4182/9bb7a/figure2-4-7.png\"\n        srcset=\"/CR4-DL/static/4b71339a3f3dd1a6eae7e6c3939f4182/63868/figure2-4-7.png 250w,\n/CR4-DL/static/4b71339a3f3dd1a6eae7e6c3939f4182/0b533/figure2-4-7.png 500w,\n/CR4-DL/static/4b71339a3f3dd1a6eae7e6c3939f4182/9bb7a/figure2-4-7.png 676w\"\n        sizes=\"(max-width: 676px) 100vw, 676px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Two ways to construct an object-based representation\n<ul>\n<li>Boundary approach: extend the surface-based representation to include unseen surfaces in a 3D space.</li>\n<li>Volumetric approach: conceive of objects as intrinsically 3D entities and represent them using some set of primitive 3D shapes.</li>\n</ul>\n</li>\n<li>In object-based representations, are the primitive elements surfaces or volumes? We don’t know.</li>\n<li>The ultimate goal of perception is to provide the organism with accurate information about the environment to aid its survival and reproduction.</li>\n<li>This strongly implies that the final stage of perception deals with recovering the functional properties of objects.</li>\n<li>This final stage for vision is the category-based stage where we believe the functional properties of objects are accessed through categorization (or pattern recognition).</li>\n<li>E.g. Visual system → Classifies object by visual features → Access information about the object class such as function and form → Apply information to object.</li>\n<li>This scheme means that any functional property can be associated with any object.</li>\n<li>An alternative scheme is that the visual system perceives an object’s function directly from its visual characteristics without categorizing them.</li>\n<li>Affordances: whether an object allows a function or behavior.</li>\n<li>It’s possible that we use both types of schemes to perceive function.</li>\n<li>These four stages of visual processing (image-, surface-, object-, and category-based) are our best hypothesis on the overall structure of visual perception.</li>\n</ul>\n<h2 id=\"chapter-3-color-vision-a-microcosm-of-vision-science\" style=\"position:relative;\"><a href=\"#chapter-3-color-vision-a-microcosm-of-vision-science\" aria-label=\"chapter 3 color vision a microcosm of vision science permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 3: Color Vision: A Microcosm of Vision Science</h2>\n<ul>\n<li>It’s isn’t obvious why objects should be colored; they simply are.</li>\n<li>People universally believe that objects look colored because they are colored.</li>\n<li>E.g. The sky looks blue because it is blue, grass looks green because it is green.</li>\n<li>But these beliefs are mistaken because neither objects nor light are actually “colored”.</li>\n<li>Color is a psychological property of our visual experiences and not a physical property of objects or lights.</li>\n<li>Color is based on physical properties though and these physical properties are different from the colors we perceive.</li>\n<li>Instead, a more accurate description of color is that it’s the result of complex interactions between physical light and our visual system.</li>\n<li>Color perception may be one of the best understood topics in vision science and how we reached such an understanding may teach us how scientific discovery unfolds over time.</li>\n<li>Steps to understand color\n<ul>\n<li>Consider the nature of the input information: light.</li>\n<li>Consider the nature of the output information: experience of color.</li>\n<li>Then consider the relationship between the two: how the physical domain of light maps onto the psychological domain of color experience.</li>\n</ul>\n</li>\n<li>We’ll discuss color in terms of image-, category-, and surface-based computational stages. An object-based stage isn’t needed because color is a property of surfaces rather than of volumes.</li>\n</ul>\n<p><strong>Section 3.1: The Computational Description of Color Perception</strong></p>\n<ul>\n<li>Review of the physics of color (the discovery that white light consists of colors, review of the photon, electromagnetic energy, wavelength, electromagnetic spectrum).</li>\n<li>The photons we experience as light are just a small part of the electromagnetic spectrum.</li>\n<li>E.g. From 400 to 700 nanometers in wavelength.</li>\n<li>Monochromatic light: light with a spectrum containing only one wavelength.</li>\n<li>Polychromatic light: light with a spectrum containing more than one wavelength.</li>\n<li>E.g. Sunlight is polychromatic because it has a roughly equal number of photons at all visible wavelengths.</li>\n<li>Color only comes into the picture when light enters the eyes of an observer with a visual nervous system to experience it.</li>\n<li>E.g. There’s light of different wavelengths independent of an observer, but there’s no color independent of an observer.</li>\n<li>Different colors appear because different surfaces reflect different proportions of light at different wavelengths.</li>\n<li>All colors can be described using three properties: hue, saturation, and lightness.</li>\n<li>These three properties define the color space: a 3D coordinate system where each possible color experience is represented by a unique coordinate.</li>\n<li>Note that there’s an enormous reduction in complexity from the physical description of light to the psychological description of color.</li>\n<li>E.g. There’s an infinite number of wavelengths between 400-700 nm, but color can be defined with just three values.</li>\n<li>This implies that color experiences lose much of the information carried by the full spectrum of light and that many physically different lights produce identical color experiences.</li>\n<li>Perhaps the lost information was deemed as unimportant by evolution.</li>\n<li>Hue: what we normally think of as color and changes with wavelength.</li>\n<li>If light is arranged by physical similarity in terms of wavelength, then it lies along a straight line where the shortest and longest visible wavelengths are least similar in physical terms.</li>\n<li>But if light is arranged by perceived similarity, then it lies in a circle where the shortest wavelengths (purples) and longest wavelengths (reds) appear close together.</li>\n<li>Thus, the physical dimension of wavelength and the psychological dimension of hue are related, but not identical.</li>\n<li>E.g. The hues between purple and red, like pink, are non-spectral because there’s no single wavelength in the visible spectrum that produces them. These hues can only be created by combining two or more wavelengths of light.</li>\n<li>Complementary colors: hues on opposite side of the color circle.</li>\n<li>E.g. Green and red, blue and orange, yellow and purple.</li>\n<li>Saturation: captures the purity or vividness of color experiences.</li>\n<li>E.g. Grays → Pastels → Vibrant.</li>\n<li>Lightness: refers to the luminance of color.</li>\n<li>For objects that emit light rather than reflect it, we call it’s luminance as brightness.</li>\n<li>Otherwise, the shape of the color space for emitted and reflected light is essentially the same.</li>\n<li>The perception of lightness/brightness is mostly due to the contrast of one region with a surrounding region.</li>\n<li>E.g. If two surfaces reflect the same number of photons per unit time, the surface surrounded by a darker region looks lighter, and the surface surrounded by a lighter region looks darker.</li>\n<li>Psychophysical correspondence: how physical descriptions map onto psychological ones.</li>\n<li>It would be convenient if there were a clear and simple mapping between physical light and color experience, but there isn’t.</li>\n<li>One way to simplify the mapping is to only consider surfaces that reflect light who spectrum is approximately in the shape of a normal distribution.</li>\n<li>Three simple psychophysical correspondences\n<ol>\n<li>Mean wavelength determines hue.\n<ul>\n<li>Not all hues can be produced by normally distributed spectra.</li>\n<li>E.g. Non-spectral purples.</li>\n</ul>\n</li>\n<li>Spectral area determines lightness.\n<ul>\n<li>The area under the curve matches the total number of photons that hit the retina.</li>\n</ul>\n</li>\n<li>Variance determines saturation.\n<ul>\n<li>Monochromatic lights (which have zero variance) are the most saturated colors. In contrast, white light (which has maximal variance) is the least saturated.</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 561px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/031404db45a7e69cb4723bac5c24c9de/410f3/figure3-1-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 144.79999999999998%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAdCAIAAAAl5NuSAAAACXBIWXMAAAsTAAALEwEAmpwYAAADyElEQVR42kWVCZebOgyF+fl9TU+nM0lYwr4EA7bBQJb5ef1s56WMDgPGV5LvlZTgdtux/bat+7puq1nNtm/DOGR5fjwdq7q6ij5JkrKutJnVrL1JrbDg8biP0ziMQsppv+3btm77KgbRdt21v9ZN0wvRtG0v+tksetHY20Xw/H6WVVU3tVQOvG+LWbKcwMXxeLyk6dfXVxhxxQ6MzWqZ/4GLouiu7TS9wATRs+6uXRwnYhiOpzOJdH3fi2FeVzkDXhT3eQ7ujzuJucyH7baN4yhEz3kAg4zimK9t20mti6ruxxGwNsbilyW43W8YPJFhcrnE8SVO7BW7f2mans9hFF/OIe9plKTnKK6b1rqwYEv1vrujYjN/7g7zmHbHY9+8GvKcFDzrUSmf/CsytjsvPCil2q4li6qqOAWEH08nwjfddd232Zj5nTZSEUdpIIrzgyfgYozWGrXKsuLYTddOSrLHflqNx2PB8/mo6ppQfCMsbGO4gAUhxCRlUZRSgtTvsHoxnjMHrkq4ZYcPi+FlGAa/SBGQNLpf+96BrdqvtL+/n3Vds5UiIW17+H3nCGxlPY4t6xRb27bU0rpb72YzypUKbN98khAzDCOAi71SUJz248+fn4cDfFFtRUmC16btmg7ZlQPfb14t46SSSk5ywoTNRVJqwzh66/orK6OUA99pDMrz7sGQtDudtK5c8DTN8qLgqHBOnZzDkFcSNJbt1VeoA993X9U0pmd7db1FnKLI0yyDJA/z9cPrPzC5cbHVu/BdzXPTNqfTicjb/+I7qi1VvjGDx/NOnl3XobMHezyWZWSdw1NZ11ch3rXhwnrw9yMvciQF/IahGW0YxRH0/v79UVQlPJltm528trBf4CfgwvcwGDS0tTWIz89PnOLx8+tIbyEhCi3rumwbXl5gX1XQELlxgZ6HX/YKoxCPNHlRVuB//PgPwenLEP2TxJa6B3s8oRgpxCcadz8erMLThMGtVMrpLz7YWZV0SOBL8t0PPLsDD2EYns5nqjrN7JDgzBSCIe11dTNIa7MEuEckBCSUVWxWNrJTTro2tA9MVjkqu6gmZYeuIKlpDCiew+FQlgVVhbBcxGGRy5LUNghGbzBA0Y3egF23IYySOCgrdKRnrTHimcGsIC+rDOA4iXHKA4sIbudaekHCvCyyIg/AJJfEzp0kAYzXLEs9mDMzCInsCp06TQGA5JeEhkvzLKA8wOMbPM88cCdbNpEFUtsiPZ94ZRvPRVUR/wVmB4MO9xBmO4ayddUPVbxyp6qYmAjrf58wqBJWwvEvFkBxOsjkK/IAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.1.3\"\n        title=\"\"\n        src=\"/CR4-DL/static/031404db45a7e69cb4723bac5c24c9de/410f3/figure3-1-3.png\"\n        srcset=\"/CR4-DL/static/031404db45a7e69cb4723bac5c24c9de/63868/figure3-1-3.png 250w,\n/CR4-DL/static/031404db45a7e69cb4723bac5c24c9de/0b533/figure3-1-3.png 500w,\n/CR4-DL/static/031404db45a7e69cb4723bac5c24c9de/410f3/figure3-1-3.png 561w\"\n        sizes=\"(max-width: 561px) 100vw, 561px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>This is simple for light that’s normally distributed across the spectrum, but once this restriction is relaxed things get very complicated very quickly.</li>\n<li>E.g. We can’t predict the perceived color of a surface from only knowing the spectrum of light being reflect from it because of the surrounding surfaces, lighting conditions, orientation of the surface, and shadows.</li>\n</ul>\n<p><strong>Section 3.2: Image-Based Color Processing</strong></p>\n<ul>\n<li>It isn’t clear nor confirmed if color is represented at the level of a 2D image.</li>\n<li>Many phenomena discovered more than a century ago helped us better understand color perception.</li>\n<li>E.g. Color mixture, types of color blindness, color afterimages, existence of induced colors, chromatic adaptation.</li>\n<li>Light mixture\n<ul>\n<li>Only a small fraction of colors in the color space are monochromatic lights.</li>\n<li>Nonspectral colors (purples) and desaturated colors (pastels and grays) are when two or more different wavelengths are mixed.</li>\n<li>How do colors combine?</li>\n<li>It depends on whether one mixes light or paint.</li>\n<li>For mixing light, it’s simple. When mixing light A with light B, the resulting color lies on the line connecting A and B in color space, depending on the proportions of A and B.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 803px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/77b6f43b5aff418271889ebaa2fa97ed/e1031/figure3-2-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 103.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAIAAADJt1n/AAAACXBIWXMAAAsTAAALEwEAmpwYAAACSElEQVR42m2UyXbiMBBF/bOs+YjOIp2EJs0GGmx5kI1nY3mC/F1fSWByCJxCp5D06r0ahDOOw4MNQz+OPWvXda7rtu2JzX7orLE/m/MU3HWqqsrddiuEKIpcde0dM/Y3gsH5iZymsWnqw34vpbxczvhlWfzk0ODvMrSwviP2brcLw2Cc2EK7yrKUiJbtbtPocGyt61VnHKXaLE1RqwMNfZ5nkAOeMaP1Z7BBXsFWeVmVhMCapjmfJ65O52m2q2xUqZvhA66qSspov98J4eEEYeAJz1JNM60tGCKVOn3Hs9IepJ5ODSpYufOsnRp8msEgZ+VUC7Vkjn+56Xys9g3cWqQGm0mA2Rdi9fFBn+7VegCjELPktmYg0RnH8Wr1Z7PZlEVBzaZpsO18JpvMb5rhkVH0+/X1/f3t5eWXjGVdV3pCbTpPc+57NZhjLvm+OBwO2+12sVgsl0sLexySK5iEOe7N8TBM5/Hr60IuECJhvV6f2uZhwq7MbdvonG9NZhcMRWawkiTGqes6TY9wzFM8p8CEacGmyHwVj5H3dDwmKD+CjklZCk9APk5Xwrn4jr6UJnmRJ0lSN3VeZFVd0Sf4TRdafPtCbSK5/mT8ZN/5+/m53/+LZMQMwiJ83/NcInqe/SkYTxlHjCrREyOEf4gg8KMo1Mx8wsDnEkizijAKAWBhxIGfHo+8UIIaDNPOPR/fIYDrHoRwfYNnjWTIKkw67ADjjrEIpjDUnH4ghO85OpCJiqBQajZCZnlK8Wjh/Nrvz94MIkdKtf8B105TUpvxmDYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.2.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/77b6f43b5aff418271889ebaa2fa97ed/e1031/figure3-2-1.png\"\n        srcset=\"/CR4-DL/static/77b6f43b5aff418271889ebaa2fa97ed/63868/figure3-2-1.png 250w,\n/CR4-DL/static/77b6f43b5aff418271889ebaa2fa97ed/0b533/figure3-2-1.png 500w,\n/CR4-DL/static/77b6f43b5aff418271889ebaa2fa97ed/e1031/figure3-2-1.png 803w\"\n        sizes=\"(max-width: 803px) 100vw, 803px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>E.g. For lights of complementary colors, mixing them produces achromatic/white light because they’re on opposite sides of the color circle and the line crosses the center point.</li>\n<li>If three colors are mixed, then the resulting color lies on the triangular plane connecting A, B, and C.</li>\n<li>E.g. If red, blue, and green are mixed, then we can produce most of the colors in the color space by varying their relative amounts. Exceptions include highly saturated yellows, purples, and blue-greens.</li>\n<li>Metamers: pairs of lights or surfaces that look the same but have different physical spectra.</li>\n<li>The fact that just three lights can produce almost any hue is important for color technology.</li>\n</ul>\n</li>\n<li>Color blindness\n<ul>\n<li>This is a condition where a person can’t discriminate among certain colors.</li>\n<li>Review of trichromats, dichromat, and monochromats.</li>\n<li>Three types of dichromats\n<ul>\n<li>Protanopia: red/green color blind with a neutral point of 492 nm.</li>\n<li>Deuteranopia: red/green color blind with a neutral point of 498 nm.</li>\n<li>Tritanopia: blue/yellow color blind with a neutral point of 570 nm.</li>\n</ul>\n</li>\n<li>Why do these particular forms of color deficiencies exist and no others? We’ll find the answer in physiology.</li>\n</ul>\n</li>\n<li>Color afterimages\n<ul>\n<li>There are aftereffects when viewing highly saturated colors for long periods.</li>\n<li>E.g. Each hue produces its complementary hue in the afterimage such as green with red, black with white, and yellow with blue.</li>\n<li>To determine the complement of a color, just stare at a highly saturated patch of it and then look at a sheet of white paper. Your visual system automatically gives you the answer.</li>\n</ul>\n</li>\n<li>Simultaneous color contrast\n<ul>\n<li>When two regions of identical spectra are perceived differently due to the spectra of surrounding regions.</li>\n</ul>\n</li>\n<li>Chromatic adaptation\n<ul>\n<li>Since the visual system adapts to both light and dark conditions, it isn’t surprising that it also adapts to chromatic conditions (prolonged exposure to color).</li>\n<li>Chromatic adaptation is eye-specific.</li>\n<li>There’s a close relation between adaptation and aftereffect as chromatic adaptation causes chromatic aftereffects of the complementary hue. However, they aren’t the same.</li>\n<li>Adaptation is the lowering of sensitivity after prolonged stimulation, while aftereffect is the experience of an opposite perception after prolonged stimulation.</li>\n</ul>\n</li>\n<li>Trichromatic theory\n<ul>\n<li>Hypothesized that there are three types of color receptors that produce the psychologically primary color sensations of red, green, and blue.</li>\n<li>All other colors are combinations of these primaries.</li>\n<li>The three types of receptors respond differently to different wavelengths of photons.</li>\n<li>E.g. Short wavelengths for blue, medium wavelengths for green, and long wavelengths for red.</li>\n<li>Each receptor’s sensitivity to wavelength overlaps and any given wavelength stimulates the three-receptor system to different degrees.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 813px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/998b1a7e104fab7c37e94fde877c6fb7/baaa6/figure3-2-8.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 108%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAIAAABPIytRAAAACXBIWXMAAAsTAAALEwEAmpwYAAADbUlEQVR42lVUaY/bNhT0D02ytlOncJNPm6BAi6It2q69G9+2JIrUSZG6SB0+ZO/f61DbTRHhQSBtDee9mfc4uF4vl8vpduukFMPhcDQaD4fj0WiE9d3d3dt37968eTsev18sF1yI1Wa7Pxx0patK1Y0eGHB3Br6qdRD4fhi6zHMpIS6xiWsTajnEcWmciCTN40TKPC+00rV+BYP5+ZqmYjqd/vTx03T6aTL5MMbzfvLD5MfJh+mvv/25PTgsCFebXSyEquuqqf5n7q6Xtm0oo5QxmxBCqRdGzA+Yj1wiL4xjmSVZ5sdxJESmVNU0dVMb8LnrmbP0hRZsX37+5fHr6mE2n83n/zzM/354ItTDcevt3nZZWpQG3LbfmLuqrg6WFUZRzBNQUT+gzDNbIUMuUG2SZthjm5XqO/Dz860o8vv7z49gXG+fFmvE7PER8tqE7S1iykgEtLZdmqvvwddbp3S53mwC841MswIvRBibLFgQG3IhWRAgI2gGJPADONx1l8v1AsTvf/yFs4nr+UGEnBHEZbYDtygNwjARSGB3sPwortujYQbY+Nx1aV6gBWIuERFPUDoK9EOOYH4YS+lH/GATFKybVtf1t7QRHXSazZ9A6vmhH5iAVS5F+GiV7cFiYeRQBtkKrXurGgPuemYN24/H7gbpYV536rrj+YxoT+f6eDw4hMu0OZ1zpXOte+Ye3Kul0HkiTdM8R/Fo48SoVmLBBX7PQQjxIi6A7MHVf2kDnOWZ7TiMIWtjr2XbtuNCNstx0OrIn1AG23aWnVca5Pqlw26vTVKUiouk7g1ESUis7BlKrWCsGaK2Kav6lfm15rMZjNtyub7//MUwuu5uv99stwiksNpslusNOrzQlSlY6aKqcGiDmkF7Pp/A7BB3NpvvdkjNRp9+XSyWq5U5YrdbLJcuYxBZN7XqkS9vMHemS7oLeltKibFF8AQ+w+lYptILAriIKuAFFEaYI2pENQAAGqMwgUHVqlSqUKW5K/C31lmRZ0WB4vOiSCRmIoUpqVkbXwZGV8cB0nZsUOFhnodexAgHUYQ1Esb1whOONabdpeju0HGIHwZgFpzzLM/BnyBlKRCYxAhzb8aTez4+hsdYeKgFATZcGzB1kGUp5gc+G5hpDYkjYER7bKEn3tACW9SMBW4fvNHdKERp9S/Q4TmH7pLQsQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.2.8\"\n        title=\"\"\n        src=\"/CR4-DL/static/998b1a7e104fab7c37e94fde877c6fb7/baaa6/figure3-2-8.png\"\n        srcset=\"/CR4-DL/static/998b1a7e104fab7c37e94fde877c6fb7/63868/figure3-2-8.png 250w,\n/CR4-DL/static/998b1a7e104fab7c37e94fde877c6fb7/0b533/figure3-2-8.png 500w,\n/CR4-DL/static/998b1a7e104fab7c37e94fde877c6fb7/baaa6/figure3-2-8.png 813w\"\n        sizes=\"(max-width: 813px) 100vw, 813px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>Thus, the pattern of activation across the three receptor types determines the perceived color.</li>\n<li>This theory accounts for many important phenomena of color vision.</li>\n<li>E.g. The three dimensions of color space match the existence of the three types of receptors. Metamers are explained as the same pattern of activation produced by many physically distinct combinations of wavelengths. Color blindness is explained as the absence of one of the receptor types.</li>\n</ul>\n</li>\n<li>Opponent process theory\n<ul>\n<li>Although the trichromatic theory explains many phenomena, it doesn’t explain the nature of people’s subjective color experiences.</li>\n<li>E.g. Why are color experiences always lost in certain pairs? Such as red and green, or blue and yellow.</li>\n<li>Colors are never lost independently nor are they lost in other pairings such as red and blue, or green and yellow.</li>\n<li>Another piece of evidence against the trichromatic theory is that colors other than the primary colors (red, green, blue) don’t subjectively look like a combination of the primary colors, but instead look primary themselves.</li>\n<li>E.g. Yellow doesn’t look like a mixture of red and green, it looks like yellow. Yellow seems to be psychologically just as “primary” as red, green, and blue.</li>\n<li>Furthermore, evidence for polar opposites among colors comes from color afterimages and induced colors.</li>\n<li>From this, we hypothesize that there are four chromatic primaries rather than three and that they’re structured in pairs of opposites: red versus green, and blue versus yellow.</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 807px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/043524d5b92362f4ed1ee2bbb568953e/d2a60/figure3-2-10.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.80000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACBUlEQVR42jVSyW7bMBTURzWn9H8atEWBAE16S9EUyK1wYseyZUkUSW0ktXOTnP5cRw76QAjUaObNWxQ4N41D1zTSOmPt5Jz2Xnun20a2bWOMBuKcmWfbtlLrERyjxwvNBM5Obav2+91m84dSAsGy+GFoOaNKiqoq4/ikGuVnn+f51y+fn55+5wWv62qeXWDtOAzd7e23j9fXVV0XZWGs7vo+io4/7u/DcC+l0EYbayhjVx+ubm4+SSXruka61RnFbLcvj78em6Ypq0qjVms4Z3ff7xhjSinrLKj4+vDwE4VIKXH3q7MZ0eE8+/N5mRfv/No62DNiWZa387zMeAUOwnJe3v6+Lf+ZqzPNyOZ5s9vt0jSFLcS1qPfvEYbjNMJWNc3uFZTX55ct5gIlMgazN+gqPIC2Z4za1dYN43g4HsMwPMXxpCcUgbajKDocDkD6oUc6D+eqLISoxrFTStRC4AOabLseJfTDGnAGiIPZYJDI9X7argkwKngSkm63uyRJec6TNEkJKcsqjhNCSJ4XlFHGeVEUSZJgYVVdMc54zgLsNsvA4BkhUB6jKMsyMDKa5TwXQnDIygITRgqer0FphixFkQeExBAn8YlRCsOMUpCgR+6UpNgnAIhxhzP2d4pPmCs4KDZI0xiQVKLr2uHSW9d3OGh1wO8zDkDwvCBTfwnQhpXa/wN74NLDJn56bwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.2.10\"\n        title=\"\"\n        src=\"/CR4-DL/static/043524d5b92362f4ed1ee2bbb568953e/d2a60/figure3-2-10.png\"\n        srcset=\"/CR4-DL/static/043524d5b92362f4ed1ee2bbb568953e/63868/figure3-2-10.png 250w,\n/CR4-DL/static/043524d5b92362f4ed1ee2bbb568953e/0b533/figure3-2-10.png 500w,\n/CR4-DL/static/043524d5b92362f4ed1ee2bbb568953e/d2a60/figure3-2-10.png 807w\"\n        sizes=\"(max-width: 807px) 100vw, 807px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Dual process theory\n<ul>\n<li>We can combine the trichromatic and opponent process theories into a two-stage theory of color vision called dual process theory.</li>\n<li>Both theories are correct, but for different stages of visual processing.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 810px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/8c66952c0ffc2c533d760dbe8f34ff8d/d7542/figure3-2-11.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 164.39999999999998%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAhCAIAAABWXBxEAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEJ0lEQVR42nWVy1LcOBSG+70msAhVkxVbsmLBInmIJJUhE6aYpAl98U2+tW/tti3LNjB5wHySOjRkZqhTKmHp13/Ofy49a7sGa9rGbqptWe+2fuDfLm5l36lBYnaTZYlS3TD2ozY1TcPsADZ413OSNPEDb34zb2W3a3YcdX0HvmnqYZB7Aw+4kwbctW3XdrL1hR8n8SZLl8ul7OXHPz5++PB+up/YD4Mahh6YMQX5jOetNV0nlXScdSCCvMiFEPP5/NWr31+8+I0QHr4/qKFXGqyGccA0s0V2WC+l6h3XgRnPXc+9uLg4Ojo6Pj4+OTnBBS6M0A3/A8bKbRWnydoh8vT67+ssz798/eK47nK14rkJlTQYn8dxGn8F94PaoZD5uHadJEmWq2W1re7u76XsLOdP5vEQswXvn1A9BibLs7IqtVr7UNUTMGo/wWBBKHCV29PdFIbi6q8rkgSJUr0IhRFsGI1Nh5ifgIm2k/Lhn4fPV5/Pz88B39zMdZ2ovdSjRj8Hs6I2tQV4sVwWZfHm7ZuTly+jKLq+vs7znGwjtUKtSfu/V9tyavCgVusVqQqjkAo7PT0lyZ/+/ETmbheLuqlhsw5bO4ClEUkYt8MwvLy8PDs7e/367N37dyRpsVgUZX53Nz4iJyOYKQ+rsC4DRXnBjNSu65CetbOqqnJbb6lZDbZ4s3mWKmmMDbFR6r7vRXG0dta02t09ge6bSTv/6HZruoIN5fUfCR+op6Hvu022MWrrf63sTxqjbSjsqt6SrbbTT9S7Glpkj5OIbqH50jSxePvEIVV1sxNRCDmFHQjhB8F2V8NmBwMxE38g/J+Npe1XcFbki9Wybhr4002Kb1m2YaMjr0r48Z9+tk8cwMwDuocVqRkgrJDTIUkSNe0OqTeblGw90v4b7BA5PqM2oSI1gpVVgc7T/cicQXZbXo+TpLVgOjFO47wsqC06IS8KvEWzqiqmqWduARa4reQz5sbkiZg936OlXE+vG2I1/YgLJBkeKds0jdVhBj4HozOpiuKYDfEjHiXNMEDtsqq2dVkU2dMBOgNpRyfg0ASJ5nqSpcnNt29k7uvNnFYhYUxlSlIP0Edm+sG2lO0Nq58dRqQHzHK9os6ZDXzUGWKGKm1sZvhWmLrnBlUBBs/thtUaKcBwzcx2M/AgArxYLpiSZJX56PlMfIau5/qeLwKiYI2i0OacU1o1Nn8kIi+yWZLGRMg9goySmCTT+oQtgJhQmYF0OF+oOTJPFkB6nhfH0UyEAXrycGTwZIt7mjnweRHNodWvCB8vONqYaqVOYZ1xienBMfKweoGPwa+ZY+0I3nKBlb3m5FyACbTbegdJRHgcByAtA7FEiY5ZvxUKtMA1YTznmtATOpuFkdC34wgeHXnCLxUxc0l7y1UuhGbPbXyu69r+nsi++wFxPdwqnwBMegAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.2.11\"\n        title=\"\"\n        src=\"/CR4-DL/static/8c66952c0ffc2c533d760dbe8f34ff8d/d7542/figure3-2-11.png\"\n        srcset=\"/CR4-DL/static/8c66952c0ffc2c533d760dbe8f34ff8d/63868/figure3-2-11.png 250w,\n/CR4-DL/static/8c66952c0ffc2c533d760dbe8f34ff8d/0b533/figure3-2-11.png 500w,\n/CR4-DL/static/8c66952c0ffc2c533d760dbe8f34ff8d/d7542/figure3-2-11.png 810w\"\n        sizes=\"(max-width: 810px) 100vw, 810px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>Both stages of the dual process theory are now known to occur in the retina but importantly, the theorizing was done before the relevant physiology was known.</li>\n<li>This was, in part, due to technological limitations as it was easier to perform behavioral experiments than to dissect the retina.</li>\n<li>Another factor is that it’s generally easier to work from the abstract functional level downward to the physical implementation than in the reverse direction.</li>\n</ul>\n</li>\n<li>As predicted by the trichromatic theory, there are three types of cones in the retina that each contain a different light-absorbing pigment.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 811px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/b34cacfd3c7aa45afd8a94f667b3157c/fd28b/figure3-2-13.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 98.39999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAACnUlEQVR42m2T3XKkIBCFff+LzcUmmXEQFcRfREVxZvbt9kM3mdqqWF2UIqf79OlDEvYthBj7HrZtXVc/zZPIMtZ1W0fnLteryMRNiFKVdrTTMrvJnZHs993aoe87rdTiFwB+XbXSi5+XZZmWxdRNqdRAmmkCQFI3T//A98e9MtVgh44EfXfkskIIPifvrXO/3t5+v79Xde3mGdgU1xi8JI/nAz7A/Or7Ydj2wDuHVFUN02SaplAqy3Nt6pHKYJblXIlYuSiKvu8hDBNKDdauIZi2rbsOMAGyJeUwzN6fsBc4L/K262COHuCh4LeN5ipjwNRt21t74n8AU5m8pjajG+l5DRvBP5FJU9f0hlhN21Hgm/ALbIyhnS1sHAoMjJGFDa0zmdd1E0JgCm1L9uYHsK4q2DIYmId9Z1zQXiPzqeuHxXuG17Qt4Lb/j/kJ1nQLmPX55wk+1o9ZVo9toLIHatLzMI7sfOMPsFbToRMK1Q3CtE3bVFXFhywUzBUnqqrUMSpD+x1ji2CyIhIrQW+ZlIyKwKH9OH5eU8imN6GNuaRpestAU/lsPonePpBfsdMhHrSj6+3IkKCKZ6JXV9xL//5F+/G8E/fHfsT9TIEKVM4yCZGyxPWKXrao4qHC4X/SJUw43pXD68yZrUP52TlH8XmJfnbxOQ8c9yn+wk8u+fj84BpgMpnnZam6rqMiAhVlyQ6ySSkRjMuDWFA49gsMXyqdfAmL0KYoC2D8grPMJRGPaX0TNxKT4HK9CSkvVxQUssgT7MWtwqE6zkPzAgiMjikNr1QDUMSaqtRaFgUsWDmc4F44p7dURSZKUTnPo05K83lWZge22dFYKgQ3lIBVgivQljo1V4+cxoAHkEUCkcKRMYpPOjCw4G4yv97av3EFF+eriZuOAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.2.13\"\n        title=\"\"\n        src=\"/CR4-DL/static/b34cacfd3c7aa45afd8a94f667b3157c/fd28b/figure3-2-13.png\"\n        srcset=\"/CR4-DL/static/b34cacfd3c7aa45afd8a94f667b3157c/63868/figure3-2-13.png 250w,\n/CR4-DL/static/b34cacfd3c7aa45afd8a94f667b3157c/0b533/figure3-2-13.png 500w,\n/CR4-DL/static/b34cacfd3c7aa45afd8a94f667b3157c/fd28b/figure3-2-13.png 811w\"\n        sizes=\"(max-width: 811px) 100vw, 811px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>E.g. Short-wavelength (S), medium-wavelength (M), and long-wavelength (L) cones.</li>\n<li>The ratio of L to M to S cones appears to be nonuniform at about 10:5:1 with the receptors in the central 0.1 degree of the fovea being almost exclusively M and L cones.</li>\n<li>These three cone types, sometimes misleadingly called blue, green, and red cones, work as predicted by the trichromatic theory and explains some cases of color blindness.</li>\n<li>E.g. Protanopes are missing L cones, deuteranopes missing M cones, and tritanopes missing S cones.</li>\n<li>Responses in the LGN of macaque monkeys were incompatible with the trichromatic theory but conformed to the opponent process theory.</li>\n<li>E.g. Some cells were excited by red light and inhibited by green light and vice versa. Same applies to blue and yellow, and to light and dark.</li>\n<li>Further research has found this pattern of response in the bipolar and ganglion cells of the retina.</li>\n<li>How are the opponent responses of these cells derived from the outputs of the three cones system? We don’t know with certainty.</li>\n<li>Although the dual process theory is elegant and accounts for most color phenomena, it isn’t perfect.</li>\n<li>E.g. It isn’t clear why the shortest wavelengths of the spectrum (violet) appear reddish.</li>\n<li>The two initial steps in color processing are an example of how an information processing system starts with one representation and transforms it into another representation.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 560px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/c7ee3535565f5bbe59989927f3db024b/b06ae/figure3-2-16.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 151.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAeCAIAAACjcKk8AAAACXBIWXMAAAsTAAALEwEAmpwYAAADfUlEQVR42n2V6ZLjKBCE9f6/5tixbOsEdAsJdNk9bzcfsHK7N2JWUcYYkWTWQTk6jn3f13VbnO2rXUzXtlqPdVP1QzeMfHrs1+WilLTWzPM0m2map8lMEeB13wJ4c2A76vF4PvKy0JNettUs1q5L03VVXc/WaIeetbdoB7ytwbZ9AwzbfhxSVaPWnGvWxSxmnHTbdfNiJ2Mma9xoTGAO4I2DjLX9MByPvet7syxdP7Rdv+57P46AUeGQpzlmCIMxX9YF5uNx1DxNU5QCcDiFAKAfzPw3MLL7vn9+PNu2LcoC5egXqqqb1oPX2YPnvzEDnnHJBwYVeDFOU1W3/agBv5TPHry9wMHQWyBXyDTN7vckz4t7ksXxjQh58DJb8O6IyBNugT/I7vqOCTHD5zRLSRsk37//HHRgXgJ+DuBT8+nz0D+eD2KGeFVVjGSoIWJakzYwJ/kp2yM/mXEVIzeXS3y73aumFkpRFR5mAq1jJs+uQt/ABIwi+/btxyWOkzRp2hYj5oBPn8+ABXCwx+NY1wUkWqAl8ozH48E7Ys5uqtWubz7/B7yslGePwx+/P0KqmaOoo7ysWR3Yerz9N1X7W554XdVV03AROqIlpYK8aRtVKVJlVmvOIvkS7Re4biinBmMilcrynPHnP78ceLHz1/L8AubgkGdsGEdqJcszdmd54aN9go0JVxK27XUlAxhP7LJQYdTJRs1uWzcMX8D+YkfvggOYDBNn0lMKSQPBwLR995bnT+b9BX4VCeA4jouypLDJNs2AEPwf+OUz5cmEK0Womq49Pp40A8D0Lbsun1eSTkKe4MFestnHiM+DHuk+epomv8h38Dk47JiBBc3+CDe6zsjDfTa0ron7wDg66BkqczZAqvd6u3KwkKIUJbVR15V7apdk3CbgNJLws24bWVWqrgohuPIR62SSMpRUU6WElGVZsiIUv9U9TWjXWZFneUrObzSHNGUkHHlZRgCkklx6UULsP6K8J4msVCFKXuUF5zITdPL4GmdFcUsSDy4i6RhkkO225rnnFzSjFL6yYB+uMUEFxwHmil/vd46LHJkUhTteNq7X4o7wGnghEFn4DWmWIR5HcAdd9zTF+Shs9eLdI53rkiNgI0hcLKJQuGLJob3SVtIE2amLUxGx6qVWDqUUzsPBIjx61pOZxwkb6RD8aXR0YjqU1v4/oP8DJKGvpAzg+2UAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.2.16\"\n        title=\"\"\n        src=\"/CR4-DL/static/c7ee3535565f5bbe59989927f3db024b/b06ae/figure3-2-16.png\"\n        srcset=\"/CR4-DL/static/c7ee3535565f5bbe59989927f3db024b/63868/figure3-2-16.png 250w,\n/CR4-DL/static/c7ee3535565f5bbe59989927f3db024b/0b533/figure3-2-16.png 500w,\n/CR4-DL/static/c7ee3535565f5bbe59989927f3db024b/b06ae/figure3-2-16.png 560w\"\n        sizes=\"(max-width: 560px) 100vw, 560px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Assuming that there’s a later cortical representation of color in terms of hue, saturation, and lightness, the red/green and blue/yellow axes must be further transformed.</li>\n<li>The effect of these transformations is to reparametrize the 3D color space so that different, evolutionary more useful, information is made explicit in successively higher levels of representation.</li>\n<li>Reparameterization: the process of changing the variables that directly control a system’s behavior.</li>\n<li>E.g. Your bathroom faucet transforms the 2D control system of hot or cold water and how much into one control function, like back and forth for volume and left or right for temperature.</li>\n<li>Why does the visual system use three different color representations?</li>\n<li>E.g. Hue, saturation, and lightness.</li>\n<li>Because different representations provide information that’s useful for different purposes.</li>\n<li>To understand brightness contrast and simultaneous color contrast effects, we need some kind of spatial interaction between neighboring regions of the retina.</li>\n<li>Lateral inhibition: when neurons are spatially organized to inhibit neighboring neurons.</li>\n<li>Interestingly, a lateral inhibitory network has a pattern of outputs similar to the Mach band phenomenon.</li>\n<li>Review of projective and receptive field.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 817px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/0ef0d9125ea2350f2ab3216009a0f2f2/98314/figure3-2-18.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96.39999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC50lEQVR42j2TaXPaMBCG/WvTT81MM23STCAJbQKUXBwBG/Alyfch2xz5fX1s0np2hIS177G7No7HfdPUuiqLstCVruqq2+dVrTmWuszL3Jf+fDGzLHOxmK2sZRyHXCCMuuG2bnbNbr/L8lxKEcVRXuRSCRWoMI5m81mv3+/1eveD+8FgcH19/fD4sN6s0yw1OipdN/X+sI/imFBKoiKMIlDiNJnO5sPR6OamNxqNB79+92/v/jw9maaZZplx/NhDe1IImFQySZNOP174qYIwXJrmyrKenp9n8/nbdLrZbpMsK7Q2Dsc9nASyQQnCIMtSrcuiKHBRah0nycoyVRCQo8LA830pla7rU/KBtFMAYds2pfqHtaubpmoaLNiOjQTbcTzhZxS00p/JBLePH8cwCikJfuq2/jrvyFmDKDLX663ttORBoOuqrKo2mTSM4XM+n19dXfX7/curn8PRME7ioiwpBMqBINlcWyvTlEpRmk/PiMQe1f96fv7t4uL29u77j8uzL2f0RkhpWtbr6xvo78vVcDR+fnnJy6LeNVmRx2lKn2sa4/neeDzu9fqTyaTXv314fBRCVE2dpCkQu/2eDYXCdsHYFAXMaZ4bJ8MEgxGGIfppL0H/0YxyaGkpyeTkLWECeY6dqjKOXcGoVmvMsqbT6fvyfbFYYOyUT3kYNdo7mTzR6tfpGzVHOQVnSMg8HKj28UCFiw4eL/9biCiO0NJwFKECZhAJY7NdbzbrMFRS+J7nQoh4RjNJkiAIcKGU4sxKkxhe2Z4DX7X+DSGF4zhAoo09CVzyhWAqpJQU0he+4zqOa/Op+L7ndAdmzpfCYDCYZ7pKVT8hhOAfcvDGysBydF0HaNfzNtsN0I7n8cEZpPGu5eRSR8URCB72KmTlsg80ithDLIDm05XSgI3b/MWKW1BObB2W6Njck/hWv+c6rouirWPzyog7qV20YK1k1ZongYerbabflsB13Y29xSqtpu1pnv0FBbzVLHR51S4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.2.18\"\n        title=\"\"\n        src=\"/CR4-DL/static/0ef0d9125ea2350f2ab3216009a0f2f2/98314/figure3-2-18.png\"\n        srcset=\"/CR4-DL/static/0ef0d9125ea2350f2ab3216009a0f2f2/63868/figure3-2-18.png 250w,\n/CR4-DL/static/0ef0d9125ea2350f2ab3216009a0f2f2/0b533/figure3-2-18.png 500w,\n/CR4-DL/static/0ef0d9125ea2350f2ab3216009a0f2f2/98314/figure3-2-18.png 817w\"\n        sizes=\"(max-width: 817px) 100vw, 817px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>For a 2D lateral inhibitory network, a center-surround organization emerges where cells respond maximally to a pattern of a bright spot activating its excitatory center and a dark ring surrounding the center.</li>\n<li>The ganglion cells of many mammalian retinas have a center-surround organization.</li>\n<li>The lateral inhibition theory explains the Hermann grid illusion as the intersections receive more inhibition causing the illusory dark spots.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 805px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/702f7233d26a751510d22f1e3788b1e2/c946b/figure3-2-21.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 52%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAACJ0lEQVR42iWQ228SQRTGNzzxwouNVjEKielLQSHGS9WQ2LTYy4O29rVF5NFyqU0IdVEoTcSCtKGwWG47w15AbrsLyy50pWBN/zEPYTIz+TLz/eZ8Z4h+v6co3cvRMJWMz809sFrmM2enzuWlO7dnfV4vSYbu37v77OmTfP7XwovnZpMpTB5cXV0CAiAxhUfjUSz2Ta/Xz9yYOf5xbLfZCILY3t4J+P06nc5sMlNU1mg0wqHP572+/gfIYKBMYFXtDS76tRofjRyGya8URUUikf39z5kMlcvlPB5PMBjkeZ78Qvp9gVKpOBxqqir3AVbVbk+RNU2jsun1tZXNzY1UKuV2u984l4+OYolEYnXF6XLtMCzz0e3aePsunT4djYaTegOVUJROVxaHoz9hMgSpDAZDMvnTarWC/uBy7e5+AmEymSD27K2boPf2AhC715PVvkJAdMigaRcnJ6lXLx0Ox+vz88LW+y3LvCUUOohGDx89tK+triOEFheX7LbH8e/x8d+xPIV5nqlW2Xa7CUMQxK7cqzca9XpD6nSazVa7LQiCBAKk1OmKYkeUpK7cgU5hJyjqDCKxHAM/kc3maBoXS6V8oYAwiDKNMMtyhUKRYdlKBTxlhoFi1drvGsexBMNghMoI0TBhFYolhDBYMa6AG55gwcVxGON8Pg8G+Dm+ykMXcD2BKxiBeQJPBirTNJQEGMENrpShPMLTXlqtliiJoiS0hZYotv8DBm090IWVGFYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.2.21\"\n        title=\"\"\n        src=\"/CR4-DL/static/702f7233d26a751510d22f1e3788b1e2/c946b/figure3-2-21.png\"\n        srcset=\"/CR4-DL/static/702f7233d26a751510d22f1e3788b1e2/63868/figure3-2-21.png 250w,\n/CR4-DL/static/702f7233d26a751510d22f1e3788b1e2/0b533/figure3-2-21.png 500w,\n/CR4-DL/static/702f7233d26a751510d22f1e3788b1e2/c946b/figure3-2-21.png 805w\"\n        sizes=\"(max-width: 805px) 100vw, 805px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>However, lateral inhibition doesn’t explain why the same colored shape on a black or white background appears darker or lighter respectively.</li>\n<li>Adaptation reduces sensitivity to light after prolonged exposure to an unchanging stimulus by temporarily depleting resources in the visual system. The reverse happens in dark environments.</li>\n<li>Over time, the activity of the visual system shifts away from the perception of the adapting stimulus.</li>\n<li>Double opponent cells: cells in the visual cortex that have an opponent center-surround organization and provide the basis for simultaneous color contrast effects.</li>\n<li>E.g. A red/green double opponent cell would be excitatory to red and inhibitory to green in the center, and excitatory to green and inhibitory to red in the surround.</li>\n<li>This applies to red/green, blue/yellow, and white/black opponent cells.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 671px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/2ab0b894ae21f02ec02bf7b0209cf139/d0e73/figure3-2-24.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 107.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAIAAADJt1n/AAAACXBIWXMAAAsTAAALEwEAmpwYAAADKElEQVR42j1Ui3baOBTkj7fdPWfbbWHzaEnSpikNhQB+6C3LNrYkP2iyf7dzgUZHB2TrXt2ZuSNPxnEYh74fuq6LtrBSKcy62Ycu+hhOk3GeM5bluda69S1m0zb7ppmMhxGzH/rDYYwxaK2UVm3wvusa77FofNj74KrKVXW1p0Mp2eO9p2SURqYx5vrT9fsP76ez6d2Xr+W+ofwQXd3kXHKphTJcyqIsfYxNCJgTgn0YQ4zzm5uLy0ukfZ7ffpxOH5c/fezaGDmIGJtxsU1z45xQqgLoQHCo8st/L1VdT2ezz/P5dpc8PCxmFxdf7u+xjThpjDJ28bh8XK1yxgtXlnUNUKhPyc8vz41vUXZ+M98lu/tv31D54fv3OAx161HZuvJps02yzLqicA7MY9+fk4H88OuwWq+n/87+ePPm7V9/Xl5dKWNCPwAeKkttwXm13tiylBA8xpZox7PaBxJ82CXp7d3Xh8UPqU3oesBGxN57cE4Z3ySpUJoageQQ2tfKw7H4r+dnW7g2xG4csX1UFcnBd70pHLD04wjAoevCK+fhLHgQAiUtFxJ/kBrbgboVuFQZ40xIbMEzoYd/4m/OwNz3Ukk4DE4CKwRZ57phwETBnHN6KSXjggsR+w7o2nBsFQDDNM655XL597t3l9dX2mh4BlsegsFyRidpun7awKdCybPPAjmM7IW2oweLxQJi//PhIxKU1pCw3u9RCsbYbHeQM8sJF/oMOufKGLGLgA1j397dwSdSK23tqZkMyVoh8+dqjRgUP2nZnu1JUo9t2+LgXZLkeW6tHQZSFflN60E1YwzJAH/E3J0FOxwFw8ACb8EV/PEwEBxqyUCjFwTM4Cb9zgzEGe1A0GniLVkPV7vv8QO14GGMgeTFTaDr+ZpJyRuMpycUBOA0y0xRUGOUQrfwiJaDssSvUmnOci4QCRRgjhZM0FuKIF8YKIwg9BNHYBtSQd/TNwScM85gFSwQkOYZl2ICLpx6oWEhUosx8omCTwS5QpBU0Amh2yTBBC6qLDhOnDDGAA+hZVWBML5MuNtQG4/oJyAUpcPEwtUV7IIvCda4m9qa/wHMwi285GgAXAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.2.24\"\n        title=\"\"\n        src=\"/CR4-DL/static/2ab0b894ae21f02ec02bf7b0209cf139/d0e73/figure3-2-24.png\"\n        srcset=\"/CR4-DL/static/2ab0b894ae21f02ec02bf7b0209cf139/63868/figure3-2-24.png 250w,\n/CR4-DL/static/2ab0b894ae21f02ec02bf7b0209cf139/0b533/figure3-2-24.png 500w,\n/CR4-DL/static/2ab0b894ae21f02ec02bf7b0209cf139/d0e73/figure3-2-24.png 671w\"\n        sizes=\"(max-width: 671px) 100vw, 671px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Double opponent cells explain why a gray circle on a highly saturated green background appears pinkish; its because the surrounding green region falls on the red/green double opponent cells, causing it to inhibit the center thus signaling red and giving the gray circle a reddish tint.</li>\n<li>Achromatopsia: inability to see color due to cortical damage and not retinal damage.</li>\n<li>Achromatopsia differs from color blindness because it isn’t caused by the lack of one or more cone types, instead the problem lies with the brain.</li>\n<li>Color anomia: inability to produce linguistic labels for colors.</li>\n<li>Color anomics can see colors perfectly well since they can discriminate and match them, but they can’t associate between perceived colors and their linguistic labels.</li>\n<li>Since color vision is genetically determined, how early do babies begin to perceive color?</li>\n<li>Evidence from preferential-looking-paradigm experiments suggest that two-month-old babies have normal trichromatic vision.</li>\n</ul>\n<p><strong>Section 3.3: Surface-Based Color Processing</strong></p>\n<ul>\n<li>Our image-based description of color processing is only a rudimentary beginning of an explanation of real-world color perception because it fails to account for what we perceive.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/783b899f7b669a2de67cbdf453882007/82c1e/figure3-3-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 55.60000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACQ0lEQVR42j2Sa08aURCG+S31W1GCQQUTIFARAqREBFOlXIoLsVJvFKTFWosRY2Njy3JZdrkuuyycve9C+/86mtrJfDnJPDPvvHMMvDBF/ERWJCyD+f3+10/x8POhWCwcHr7vD3o1/Fe5/LnewC8vvxBEiyTb87muarKmKwZe5JGAVF1LJpNGo3HRuBgIBO6+34VCIafDWS6Xt7e3V1dWU6lU7kOu1+9xU07TVUWVIQEWkMArmhKPvzWbzBAHBwd4veZ0OiORyP2Pe78/YHxp3NzcBEUgp9VqzuY6kCD2HywrcjabXTYvm0ymbDaDN/D19fXY3h7VIePxOEx2uVypd6l8/gzE//4zB1JRn2FVU9PptNVqtdlsdru9entzenoSDAZvqjdtsl29rR4dH2EYVql8K5XOARYlUVHEZ1jXQNWKxeJ2u4GHnSvXFa/Xm8vlGs16JoOVPpUcdsdZ/ozqUqBZlARJFh5h/mkyWAK7ra2thcNhot2KRqM7OztXV1+L50WPx7O/n4Zb5D/mwXZFA5gXJfTfMDUaiS68WDAtLfl8vhbRDD/F9XWlUCxYLJat8FYsFru4uCgUCrO5JohTQUQGQRJFWdRnj6eyWW0brzZ2d9+MGPrk5DiRSICEGl6DXeByAON1nJuMwSpJhsm8odPtDOghM2aG9HDEjCAZZgQVLMtMphwvoDHH0vRwMOxDATtm4QnJsCN6NDC0CAL8bFMkQbahEXwDSJIiO10KWiA0hS7QAvFoijgOXmOGHUMPGEP/BZqBSlbxUkSkAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.3.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/783b899f7b669a2de67cbdf453882007/00d43/figure3-3-1.png\"\n        srcset=\"/CR4-DL/static/783b899f7b669a2de67cbdf453882007/63868/figure3-3-1.png 250w,\n/CR4-DL/static/783b899f7b669a2de67cbdf453882007/0b533/figure3-3-1.png 500w,\n/CR4-DL/static/783b899f7b669a2de67cbdf453882007/00d43/figure3-3-1.png 1000w,\n/CR4-DL/static/783b899f7b669a2de67cbdf453882007/82c1e/figure3-3-1.png 1398w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The “color” of a surface is a psychological property attributed to an external object and not a physical property of surfaces or light.</li>\n<li>The physical attribute that mainly determines the color of a surface is its reflectance spectrum.</li>\n<li>Reflectance spectrum: the percentage of incident light reflected at each wavelength.</li>\n<li>The reflectance spectrum (reflectance) of a surface is invariant because it doesn’t change under different lighting or viewing conditions. It always reflects the same proportion of light at each wavelength independent of the illumination.</li>\n<li>Luminance spectrum: the light that falls on the retina.</li>\n<li>The problem the visual system faces in perceiving surface color is that the luminance spectrum is determined both by the reflectance spectrum of the surface and the illumination spectrum of the light that strikes the surface.</li>\n<li>E.g. A red surface seen under a red light looks very different when seen under a green light.</li>\n<li>This is known as the color constancy problem.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/b0cebf0755be7b49431a3c963de0d4bd/5d6a0/figure3-3-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABi0lEQVR42j2SyW7kMAxE/f/H6Rw6M93Wbu0LRTvfNyUniFEwBIFPRZa0XV98fZ3Q5AmNSc/n0xhzXiyEeH5+Dhop58fjw4VQek+1tt76rQ1FEF8Mkpljzu443OFDSj4E7Zw9vPNBWaesBZ9ba2PUMdroG583dk6aE7uvfd+F8DFKpeGPahfiv/du3RFyyXBurRM1ojpoI3TKdMNEzPA8QvApSWO89zEl8DgHjViPtkdufcBmwWPDKXMCZjhDxtpdyFwrek4pldZiKSGmvBZ1wR0wLfMFzw7n80JgjBBe711pnUqRSkqlMG2IEaNo65bzgHMDPIj6j/MPvJztajWjAkzKBfEiQizh49d+L7fzL9zvwBid1N7/vt5/Hh8hZalXYCARAWbB4ia/ZyaisWA+6TwZwkeT4XzEGHK2zikljUPKaB1XlJ2PgOFMTEh3wTH5UtFmhXDv7VetppxiirggvJPee6l1aZWuBWbahHhLJbRRSuMVwMniD2mt+mgI2Bj9LRRASBGJCimFlP8BlsC4LZLibwcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.3.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/b0cebf0755be7b49431a3c963de0d4bd/00d43/figure3-3-2.png\"\n        srcset=\"/CR4-DL/static/b0cebf0755be7b49431a3c963de0d4bd/63868/figure3-3-2.png 250w,\n/CR4-DL/static/b0cebf0755be7b49431a3c963de0d4bd/0b533/figure3-3-2.png 500w,\n/CR4-DL/static/b0cebf0755be7b49431a3c963de0d4bd/00d43/figure3-3-2.png 1000w,\n/CR4-DL/static/b0cebf0755be7b49431a3c963de0d4bd/aa440/figure3-3-2.png 1500w,\n/CR4-DL/static/b0cebf0755be7b49431a3c963de0d4bd/5d6a0/figure3-3-2.png 1580w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The visual system solves the color constancy problem by somehow disentangling the effects of reflectance and illumination so that the invariant property of surface reflectance can be perceived despite changes in illumination.</li>\n<li>This is the inverse problem for color and once again, the problem is underconstrained because the eye receives information about only one known variable (luminance) that’s determined by two unknown variables (reflectance and illumination).</li>\n<li>To recover reflectance from luminance, we believe that the visual system uses additional sources of information, probably in the form of clever heuristics and assumptions, to accomplish this task.</li>\n<li>Color constancy is a special case of perceptual constancy.</li>\n<li>Perceptual constancy: the ability to perceive the properties of environmental objects in spite of changing environmental conditions.</li>\n<li>Lightness constancy: perception of a achromatic surface as having the same surface lightness regardless of differences in illumination or viewing conditions.</li>\n<li>Adaptational processes are involved in lightness constancy, but it can’t account for all of it.</li>\n<li>Adaptation takes a relatively long time whereas lightness constancy is virtually immediate.</li>\n<li>E.g. If you’re reading a book and the light suddenly dims, your perception of the page doesn’t turn gray and then gradually lighten.</li>\n<li>One hypothesis is that perceived lightness depends on the relative luminance (contrast) between neighboring regions.</li>\n<li>The boundary between regions, the edge, determines the perceived relative lightness of the two regions. This then propagates across the entire region.</li>\n<li>Retinex theory: proposes that lightness depends on the global integration of locally determined luminance ratios at edges.</li>\n<li>No notes on the details of the retinex theory.</li>\n<li>Although the contrast between regions is useful, it doesn’t specify what scale to use.</li>\n<li>E.g. If a region reflects five times as much light as its background region, we don’t know whether the lighter region is white and its background is gray, or the lighter region is gray and its background is black.</li>\n<li>Scaling problem: how are luminance ratios mapped onto the white-to-black scale of achromatic lightness?</li>\n<li>The visual system appears to solve the scaling problem by using a simple anchoring heuristic by assuming the region with highest luminance is white and scaling all other regions accordingly.</li>\n<li>Experiments support this idea with darkish gray regions being perceived as white under specific conditions.</li>\n<li>Once again, this isn’t the full story though as regions with the highest luminance aren’t perceived as white, but instead as luminous or bright.</li>\n<li>Two kinds of edges\n<ul>\n<li>Reflectance edges: changes in image luminance caused by changes in the reflectance of two surfaces.\n<ul>\n<li>E.g. The edge between white and black paint.</li>\n</ul>\n</li>\n<li>Illumination edges: changes in image luminance caused by different amounts of light falling on the surface.\n<ul>\n<li>E.g. The edge between an object and a shadow.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 560px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/dc12f75a369a041f2d7e9affb215a121/b06ae/figure3-3-8.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 155.99999999999997%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAfCAIAAABoLHqZAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEN0lEQVR42lWVyVLjSBRF9VN8Ayyhmh0bIhrWZao3XQEBNlUMHmVbc2qyrVkeMIaI/r0+qXQDrXihSKfyvnvflNaKMs+LDJOLMi+r0rLN4XBw/6vTarXanbbt2FEcrdar5WqJsVC23qy1oiryUhqwelmvNquLy4vT09Pj4+Orq6tut9vpdP7++ZOjX5F7cFkV4Mu6SrJkPJ1Ae8Fzefm99T0v8t37WxiFD48PillhsP+Y60pZVhbjyWSkj378+Ovx6cnzRZKlVV05ngs5pzcvmw8wxs9PMFavllmR/fr9W0WhHphvbm4s28KvYRrIBPY/MLKrZV0ua/ivb26Go1E8n2V5xlHhi9vbW4ICnGXZ+mX9Qf7JDFJ58QN/pOtFWSRpcnt35wnR6/fSLHl7f3vZvqzWX2SXX8CQk5WiKikP4Pli/ufFxXO3O56MCfv6+poUgP+UrdQqJEbYFEzVbJEgVZ/NZ8PR0LSsfn9ACravW2AYXiTz3hQYGGXc7LO6e9sRc7fX9YTHvqL9BBdNqiTheqUWVMgPgqkxtR3HtEwWQRAgGL9KLbB9wuDMq3KeLFKZW5psWS0r6kTMZBujVb5yqg5Raw21iyxt33du23dwbnev6vOHwYkv3tKWFSnQx2MKIWXTD+Es9sMgjCN9MiY8PnBONdMiWQTNk2YZrtPG2IxnMQe0tMjjxZyAaWN6+Nu3P9rtDl1FnQDzzosC5S+vW+qsSv32vsvznKJqtBTNRLRkGFqGQh/rnBC+Dz/V4hyRo7zX6w2GA3SlKeSJYs6Qvd5u3v95f3p+Pjg4YCoM04zjuKprYLxhRu3h4eHZ2ZnZPLiQzEXN9NsAQLZaV0dHR/1Bf5EmgKkN4em6zhvwycnJ+fk5BXdcJ0n24IqcTQ2j3+/f39/3BwN5dcxiPFItwqZJACPSdh3X89hECDnH9X4wVhR9uyFa5onPveYhbaqwQRgA+CgYnID3zEXTJ3lZomdiTCkJlUDex9UBgCDlhDa0ypFsErJN6QVjOB4P9dForHOfPDw+4kh1C5pt2yao0Ygx0ZlqJlQ1mVbR1asl1RZhwI2DF7oFZMyN2XQyPKjAuEN5u5zxxX4kp3S+aQRx5AqPgpNnsoJIzI+CIArJM4LxRZyowBc/VRQaSHSCpGNh5t4wLBNmSk3C2We2HNeV6ZhOmlGzWTNtQRhqyLBcBwZ2LceGmXkkz2gjbUIIjpJtz5PKuaGiOJbvKGJT45uFa+HRj1RSvh3eEgMz6yAKWOOaB4AgbFeG7fu+xv0MrckXNAd+EEUybT670hdfRWOKrSmh2vGJRXOFQC0xuL7AxcQw+AAnLsgckTtSjmBNtmWqhWBCoSWFUjZbhmVJzUFAk8ifpnShskWwqIV5Mp2iLwxDdoiCvGhci1PTRDPicQEDfw4Q8pMbl6Pyr6OxTP6ZFjSZuqHKqvwXpQd9b5lAWhkAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.3.8\"\n        title=\"\"\n        src=\"/CR4-DL/static/dc12f75a369a041f2d7e9affb215a121/b06ae/figure3-3-8.png\"\n        srcset=\"/CR4-DL/static/dc12f75a369a041f2d7e9affb215a121/63868/figure3-3-8.png 250w,\n/CR4-DL/static/dc12f75a369a041f2d7e9affb215a121/0b533/figure3-3-8.png 500w,\n/CR4-DL/static/dc12f75a369a041f2d7e9affb215a121/b06ae/figure3-3-8.png 560w\"\n        sizes=\"(max-width: 560px) 100vw, 560px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>How does the visual system determine whether an edge is reflectance or illumination?</li>\n<li>One factor is if the edge is fuzzy or sharp. Illuminance edges due to shadows tend to be fuzzy and graded whereas reflectance edges tend to be sharp.</li>\n<li>Another factor is depth. If depth information says that two regions don’t like in the same plane, the edge tends to be perceived as an illumination edge.</li>\n<li>A third factor is the magnitude of the luminance ratios at the edge. Illumination edges can produce much greater changes in luminance than reflectance edges.</li>\n<li>Generally, if hue or saturation varies across an edge, it’s probably a reflectance edge. If only brightness varies, it’s probably an illumination edge.</li>\n<li>We can extend our facts and theories about achromatic constancy to colors.</li>\n<li>Chromatic color constancy: the perception of invariant properties of a surface’s spectral reflectance despite changes in illumination and viewing conditions.</li>\n<li>Objects appear to have the same color under different illumination, but different light sources can have substantially different illumination spectra, which causes different luminance spectra to the eye.</li>\n<li>Chromatic adaptation is believed to be important for color constancy.</li>\n<li>Human color constancy is remarkably good as we perceive mostly a surface’s reflectance spectrum even though the light entering the eye (luminance spectrum) is actually the product of its reflectance spectrum times the illuminance spectrum.</li>\n<li>The visual system is somehow able to separate out the wavelength information in the illuminating light from the wavelength information in the surface reflectance.</li>\n<li>Three constraints under natural viewing conditions\n<ul>\n<li>Consistencies in the illuminating light within a scene.\n<ul>\n<li>When the intensity of light is constant for parts of an image, it reduces the number of free parameters of the illuminance spectrum down to three for that part of the image.</li>\n<li>For this to work though, the visual system must somehow discriminate between illuminant and reflectance edges.</li>\n</ul>\n</li>\n<li>Restricted range of illumination spectra for normal illuminants.\n<ul>\n<li>We don’t encounter all possible light sources under normal viewing conditions.</li>\n<li>Color constancy tends to fail when there are strongly chromatic light sources, but we rarely encounter these outside of stage lighting and perceptual experiments.</li>\n</ul>\n</li>\n<li>Restricted range of reflectance spectra.\n<ul>\n<li>We also don’t encounter all possible surface reflectance spectra either.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>If color vision evolved to allow organisms to perceive the reflectance of surfaces as approximately constant over variations in normal daylight, then one method is to remove these variations by a compensatory internal process.</li>\n<li>E.g. The black/white, red/green, and blue/yellow structure of color vision may be an evolutionary adaptation to compensate for changes in normal daylight.</li>\n<li>But this conjecture is speculative and doesn’t have any evidence supporting it.</li>\n<li>We now revisit how the visual system distinguishes between illumination and reflectance edges.</li>\n<li>Illumination edges have similar chromatic aspects while reflectance edges have different chromatic aspects.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 648px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/3dbec0e1e7b2eea925b8e1cd654cb083/3996e/figure3-3-10.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 122.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAIAAAC+dZmEAAAACXBIWXMAAAsTAAALEwEAmpwYAAADBUlEQVR42oWU63LrKBCE/fwbO7ElgW4g0P2GZGf3BfdD+Lic/DmuKZmSpumhp4fTOA3Dn/DrsdeVkqmIk+QWRUIKnufzeRiHeZlXt7htDbFt7jTNb+B5nJZpmHjOw8RiGud5WpbXYnUe8w4ex5lvPrUfh7ppmq5rhyEAlK5MXfOmbls2Xbd137ftiPt9P5FBkDqvqzYmEYKn0ppsY2vAlbVxIs6XSyIlHPt9d/vmDvwTTCzOAYuFSLPcNFZpJaQEXOqq7bpQ3epcQIY4cZIj5nVzprYww0k25LDO69KPY5rn/3x88CQN5lA2ixcYtN+CYyNvqct+6Ju2qdumLFWp1ear3cNRX3E6ME8wsbi1UCor8rZrur7t/BZtURZuc++wH+CADLE/HrZpbhEyyc+vK6f4/u/beeIfyN9lB2aeXsz74yiVhuIM3nip/gJ+bUFLgREbOzz4wxvu95nf1PLRDUNeFFGSXD4/oziWWYr+19sNLY/2OPdukoAJPnkZhi063OwdOtIqFhyBKo6i3PrLYS9y78SmwW1t39On++O+rEuWYZzMWMv7GTAmvW/7u8NwOsLisLpp2b7EI8boStMzY01ltK09mE8c3rf8Dbwsm6usiZIYwkKV8FNzmkkw1Lw/vLzuKeH2QzA/bgeY8vATJlGVto19fN+brvm6XdEM8wZX/lYb8sPGAxkf53OW59pUNe7EoQ1/9bKtvtsH8/5iDvKGCLtco4gZANf1XT901hohEt/2wz/zYZjnmRGAQIlgCfB8gfnzek1Ekoi4H/vvf3FbsBr4FfCTWaSpzDwPA0wwCdwBFsX7nluJ2SKbmtrOF8KocjQcsMwTLTtdvr4+LhdVVbfDT+iEpQiUU7pE7bzMGUqZyjiJuQ8hqKpKqRIpTllRyCwrGcPcK4zURUl6wU0ApshzKYUHS6HpQp5xvZQF6coYcxIyvd2ivCyByRQn5YBBJmC0Iuu4jJSx7OU52ajyP13X9kQueORNswwAnAR9psI0S1nHIjm8WTEwLLibw9iu6/I/mC7CUqyAgdgAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.3.10\"\n        title=\"\"\n        src=\"/CR4-DL/static/3dbec0e1e7b2eea925b8e1cd654cb083/3996e/figure3-3-10.png\"\n        srcset=\"/CR4-DL/static/3dbec0e1e7b2eea925b8e1cd654cb083/63868/figure3-3-10.png 250w,\n/CR4-DL/static/3dbec0e1e7b2eea925b8e1cd654cb083/0b533/figure3-3-10.png 500w,\n/CR4-DL/static/3dbec0e1e7b2eea925b8e1cd654cb083/3996e/figure3-3-10.png 648w\"\n        sizes=\"(max-width: 648px) 100vw, 648px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>We aren’t certain, but maybe the visual system uses the red/green and blue/yellow opponent systems to compare the chromatic aspects of two regions. If they’re similar, this suggests a reflectance edge. If they’re different, an illumination edge.</li>\n<li>Color constancy doesn’t seem to be an innate ability because two-month-old infants looked at the same stimulus with different illumination for the same amount of time as they did a new stimulus.</li>\n<li>In contrast, four-month-old infants looking at the same stimulus with different illumination looked away as fast as the same stimulus with same illumination. This matches what we’d predict with color constancy.</li>\n</ul>\n<p><strong>Section 3.4: The Category-Based Stage</strong></p>\n<ul>\n<li>The processing of color isn’t complete when we can internally represent a colored surface.</li>\n<li>The next step is to classify colors and attach linguistic labels to them.</li>\n<li>E.g. Blood, tomatoes, and certain apples are all classified as red even though they produce unique hues, saturations, and lightnesses.</li>\n<li>People divide the continuous 3D space of color into discrete color categories.</li>\n<li>Evidence supports the view that the fundamental color categories are primarily determined by the physiology of the human visual system.</li>\n<li>How are colors named?</li>\n<li>The first breakthrough came from cross-cultural research on color naming.</li>\n<li>E.g. How people in different cultures with different languages apply labels to colors.</li>\n<li>Review of cultural relativism and the Sapir-Whorf hypothesis.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 810px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/86b1980f4fe2b5aba44f20aeedf3683d/d7542/figure3-4-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 82%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAACuklEQVR42iVTiZKqMBDkh3c9UEQIICqKyqncIeHyelXvrav7g69xQ2qqCOmZ7plG8Hzv4Dq7w87eb1zP4RVPs4xV3PV9lagqIUQjrudmeZ4XRZqlOJzJE2k2I5oi7Pd7a2OtN9Z2t/UCp6p5WbKuO+O2qpG5IusLMzwGJWNxkuZFPpvPRXEsiqJmGMLusF9b1mq92tg2WDDGm7a73e8ooi90wzQXphmEQRRHab+SyXQ6GI4UdY7UAqgGp9ALfNd34jS+3e7g13bnKEmJToiugafre7yuaFnSkipEmUylqSSqRAHYC44BwI7vJGny+H5+PR7P14sypgGpaYZpnKL4dIo830/zTJKlz8/BaDQgmipY1ma5Xi7XK2hGIsY55LVt6wchiC3MBXaa57yq6rZhVa2oChRPJUklsrC1bcgGEjE8hrf77Xq73v/coyTGvcVyYS6XcRIBiaSnOAZMnI6H44mizgTH3R+jkx+CuVfQ/PUDys+fnxevOUagm4a5NOMkbs8dimME4mT6OfiUFQWiBMdx3pqx3SRNH98P7OfzWdDCPuz27gHU6qb5+/UPvWi7ZmVZ+sLQDU3TDcHe7zb2VtN1KAfgcrlizufLhTJK+jnPiaGhz3lRoiwuoMljcSKKI1VVBdu219vtcr2GNvSTMZbBYYyFx4hoOuYkzxU/DOEQPwigTppJw+Ho4+NDlmWhO2NdEHnVVHXTnS/n7ty0LagiVnVd8qpkvI+8ymkJl4ECLAw6AnRGUYRmFLTEOC7XK6VlluWAYUECzrF7xu8IhxeUvq1aCE1TozRciQkz3qfIoK9kKIwsOMwpDY8nAOCTNMujCMA0CEPUF6q6wj2Q/UWWDOw4NPP+FdUoGMIkMFmUJO8fKwe8f9JUqKq+JkiiNnbbtWCLD+9coJcAAy54bbqu70Xb8rr5hfwHAoAhmKBxlQQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.4.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/86b1980f4fe2b5aba44f20aeedf3683d/d7542/figure3-4-1.png\"\n        srcset=\"/CR4-DL/static/86b1980f4fe2b5aba44f20aeedf3683d/63868/figure3-4-1.png 250w,\n/CR4-DL/static/86b1980f4fe2b5aba44f20aeedf3683d/0b533/figure3-4-1.png 500w,\n/CR4-DL/static/86b1980f4fe2b5aba44f20aeedf3683d/d7542/figure3-4-1.png 810w\"\n        sizes=\"(max-width: 810px) 100vw, 810px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>It seems that if a language has only two basic color terms, the terms will always be light-warm and dark-cool. If three, then add white and so on, each time adding the next basic color term.</li>\n<li>The regularity and pattern that languages follow when developing and adding new color terms undermines the cultural relativity hypothesis and suggests an alternative hypothesis.</li>\n<li>Linguistic universality: the idea that language is determined either by invariant physical characteristics in the structure of the environment or by invariant biological features in the structure of the organism, or both.</li>\n<li>For color naming, the evidence strongly favors biological determination.</li>\n<li>E.g. Is it a coincidence that the first six names for colors in any language match the three opponent processes (black/white, red/green, blue/yellow)?</li>\n<li>The compelling conclusion from this research is that the fundamental structure of color naming is fixed by genetically determined physiology.</li>\n<li>Color classification may be structured around a focal/best-example color rather than class boundaries because people more quickly and reliably picked out the focal colors than color boundaries.</li>\n<li>The idea that color categories are based on prototypical examples received further supported from studies on colors naming in tribes and from studies on the speed of classifying focal versus non-focal colors.</li>\n<li>Focal colors serve as cognitive reference points for color categories and other colors are learned relative to them.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 670px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/9bc3358288a4f18e8052240b3c28df7d/d67fd/figure3-4-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 122.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAIAAAC+dZmEAAAACXBIWXMAAAsTAAALEwEAmpwYAAADQ0lEQVR42l2U6XajOBCFefjpX1nttmPMogUBNptAwk4ybzefxHR30rgOR5Z061bdqiKZl8UGm1n41Tm/nLPzZKd+6NP0NM+2rs3b29viFl3pLM/tbDnFxmlMFueGaRr5D9y59eaFFNzgLM9z59zlei2KwnnXXhql1bzMHEWbEqWrohR12167jv+QSym2Y3ic9wFclm51VW3yIieEy/XSj4Od52S32+dFUZka8LW7gmlwdL12fdde2q7jfdHGdF3Pvq4MMF1VeBxnmzw+PeNYaS2V4jZgU9csTG3qpuGeNhXvyhjAlWnY5VdK0Y9jkp7PMJdCwkCSaNO2TcpuljVt8/NwYAHydbcvhWDx8rLLS4HGaJSQVT8M+CMkzmAryxJYXTc4hVMpXQhRN62UihhJUEgNLfiECs3OLd772+3+fn//eP/895PFx+cHTklnvd8xv65cwFxczBszyC0G3hTGr56qICl3hnEgz8W7aesF7vyyKVoAR2YAINf1toK/3WHwuAjmQ1y8Q4y/7BuY4iAY5QHGGYKz6PqeLmBBYdBicX7L8Q8Yr6O1+/1P1KLUXMVeX14Ju67r0+lE/mmaoubtfrdfaCfqvKy+G4bHxyfSG4aBOENXn9Pj8e3h4eHHPz8Oh2OWZeiM4H9oraWjA3MfwI9EOIwjatFeT8/PWZ7RJ3QLsN1+J2JXbODvOS+0a2eapu/7LWyl1OF4lIquUyeCPqcE6dfbb8HsVzBGtyD1pjbvZWEGK/CER3ShovP8Ffm/YMvmLxb5RifEUtMt9BnMCIYQW1G+l8omSE32NmpAbQMy9EpwFDAB5eJi+atDwjwfDgdyQq1zlpMzJaWZmdjLpYUWzcg7TFVtGAyWjBz/aQQ2E2SMuakqPk0YQ01LGGM44ilFSeYgGRgwdHspZZhDIRKGswoTG2eWUWoaxQgHQqm1Bs8pRhTwgqRyGEhqmdQRwNVQGK1Yg+TBndYVO0VZqLAM0YFimIFuZAleISdJtAVGdDEEE9PRfPqkJIQKio0zOtJCCfYTHT5Asm2RQW3+eEDyVcEFtCFbIWO2VV6WGPFDz34C50YOM7Mhg8LhY6LCDQFV/EgJrUNiIPnODtYy6v3Q/wdlWdKoNyRgfAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 3.4.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/9bc3358288a4f18e8052240b3c28df7d/d67fd/figure3-4-2.png\"\n        srcset=\"/CR4-DL/static/9bc3358288a4f18e8052240b3c28df7d/63868/figure3-4-2.png 250w,\n/CR4-DL/static/9bc3358288a4f18e8052240b3c28df7d/0b533/figure3-4-2.png 500w,\n/CR4-DL/static/9bc3358288a4f18e8052240b3c28df7d/d67fd/figure3-4-2.png 670w\"\n        sizes=\"(max-width: 670px) 100vw, 670px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>No notes on fuzzy set theory applied to color naming.</li>\n<li>The fuzzy logical model of color categories is consistent with a large number of findings in human color naming and categorization research.</li>\n</ul>\n<h1 id=\"part-ii-spatial-vision\" style=\"position:relative;\"><a href=\"#part-ii-spatial-vision\" aria-label=\"part ii spatial vision permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Part II: Spatial Vision</h1>\n<h2 id=\"chapter-4-processing-image-structure\" style=\"position:relative;\"><a href=\"#chapter-4-processing-image-structure\" aria-label=\"chapter 4 processing image structure permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 4: Processing Image Structure</h2>\n<ul>\n<li>Perceiving color is only one of our many visual abilities.</li>\n<li>An arguably more important ability is the perception of spatial structure.</li>\n<li>E.g. The shape, location, size, and orientation of objects in space.</li>\n</ul>\n<p><strong>Section 4.1: Physiological Mechanisms</strong></p>\n<ul>\n<li>We start with trying to understand how spatial processing occurs in the retina.</li>\n<li>Review of on-center off-surround, and off-center on-surround ganglion cells (cells that have an antagonism between an inner circle and surrounding ring).</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/da994/figure4-4-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB20lEQVR42kVSaZeiMBDkd4/j6D4/eKwieKIMIiGEK3dgZn7eFrhvN68IhHR1Vx+exXLOWNvyluT5M8s4565zOc0pzdu2lVLUTVuwgpUlpbRpWy5EO4B7xtiRyaXWMDqeL7gzzhpjhBBKKRizqgoPB8qYNAbOBox8TxmjjQETFkmawl/X99CCCHALCff7fbvbbTa/T+czzITWXCkuJeDh/EJG8vSZVXXDWCm1qtum65yU8ng67nzf3++xZ4QoyAQTfKW8QZkxXMjLNVouV+HheDyd4yRBhl3fAckj2WH5fhgOl1D0NzjIEuQx8sfH/O1tAnm0YEgBmf/8fDvnuODRLdoHATwEQZDTAsH/k7W1+OXvg9V6Qygtqyq63S7XKyEEmuumRheCMIQLuFXa/Mvc00ZDG9pyj+PH4wFfJCfr9Xr+aw4mPuI4LqsSLiAYQHWRptBj5FdJAWuNVJKVbLvdTSbv0+l0NpstFgukap3t+851mIihr6/uvMjDGb0BHOrrXBRFqxVCbrCWy+Vnknx99QNvnCVAjWRU2quqEhMCoDAoEnaBkaprjFTTNPjgr9/jAzMoRwJlXQNeniNfSouCFtiH93BiBfJEMAwpIRlsMixC0ufzkaYglLhumj/8PdHDmV/AVAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.1.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/00d43/figure4-4-1.png\"\n        srcset=\"/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/63868/figure4-4-1.png 250w,\n/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/0b533/figure4-4-1.png 500w,\n/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/00d43/figure4-4-1.png 1000w,\n/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/da994/figure4-4-1.png 1487w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The area surrounding the central region always has the opposite characteristic.</li>\n<li>Although bipolar cells come before ganglion cells in the visual pathway, they were studied later because of technical reasons.</li>\n<li>Photoreceptors and bipolar cells respond by producing graded potentials.</li>\n<li>Graded potential: continuous changes in electrical potential that travel slowly and for short distances.</li>\n<li>In contrast to spikes that can be recorded inside or outside the cell, graded potentials can only be recorded from inside the cell.</li>\n<li>Bipolar cells have similar receptive field properties as ganglion cells, namely the center-surround antagonist relation.</li>\n<li>This property is due to how the connections between photoreceptors and horizontal cells are organized.</li>\n<li>Photoreceptors are directly connected to bipolar cells but are also indirectly connected through the horizontal cells.</li>\n<li>The direct pathway is either excitatory or inhibitory but whatever it is, the indirect pathway is always the opposite.</li>\n<li>These two regions of excitation and inhibition sum in the bipolar cell to produce it’s center-surround receptive field.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 558px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/96ce589a86596604266433be4dbf6a9e/42a8d/figure4-1-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 161.20000000000002%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAIAAACdAM/hAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEDklEQVR42lWV53arOhCF/fD3JD9yXGJTRBFVDSTwecT7CUxK1ixCma3Zs6f4NM02F9nlenk87tfrNUmS6+36eb/fPm+8vCf3y+WSpumZa5aNauz7bhiHeZ68n09udt3QpVnaD10u8rbrirKoZV3KKhWi6bskzfphTPO8knXdNEKIuq7d5GbAdnLK2aKqeKjqeBUix6lu20eaaTeVUho3FVWtnZtDCOuCzcQFbKbJzjMftNbTPGGwKspSth0wZd1o7GiMsnZ3s1MMOoWAQXvG+NCNY9N1bd/DnBvZ9WUt+1HJliwHu8XAwMzr8gs8BY9HmuWZEKNSgKGNMkmW3+6J7DoiD1pjsBi1tt5/g0lGGZOLommasATAJAlnjrg9knYYZNeKouRoUZZIOBr9AkMJJAJqrSjG5Oeiwq8g245nY3ZdHNEO28iGE3fa2nuS3G638/lsrPHL8t/bnz9vb1BFbRfCjgTwOmXP1PtTVM97dIIzBaNybp5EWTySBFfCft4faS6SLKuknJeFbPeDsAi2zmW5eNBHGWQL8h+U4jiCEJwq7FLF1HbaSE3kHewicwdAG0MD+NgGK+Sj0+a6F8ns0vp5r1PM2c7Tl+DGWXTOiyLL87wsaU+4kAt+MduJFvoNdsd5GOoYa5VWNHPT9U0cggFGexdFt8Nz/lnneGSI7eoDfMO8EY4Z8jL4Q94j7JH5C+xDWLaOX5/L87mG4GlyYJtfPNEvnIrTLwM8mU1t6pEL8Uge/Gc24oTN0xKPW5fnyvUZbVnX8GWnveOzorjd7+yAJHkkacrEMg79NvRUyjqrlBrHkRsCLptFMDJiCMBbx701dCSDBcm6keMw0HZ///79+Ph4f3+nCFDYOYOPkcmqHyLPtmuLosAvhlXj9XphzGopuZZl0Q/9HhD7BSZtZY0ymnnct4KUkqZSsa907COmeA3PDfmV+em7DCHqiexUmwYnR2sN79A/yrb+lOoAT0eH7TsoVsjH+2HocWrbluCxVFv9v/Dbcd91joLFUi+wWgnboBY6E5w3/557nX7iX2A6YRhHNi4LeFuvMssySn2+nNGpqipERjZjNaf/xB8LMHa10Ua3bVNW5agVZaNmlKBpW6aNCnvabH0V+VXnowe9bBs27j7S4NlWzFZZ16yBTOTchz21IzuEPMXFSnUYeiQa4wwRcKsZVVMq7krmXGPkT4oQjOVA2smd+GW6XK9wo6XJr+t7UQgeCVXXFakSnPaQsobL0Pes1zp+qPlJOpEV81Aiy+bLTbyXkpWwH7G9KSOgKvnDrWkbiaoRLCXTwJWw/JrJtt2+N7zhETBPtC1XVgNgIstGwg5pT3nBMOZxHou4d0RVJXnOZmcNiaqEFBRY48SH59anfts58Sf2f3FcyUpI6UNmAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.1.3\"\n        title=\"\"\n        src=\"/CR4-DL/static/96ce589a86596604266433be4dbf6a9e/42a8d/figure4-1-3.png\"\n        srcset=\"/CR4-DL/static/96ce589a86596604266433be4dbf6a9e/63868/figure4-1-3.png 250w,\n/CR4-DL/static/96ce589a86596604266433be4dbf6a9e/0b533/figure4-1-3.png 500w,\n/CR4-DL/static/96ce589a86596604266433be4dbf6a9e/42a8d/figure4-1-3.png 558w\"\n        sizes=\"(max-width: 558px) 100vw, 558px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Lateral geniculate nuclei (LGN): cells in the thalamus that the retinal ganglion cells connect to.</li>\n<li>LGN cells have center-surround receptive fields like retinal ganglion cells but they’re larger with a stronger inhibitory surround.</li>\n<li>One major difference between retinal ganglion cells and the LGN is their structure.</li>\n<li>E.g. The ganglion cells form a 2D sheet while the LGN is a 3D structure.</li>\n<li>Each LGN cell is monocular and only fires in response to stimulation from just one eye.</li>\n<li>Binocular cells aren’t present until the visual cortex.</li>\n<li>The internal architecture of the LGN is interesting and has provided important clues about a crucial functional distinction in the visual system.</li>\n<li>One clue is that the LGN is laminar/layered with six 2D sheets of neurons.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/c8d6be2a4d5fb8cde51a8e9869d2c780/46cee/figure4-1-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.599999999999994%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+klEQVR42jWR+0/iQBDH+a9I9AcxORNOqDw8ITwOpVi95EAKeAcoRCQKx1Oe5QJnoA9a6NJuoQX/vZtyuclkspmdz35nZi2Goe9225WyYpjheDLudDvMiBkyg16/B94f9Ee/R0NIjJjJZNztdpbyUsUK1INbNpsNwAghKK3+qqbSKZqm8w/5+58/cvnca7XyUHiMkiRFUfF4vNVuygipGlawqqiKCYM4WqFGsxEloz6f7/Ppqc1mAyCdyaTvM8QZ4XK5fHtLJBJIWamapmAM0aKttY1uAPxYLLjdbtvRke34+ODg0G633yXvIpHI+Zfza4oiYzGPxxOJfIW28Xq9F8cmrOsGpFpv7YsLn9VqDQaDIBIOh6mbG7/fD7KBQICMkQRxls1mVawCvFc2YawbpvJgOHh6Lp18OnE4HKXyc61Rz+ayl1eXMGooFHI6nV6vFzanqABrQJow1vB2t2U5lk7R/wDQqTcbrXar2WqCd3vd7/E4QRCFYvHl9YUXhLWuq/h/27uPD47jnkqlQrEArSbp5O2320q1Avuv1WvttzZEWFW5XIakKEnG1tDMsTWLtJAQkqFtGcnwBM9DYP+8v4MCzCWKUCzCR3I8N51OoWYhL6XlQlxI4BaWnfECD6XAQRTmpgEBBl9onvcm7G9ZjpvOZnNJgqwgzv8CLtZJhPXVtrEAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.1.4\"\n        title=\"\"\n        src=\"/CR4-DL/static/c8d6be2a4d5fb8cde51a8e9869d2c780/00d43/figure4-1-4.png\"\n        srcset=\"/CR4-DL/static/c8d6be2a4d5fb8cde51a8e9869d2c780/63868/figure4-1-4.png 250w,\n/CR4-DL/static/c8d6be2a4d5fb8cde51a8e9869d2c780/0b533/figure4-1-4.png 500w,\n/CR4-DL/static/c8d6be2a4d5fb8cde51a8e9869d2c780/00d43/figure4-1-4.png 1000w,\n/CR4-DL/static/c8d6be2a4d5fb8cde51a8e9869d2c780/aa440/figure4-1-4.png 1500w,\n/CR4-DL/static/c8d6be2a4d5fb8cde51a8e9869d2c780/46cee/figure4-1-4.png 1687w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The lower two layers are the magnocellular layers because they have large cell bodies, while the upper four layers are the parvocellular layers because they have small cell bodies.</li>\n<li>Magnocellular cells are sensitive to differences in contrast, are insensitive to color, have large receptive fields, and exhibit transient responses to changes in retinal stimulation.</li>\n<li>Parvocellular cells are insensitive to contrast, are sensitive to color, have small receptive fields, and exhibit sustained responses to changes in retinal stimulation.</li>\n<li>These distinctions aren’t clear-cut and there’s substantial overlap between magno and parvo cells.</li>\n<li>Perhaps the magno cells are specialized for processing motion and depth, while the parvo cells are specialized for processing color and shape.</li>\n<li>Backtracking, two different kinds of ganglion cells project selectively to the magno and parvo cells in the LGN: the M and P ganglion cells respectively.</li>\n<li>P cells receive input only from cones but M cells receive input from both rods and cones.</li>\n<li>We’ll see how the division between the M and P pathways propagates to higher levels of the visual system.</li>\n<li>Each LGN layer receives signals from just one eye with the four parvocellular layers alternating between the left and right eye. The same applies to the two magnocellular layers.</li>\n<li>Each LGN layer is also spatially organized like the retina it receives input from.</li>\n<li>Retinotopic mapping: when the organization/map of the relative location of cells on the retina is preserved.</li>\n<li>E.g. Nearby retinal regions project to nearby LGN regions.</li>\n<li>Retinotopy is a common feature of the visual nervous system.</li>\n<li>Striate cortex: a 2 mm thick sheet of neurons and a few square inches in surface area that’s the largest cortical area in primates.</li>\n<li>Review of Hubel and Wiesel’s discovery of the striate cortex neuron’s receptive fields.</li>\n<li>Simple cells: neurons sensitive to a edge/line at a specific retinal position and orientation; often called edge/line detectors.</li>\n<li>Edges cells have an area of excitation on one side and an area of inhibition on the other, while line cells have an central elongated region of either excitation/inhibition and an antagonistic region on both sides.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 554px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/c3c8c009932ddf8c8556e2150cdf14be/04abd/figure4-1-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 159.60000000000002%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAIAAACdAM/hAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEaklEQVR42kWVh5aiWhBF+fgOZgGVnHPOOu/z3r7gOK7brNtUPnWqkJbnPIx9XuSWZbmuE0Wh49imZRZlMS/LvMxN2yRJ4rhuEPh+4Fu2hbRuakTSPE/P19P3fUVVEZdl8Xg8bvd71w/jNI/TNM1zkqRhFMqKfJXlNM3iJEGISELh+Vpczz2eDgQPguB0Psmy3HY94u1EUex63tf39/fPz+oowjHnn/HlerYsO8vz0+mkqmq3ht5OnKSeH/zudqfTOUlTPwhJ6m28vGbXxfjiul6W5bKinC+Xtus+xkmaYbA7HH5+f2MROf4XGVxs2zoc94ZpRHF8Wn9/jUcROU6IvNvvv75/XC9Is7wbhnfN8zJFcQhIYRjmRcFF07R2zVuozEtRltjLIKYoWILZJhLGeZGlWTpOI13J83wcR2QbnpwkyaqqrhsQ7Pth4MZLLNfIy2gYOh1eXgtomaax8Hu+NjFPXTdsx1XVW1lVNImOFmW1iURkz3dN0yQp27bTND2fz2gTWDR5mh344Xpplu33e9DmkufFlpSECygFsQzDiOO4bdv7/Y7Cp8l0lfaAkKyomm6sTcLvDHmkfuioMcsSXfxMwzDptm4Y1AZUwzhDUsu2QRHR/aHJys20nHdkYTwBfF9WZVVXddNUTU1mGzE5GAMVdSLjrygK4Hu+XsvzKRVFFoS+7diw1/c9z/e40G3LdoSntoUhnu/z5G0QhrCFJ0AAntR2DR7rutINHVb6gYeULOgtjRG9adosL3w/pGBFUS6Xa16U+CIXae1oD+aKqnx9fUVRxITRJrL6pM0TY1rAbByPJ/5FyntRc9+3w9Ddbuput4/ihBki4GoGZMIeeMkcKPeHI1P5gUPCcjv0eX84MJL0CX+rWCi9eZamdJvBuFyvf42nt3Hb1lf5ejgeiEDkrus+ZiuZRjEbnk9qGIstscz/WtV17e122x/2UCJO4jXy9EmPKQB/x3UYaaYVRNZy1shoV3XZNDUg98OK8TpOW3DmrOuZMKbizYWVtbPYYSSsqkqSxvjjsNxWhnAfBD26/n5/tF3LhqSiP//9eb2eAovlbdw8HncWo6Zr58tZrLEwwP1qPzHVmqbDB0Ydhcvlsq3Od2ToaBgaI50kMeQmN8xIe2N/0/UMDNrLE5736k1lqtaKROZS10HBGvv7/aZpj4f2gC0s562K1cvAUGq6TmS2FUvGdhwxVESG2wQvyxx2ZDmkK8SzKgeBm8gCPcAXBK5rWooClw1OiYCyfPE8F9cw3hOz4XKhN9yjmCmJgIq9vWLmwd+sKGh7kiaS5znkzJM2csADA7SBjRbCc7410A4XDJ/r8T2KmXgUeCPhlEosy2TZMPRUy2LZZpNNjDuxSZIEb3wYhLsgwJiXXCTfc7cvm2GaIIE9SgwmU72xjSA48tfMqcXfshCFRdIax0WBPPkIkhtp4wUzNgQJY8N6FFIWQRBQW5SgYIvIYcgbsnTZ+zhbw76DiEUimMmBad3WNtjLE851ffc/kRvUMgKpn4oAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.1.6\"\n        title=\"\"\n        src=\"/CR4-DL/static/c3c8c009932ddf8c8556e2150cdf14be/04abd/figure4-1-6.png\"\n        srcset=\"/CR4-DL/static/c3c8c009932ddf8c8556e2150cdf14be/63868/figure4-1-6.png 250w,\n/CR4-DL/static/c3c8c009932ddf8c8556e2150cdf14be/0b533/figure4-1-6.png 500w,\n/CR4-DL/static/c3c8c009932ddf8c8556e2150cdf14be/04abd/figure4-1-6.png 554w\"\n        sizes=\"(max-width: 554px) 100vw, 554px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>It’s plausible that if LGN cells are wired in a specific way, we get edge and line detectors.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 812px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/b5bb9b12a4b0ae5dfd0c1af246753152/63ec5/figure4-1-7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 71.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAACR0lEQVR42iWSWXPTQBCE9TMdeAlOIDiBN3JUQU7AlUocUrEdS9YtHzpXK2klO/Dv+CRXTU3t0d3TO7NapdQuPM+f6XomxPbtjW1RlmTbcY35nHUuZVlVpmWNRqPpdCoLKfJcA8EpIYtC1bXjusPhMBWiqNSOk8u8AFBWsiyjOAmWyzhJ2vOi0EpVl/ApU9eW43z8dLS/3x/e30MWUj48jq6ub9IsQxotaqim3oly25LBqbrxfP/o82Bv732/f+i4Hi7zsrq9+3l6dr4OQz9YhGG0DqMwikRbtiQ0mPVmA3pwfNLrvTs4OKQ+FzjMRH5ze/ft9AzyYrEyTefi4vvJyVfX8ygmZKGpZuP6/vHxl15vD77tesilIicofnl1fX5+ESepHwSO648nr5RBtSirltxstzNj/qF/OBi0zFwWWcckMil//R7+uLyK0zRJU7RU01R1vfPc2mbDkWU79HD7919BDzZbkuy0wyhehRG9JOgTuW5aPO66N9NDpQDhx/V8LwgYZpoJgFGSPjw+PT09B4sFGLR4re0A9MM4QV1rNhvUkixbrdfL1RoCmX7KSnE9mb7+eX7h56haMW0wi+WS/jE8imuocsR3idrRF2kukzSjcpuFYLGOYtkNtpt2iS5a4OmiNh5PxpOpbTu6bpB5/MvLGGemabv484PX2cyybAril/9nWvZMNwCA1CgbLJZOB2QevMqybdaeH2BvDtZxIXMIxJibONUNg5eD0exODCh8p9MAigTBTjfmjufhcxcMrDPMm7JMiP94aRP6AnLk9AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.1.7\"\n        title=\"\"\n        src=\"/CR4-DL/static/b5bb9b12a4b0ae5dfd0c1af246753152/63ec5/figure4-1-7.png\"\n        srcset=\"/CR4-DL/static/b5bb9b12a4b0ae5dfd0c1af246753152/63868/figure4-1-7.png 250w,\n/CR4-DL/static/b5bb9b12a4b0ae5dfd0c1af246753152/0b533/figure4-1-7.png 500w,\n/CR4-DL/static/b5bb9b12a4b0ae5dfd0c1af246753152/63ec5/figure4-1-7.png 812w\"\n        sizes=\"(max-width: 812px) 100vw, 812px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Unique features of complex cells compared to simple cells\n<ul>\n<li>Nonlinear: rarely responds to small stationary spots of light.</li>\n<li>Motion sensitive: highly responsive to moving lines/edges in a specific direction.</li>\n<li>Position insensitive: small differences in stimulus position won’t affect response rates.</li>\n<li>Spatial extension: tend to have large receptive fields.</li>\n</ul>\n</li>\n<li>About 75% of the cells in the striate cortex are complex cells.</li>\n<li>Complex cells might emerge by integrating the responses of many simple cells.</li>\n<li>How is the striate cortex organized given that cells are tuned to different properties of the stimulus? Is it a crazy-quilt pattern with no discernable structure or are their regularities?</li>\n<li>Difficult experiments were performed that discovered truly remarkable regularities.</li>\n<li>The striate cortex in each hemisphere processes exactly half of the visual field and maintains retinotopy.</li>\n<li>The retinotopic map is distorted though because of cortical magnification.</li>\n<li>Cortical magnification: an increase in area devoted to processing a region of retina that has more photoreceptors (and thus is more evolutionarily important).</li>\n<li>E.g. A small area of the retina near the fovea occupies a disproportionately large area of cortex.</li>\n<li>Since both eyes project to both hemispheres, are there two separate retinotopic maps in the cortex or a single integrated one?</li>\n<li>The answer lies somewhere in between as there’s a global map for each cortex, within which cells dominated by one eye versus the other eye are interleaved.</li>\n<li>This results in ocular dominance slabs/columns; areas of cortex dominated by one eye.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 811px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/a443a45b03495f9ce64a3d33cb3dd49e/fd28b/figure4-1-14.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 94.39999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsTAAALEwEAmpwYAAADLElEQVR42i1UaW+bQBDlHzaODeY0sNjpRdJG6aVIbfqpSd0obtSoqdxwGLDNvVx2/l7f0qzHCHZ5M/PezMB1Xdu2TdNUV5ffpha5+PL5588b33dLWrRd03ZtScuyLGB5kcMy/Ep2UxQ5BzBssbiRZVEUx+JYgFkWOTs7vb1dLJd/KKVNU8MFrWhVV7jJ4ag37nG/T5KEmKYsiaoiAakoEsCzqWUauj6ZfPr08er75Waz3mzxj9IsoXUFZEEpt9/vtnFs2zZwxDQUWTIMXRyPVVXBPdCapo77x5Nj++ULrOfLv0ta13lZco+P+/V6PZvODH0iiaKmabIsmaYxOHg2ODjgR4cSy0gZC4KqyNgHt2PbjtOkrCpw7uI4fn50BLAssWN+xI9GQ0mUFBl+JEQ+ms1AhKWjqCCI4+vr67btOCidZdnJ8Rv41nXDNMi7s3eTyQTiWYQMh4e4aiqLTAgTwZjoiqz+/nNfNw3jnOfZ61c2ZJ5alqqqAs8ztUy9R5pgDTkgADGJKqvw8vXia0kp47zb7TabDbSBMkiSHw2ZSOAnydgxDXM0HCJb0zCQNnRAjCAMnwRDkbfxFkwOB4cCL1jEAhiv6hOdGAQS8iPB0AlEgRDgP5/PUaSCouCU6/r+ev/+Az8UVFkDH7CdTglcICbisEQUHYmA1Pn5eZbn0BntwiKjOZH5/f09wkqijB5TFAUJQ2qtz16Rodb45PjEdT1Ea7q2ahq0G2uSvj27NM3s1/ZgMJhaU01FPQhSQJtpqsYa/uJis93SqsrywnE9z/fTPH+KDDwy91wPGGgm8L0wPEplnb49e3AcwKq6BiCOU9dzXdfZJvGT2li9C8RPbxaL+fwHeuDX3V0QBN2u7XYYrqZqahhq+3/UKEYFHYY6ocOyfiVpijfQ9JgeSjGHbPD6/QQ6pVkWJ3glRW9GoBHHnOM4y+VyFbDlB6sgCv0Ve8K+v/KR5CoMsLMKQ8d1gjAIo8jzV47nwbgkQeAtFlIAJgxxGuHR9/0ggJsVXMBZuI5c38Mx/OC6fHjALgcUAOsoivuFCmL0saARviD99KMTC1zZJwVfAtpz6e0fIXJctE6Uz0cAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.1.14\"\n        title=\"\"\n        src=\"/CR4-DL/static/a443a45b03495f9ce64a3d33cb3dd49e/fd28b/figure4-1-14.png\"\n        srcset=\"/CR4-DL/static/a443a45b03495f9ce64a3d33cb3dd49e/63868/figure4-1-14.png 250w,\n/CR4-DL/static/a443a45b03495f9ce64a3d33cb3dd49e/0b533/figure4-1-14.png 500w,\n/CR4-DL/static/a443a45b03495f9ce64a3d33cb3dd49e/fd28b/figure4-1-14.png 811w\"\n        sizes=\"(max-width: 811px) 100vw, 811px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Hypercolumn: a column of cortical tissue that runs perpendicular through all six cortical layers.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 669px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/3c07fc17b135f0a0f3b835b27ea5ece2/99272/figure4-1-15.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 119.19999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAIAAAB1KUohAAAACXBIWXMAAAsTAAALEwEAmpwYAAADm0lEQVR42j1UCXLiMBDkvQsLgZBAuGxDwDcYfF8YfF+wtfvCbVlUqClKCPV0z6hHvSzPSBQ5Ii9pFEVZVHWF/SSN/cA3LTNJ4qap6y6atqHRw1GKzIqig5VVU+dFHid3wzI5jmM57ng63eJbWRYdvvnBA1x2QZBlXd3i2LKt8+VyEPYHXtAuZ9OyXM9zXFdVFduxO1iLFO2jJWDCWVVJltiuKyvSar1iua3jOkmaRrebbugbZj2bzyVZDq4hzoP38XwScFFVcZqoxyMv8AeBP2paeL0GYaCdNVmRDzy/23+f9YsfhA7yue5JO1mOjY60zwdlzgVR2DCMH4ZxkliOI6vKar0cT8aiJKVZdk8SwLC5YTbslmPZDUp7/vnzki2rqm6YKBUKIXuxWoFTO585bsttt6IkHvjDnj8ADP28cAABlANcASxIIsS4vjf/mn3OZurphCqQZf719as/GPweLFer6fS93x8slwtRlkzbpGDCjKPoB3q7+97Zjgv8cPR7NBr+6vfHkwmBj4bD4Wg2n21YRhB523XaxwMNK9M8V9SjrKhoG8NuolskKfLobQQkCNEL5IJ8URbRre1uy1Pwk4BxSRmYcU+O5y1Xy+PpKIjiar3+nM8ZhgHb2/hNksTv/Te3ZXb7HQ6btkWZq7Kur7fIsu2LoUO2rhtoD9jG4/Fs9vm1mE8/pnBaf9BHFpZjcTVBGL7AiKppULxhmhB5u98lWXqfTifvYzAzLGR/LpaLj48PQZI8zyurEg4h9qRgyg/nwIzq8YS7WW+Iq3Bzk8kEieCZexw3jxbXC3sC2T6al2wSWFQVxMCS6AocMhy9rTcM2pjmWfN4EAyZCuJtML/sSWE0S5Zl+BsLRVHRBWD+/vvXTWLzM4kdM4mfqSIBDIbBtG3bceDwaxT5QYDwPL+s6BjWlPYFrjFfbQsYeobAGrUBD6uaeATwsSzMOMqhUrsFjbbneC5yw/rR/Y5hhtswCVmex2mKIcECz0PebRbkO8UOasQOXpseXHXSNC8ILrqBoYdacGGGdMOAeAQmEXVgjKBD13VcVRAE2ARlr3slPOj0At+ybB9PVhhAquf7tHKnS4FtBBZoBHIhBXrSwyTiqIG3xvctzITve0GonS/YtEjjXGTvwAHS4RcCzAB5vtfrtLmGaSHgbRzVTZNkBLKD2S9mIodKQBaiMfB74MShMIqITtfB/3hxLIoBayfe6RbI291ZSX1RN/V/3GtTwavgxW4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.1.15\"\n        title=\"\"\n        src=\"/CR4-DL/static/3c07fc17b135f0a0f3b835b27ea5ece2/99272/figure4-1-15.png\"\n        srcset=\"/CR4-DL/static/3c07fc17b135f0a0f3b835b27ea5ece2/63868/figure4-1-15.png 250w,\n/CR4-DL/static/3c07fc17b135f0a0f3b835b27ea5ece2/0b533/figure4-1-15.png 500w,\n/CR4-DL/static/3c07fc17b135f0a0f3b835b27ea5ece2/99272/figure4-1-15.png 669w\"\n        sizes=\"(max-width: 669px) 100vw, 669px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>No notes on Hubel and Wiesel’s experiments on the development of cortical orientation cells in kittens.</li>\n<li>Critical period: a period of rapid development that depends on environmental stimulation.</li>\n<li>There are different critical periods for different visual properties but in general, it appears that the critical period for a given type of cortical cell depends on its level in the visual system.</li>\n<li>E.g. The critical period of lower-level cells occurs earlier than those of higher-level cells.</li>\n<li>This makes sense as higher-level cells can only develop their response properties after lower-level cells have developed theirs.</li>\n</ul>\n<p><strong>Section 4.2: Psychophysical Channels</strong></p>\n<ul>\n<li>The discovery of simple and complex cells was controversial because there were disagreements over their functional significance.</li>\n<li>E.g. What are these cells doing?</li>\n<li>Unlike the study of color, the convergence between physiology and psychology in the study of spatial vision is weaker.</li>\n<li>The psychophysical community has been working within the spatial frequency theory, a theory that proposes a different conception of the cells Hubel and Wiesel discovered.</li>\n<li>Spatial Frequency Theory\n<ul>\n<li>Instead of using edges and lines as the unit of vision, this theory uses sinusoidal grating.</li>\n<li>Sinusoidal grating: 2D patterns where luminance changes according to a sine wave.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 558px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/4211fe1ed4a0227adb8f045acd6b615a/42a8d/figure4-2-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 152.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAfCAIAAABoLHqZAAAACXBIWXMAAAsTAAALEwEAmpwYAAAE+klEQVR42k2VaVMaWRSG+58lMVEWaXZEEYgLCPoh+TiVSaqmKhM1gitbs9PNvnQ3u87Pm+c2ZjKXy7Xlnves7zktLZbzarUSj0fPzpLpdCqV4jw7PTkulgrr55U5NWfz2Ww+nc7+t6eGaepsablaFEvFXafT7/eHw+G9vb1QKOh2y7e32Zd/ng1jMp6MpjNjvphNZzOh6BVsWODlogR4dzcUCh3sH+yH91mBgP/u/vbl5VlTW4NBrz/oG6Y+0Sdij0d8Tcu4NFvMC4WCw+EAjFWM7++HvV7P3f3d+nmtaRoGG81mS1WbLbGaYjdH4xHGpflikc/nbTZbMBgMBAL4jO9utzubza7WK63d7vX77U5njLlfezgajidj3ZgQ8/Ipl9vZ2fH7fQG/WGhxOOzXP6+fX54nuq4bBlGaVpTGf+EaRDGW5suFUlYI9eTkJJlMnpyeJpNnsVjs4fFxuVohZAiIucFYMBaHLiyTenyo1qqVSqXMp1pVyuVypTIYDszpdCNqgQ39F8YUzyRvJIlfdR1Rdn/Q6/V7XT69HhotgCEs/66w0GK8gsfSYrnIF/L4fHp6Ak+E24nE8fER5oXs1KzX6/F4LBo9jEajkUgkFotSjmw2M52Z0mq9zmQydpvN6XR6PR6XaxeGkLCHxweQ7JKieMXysGTZ7fW4nU7Hjx9/wxzAq+ztLRiuQ8EgJxXz+XyFQh4/ASuKQgkstNfn9fp8XpdLvrq+3FheZbIZGOb2uCkV1xuKwlkoTXjlskLxufGJrxcZWZYvr36YUx3wMnublV0uvBW3Pl8wGEKiWCy+Wi4rewIs7jYCWLq6uoShEhzMZG4gid1ux3l4Cr1k2ZXL52gD8IDxB2su3JVdIO122+Xld9OcSOv1Opd7IodHHz9CkqOjI85EIkHV54s56mu1KlexWBzmRA8P4/GP9M/NzbVhCpKY/X6/pbYoSaPZoBPqjXqtXqfUVieNYTKV53d6iweWqqkwYjwZSuiG7b1+t9vrIME1d+1OezAYWD0owKraaqoNmomN6nqj2u226XNpJRrjkU6KRA6gweFhNB6P09LFYoFswzOtrfHLwcH+ISsSiR5GIMn3738ZVsyrnzc/3755a7c7yAb0IG0fPmzdP9xtaI8jAX9A1NJaHtLpcn39+uW1VGR76907eEMunQ6hYmvrPZOE0TPWx5qmQhuPAMoeDwzz8vDt25/TmS7chiTb29sUCfWQ1LUrv38P+G6xmBNzu6MxJgBbDOWkbL8sMwAZGrYdG8V1uwVVELLt7Nz/BnfCe2HBTbEESyzLG/BycUPMb9/YbXYCtpyXt7c/PD094jYJJ/MALPLIeCZ44nR++fKHIAkppQeOj49PE4mzZCp1lk6nzxkmjAe4OZlMut3O58+fuLi4SJ+nUxcX5+fnKUiiMww29Wg0GrV6DQBs2ZzMBkhiNf1EFF9tQTXqLCRrleGoL0hSrijABA2ajZHFBwrbaDU4O902m4SpmibI19a4JQry31KbWrslFQo5uq/ZapSUEhLWJKOFS/zlZVCv12r1Kv8y25ltJeuhWq3yMqrUFGlDOmBCqKIwAy1u11AHyWhm8EJhtWyNRyQUkPlCDpJK9UatVFYwqGotbHLd6XYLxTwPaMEUbwitraIFMKGRETwl7GarLmGwKa61DZh22UxGNonktcJJD9AePHOSSLI1GPYGw96/vYravTUOc+YAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.2.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/4211fe1ed4a0227adb8f045acd6b615a/42a8d/figure4-2-1.png\"\n        srcset=\"/CR4-DL/static/4211fe1ed4a0227adb8f045acd6b615a/63868/figure4-2-1.png 250w,\n/CR4-DL/static/4211fe1ed4a0227adb8f045acd6b615a/0b533/figure4-2-1.png 500w,\n/CR4-DL/static/4211fe1ed4a0227adb8f045acd6b615a/42a8d/figure4-2-1.png 558w\"\n        sizes=\"(max-width: 558px) 100vw, 558px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>Each sinusoidal grating can be specified by four parameters: spatial frequency (width), orientation (angle), amplitude (contrast), and phase (position).</li>\n<li>It seems odd to use sinusoidal gratings as primitives because we don’t experience anything like them when looking at naturally occurring scenes, but there’s no reason to suppose that the primitive elements in early spatial vision are conscious.</li>\n<li>E.g. We don’t experience tiny points of color from the three cone types, we experience a uniform colored surface.</li>\n<li>One reason to use sinusoidal gratings is because they can be analyzed using Fourier analysis.</li>\n<li>Any 2D luminance image can be decomposed into the sum of a set of sinusoidal gratings that differ by spatial frequency, orientation, amplitude, and phase.</li>\n<li>The gratings with low spatial frequency tend to carry the image outline while the gratings with high spatial frequency tend to carry the image details.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/a4d3df5c7c67d7c3c62a3855a5a8500a/76a18/figure4-2-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 39.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABu0lEQVR42hVQy07CUBDtJ2hYsKqsdEEiBJqgaGIBBSGxC42PRCMl2Ae2vC01ram2GgF5FYtSVEwstnVl/EWHZHLvzD3nzMy5SKfzRFEXu+ldHMdJMjs2x6Y5vr1T8vk8QeztbG9zXGE0MiYTsyEKotio1ar1ek0Qrq6vRaTT7QSDwf2D/bOzU2CDEtRCQwgEAqlUKplMptNp4BiGwfP8JXdJ5kjoS1EUwzJIv9/LZDKiKDZbTej3/vFmTkxNU6PRKM3QwMhmz3v9njEyKpVyscjDmjRNQSOO5xBYG+BcjpRvZFVToQSxLEvxeDyRSJycHMMoSZaejedSuQTiQoFlWQa0kCPr62uhUAiP4VBjGIai6MPD/ebmht/vj0QiBEEEVlcxLNztdcuVcrVaAQ0szxf5UqmIrKwsLy4uHB0dkiTp8Xi8Xq8kSeFQeAlFY7EYjm/5fD7wD7ZhMltg527BDkNTNIWoqqYoiq7rrXZbuVVUVf2YTlvtlnavgdXBoA9eHpuP1sx6fX2Bb9OHuj4YDIfzC5lOP2ff31/WF8C2Y7s/rmVZjutA/P39QjiuDe/WbAYnhAPIHLVd1/4HPqHRCbqYaN4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.2.4\"\n        title=\"\"\n        src=\"/CR4-DL/static/a4d3df5c7c67d7c3c62a3855a5a8500a/00d43/figure4-2-4.png\"\n        srcset=\"/CR4-DL/static/a4d3df5c7c67d7c3c62a3855a5a8500a/63868/figure4-2-4.png 250w,\n/CR4-DL/static/a4d3df5c7c67d7c3c62a3855a5a8500a/0b533/figure4-2-4.png 500w,\n/CR4-DL/static/a4d3df5c7c67d7c3c62a3855a5a8500a/00d43/figure4-2-4.png 1000w,\n/CR4-DL/static/a4d3df5c7c67d7c3c62a3855a5a8500a/aa440/figure4-2-4.png 1500w,\n/CR4-DL/static/a4d3df5c7c67d7c3c62a3855a5a8500a/76a18/figure4-2-4.png 1691w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>No notes on Fourier analysis power and phase spectrum.</li>\n<li>Fourier analysis provides a general method of decomposing complex images into primitive components.</li>\n<li>However, mathematical power and elegance alone aren’t convincing arguments that the visual system does anything like a Fourier analysis.</li>\n<li>Psychophysical channel: a hypothetical mechanism in the visual system that’s selectively tuned to a range of values within some continuum.</li>\n<li>Each channel is defined by the spatial frequency and orientation of gratings that it’s most sensitive to.</li>\n<li>So, the spatial frequency theory argues that image processing is understood as many overlapping psychophysical channels that are selectively tuned to different ranges of spatial frequencies and orientations.</li>\n<li>There is a great deal of evidence supporting this theory.</li>\n<li>E.g. When people stared at a sinusoidal grating for a long time, their visual system adapted only to that grating’s specific orientation and frequency and not to others.</li>\n<li>E.g. Discriminating between a sine wave grating and a square wave grating has the same contrast threshold as discriminating between a uniform field and a sine wave grating.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 811px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/54e3e9e19b867983dfacb00ba0b5c2a0/fd28b/figure4-2-9.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 82.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAACc0lEQVR42j1TibKbMAzk59tmXqeTR8Jt8MFlMD44k35e19D3MorH2FpptZIDbUzX99poNal5mbGJoih8PLq+c7Ozs8PJZIw3jc23GW1NMOmJcy7qmpRkWReEiOIoSRIc9lIqPc3rYpy18wwzzofzEa3BoQeTsjzebzkOCJHleZJmz2fUti0T4n7/zIqibuqK0vDxjJM0TpKSVtpZBPJgANbjQJ6ypINSatIFKX/8vMVxKppWNM3H7z+/bh9104K5HAZe19o6D9ZacyHWfQcYDJdtm9d1f71wlxfloCZeN2H43I5DW1tSBhgcvsBGl1X1er/HURlr121zy4Ly4IEVPCvGEULUzeMZ3W4fzyiGWkAC72mjHuscrrEu6wrwhcc6TpPS+pTK+oq0xsmV1meGvBBsnFRV0apibSebTpYVbbu+aTsUjz2YUy4KUtVQkYuKskv54Ort8TqssxB52zYwh669HI6/7zTL4IpeUM6hNRRBmvtnaBw654Jt32Drts7LgppBG4aoqHnSBo2AYfPd56va/7QBW7blwiDnVTOEhSFnFCdoLDgjIWq+MF/4OQBhZN4P/D1y3w9QQJPiJLsKjuI4Lwg+cHhO2HxyPmvu/WAbOchRjdigVAyJ0mZUCnZms3IcYZ2Unfcbu15icnAVhGEohCgxXJSeSlJMEuQB2yRNKWOYAtyCRJrl7HwGIAIhsQkwt5gw/LDCo2nbK1ZBCGMsz+FZFAjNGKYFgZjvWZkTAv9gGAfKaOXTciRHs69sXHC8DUJIJ3tSegy4Qfmma+EKFhgqn7lpGwDwqtM0Q50OYmDWzt5Ydz1A+yWyMX5vrwn9Bz12lXAbRmH7AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.2.9\"\n        title=\"\"\n        src=\"/CR4-DL/static/54e3e9e19b867983dfacb00ba0b5c2a0/fd28b/figure4-2-9.png\"\n        srcset=\"/CR4-DL/static/54e3e9e19b867983dfacb00ba0b5c2a0/63868/figure4-2-9.png 250w,\n/CR4-DL/static/54e3e9e19b867983dfacb00ba0b5c2a0/0b533/figure4-2-9.png 500w,\n/CR4-DL/static/54e3e9e19b867983dfacb00ba0b5c2a0/fd28b/figure4-2-9.png 811w\"\n        sizes=\"(max-width: 811px) 100vw, 811px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>It seems that infants are overall less sensitive to gratings and may only see a rough sketch of the image without its details.</li>\n</ul>\n</li>\n<li>If psychophysical channels exist as suggested by these experiments, then they must be implemented somewhere in the visual nervous system.</li>\n<li>Integrating the cells discovered by Hubel and Wiesel with the spatial frequency theory suggests that simple and complex cells perform local spatial frequency analysis.</li>\n<li>Local spatial frequency analysis can be achieved by using many small patches of sinusoidal gratings that fade out with distance from the center of the receptive field.</li>\n<li>This modified grating or receptive field is called a Gabor function or wavelet and is created by multiplying a sinusoidal grating with a bell curve.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 598px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/1f18ec61fd037b9796f285de07cf77d6/0c69d/figure4-2-12.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 140.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAIAAADuuAg3AAAACXBIWXMAAAsTAAALEwEAmpwYAAAD7ElEQVR42mVUSVMbRxjVfwm+xLe4yoAstKIKcIKqYBNZwieHxTlw5MCFCxg4RMgum0iafR/N0t2aRfy7vO6RQEU+taRvevr1+/ZSmk6TZJrnqe+7ruuYpqHrWpom2M+yBK+EnhTHUv4ollBK2JpOWZ5ntm0bhjGbzWRZNk3TMk3PdTVN831f01TcCx0XpPymRCjJHJxlqee5lmU9Pj46jvvw8IArhqOxoijD4Qj6z58Plm0nBW/CwdA5mDEOdjmPTil1XVC6MSH+JIii2PO86TSZTII0zZIlHwowAzPYCSFxHDuOY5hmmmXYwq+m69iHgoXTWc6VZ2bQMsqgg01V4ZdmGiZ/laXAT4IgikkmwFmewxZC6TKY4m4ww2FVVWG2rhsIku/5hmH6k4njepblBGFIGZsEYUzos9mU4i7ss4mQIAjAj1t0w/Q838XXDxBCTTNs20nzLMmWAkZoRGhcgBEby7KRp8JOrDDEWwqDsbi3EJEe/o9UARyTCA/IMxyOogjWFocQHqQXdoEFn3mhLOQJHMMKWAVwkefxGBlWsWRZkSVlPJZVTXdcNxEGC5sRUQEmghkYeMvD7rhwAYSwvzAkDEMiZIGdLsAkBL7wmRe3w+sMeeJ1KCpRhKbQ0yfwvDwJ5WDEHDlbeMMLFglT5qKiQmFzyn1+CY4EmPJaYQzFBjZ4XtDiN0ec03Q2y9EzS7dPn8AxZXMwEoSw397e3t3dfb35enNzc319fXV1dXl52f+nz2GLeIP/BTPF9YPBYOv3re3tnVqtvrW1Xa3WNjaq5fK7brcrzE7+D+Z1hnjCyG/fv+1s7+zt7ZXXyx8ODur1eqvZ2txs93qHjHcgl5fMT+D7+0Gr1Xq///71r69PTk4r7yq1Wq3V2ux0OmyBXoAJr7Bl5vvBoF5vHBz8ufLLyvn5eaVSqVar7c12t9tDCjzX48lgbA4W5AQL/YySFOD6H/v7r1ZenZ2dlctl4JtNzizuJwV3AQ4FeYwVxRFaAgFrt9u7u7tvfnvT6XxEtCqVjUajeXj4qeB8NjuKMWtCwAAOoxDg/n2/0WggQm/frjabzfW18urq2traeq/XewlmLCaClptNYiRDkuXT09MvX/4+EXJ0dHR8fPz5818XFxeJEIEUFaYoEga14zq2Y2M+Yhigq4MQ/wGqHUaBhz+GmCWhx2eDj26JwhizsfTjx/d+v4/hOhqP/x0O0YzD0cgwjSEKejSSpDF6C92GA+gZQzckScaQ01QNY6Ok66qu6yh/CduFKOheFVMfwwhIbOBGtLoiKwCj53AYELRgCa/4eVWRZAkwbl4EA0OkPRYSCYGCTSgIUPFICPkP1IIjNL7ZtNgAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.2.12\"\n        title=\"\"\n        src=\"/CR4-DL/static/1f18ec61fd037b9796f285de07cf77d6/0c69d/figure4-2-12.png\"\n        srcset=\"/CR4-DL/static/1f18ec61fd037b9796f285de07cf77d6/63868/figure4-2-12.png 250w,\n/CR4-DL/static/1f18ec61fd037b9796f285de07cf77d6/0b533/figure4-2-12.png 500w,\n/CR4-DL/static/1f18ec61fd037b9796f285de07cf77d6/0c69d/figure4-2-12.png 598w\"\n        sizes=\"(max-width: 598px) 100vw, 598px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>In general, cortical cells tuned to high frequencies have a narrower/tighter tuning than cells tuned to low frequencies.</li>\n<li>Also, the frequency and orientation tuning of cortical cells is correlated as cells that are broadly tuned for spatial frequency are also broadly tuned for orientation, and cells that are narrowly tuned for spatial frequency are also narrowly tuned for orientation.</li>\n<li>Spatial frequency and orientation define a 2D space within hypercolumns.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 649px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/bcdba00b8e7c0fae9904dc83a279edc5/7762d/figure4-2-14.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 131.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAaCAIAAAA44esqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEDElEQVR42j2UCU/jVhSF/euQhvITRqJCU6lTmLKpLGELSSAJ2Wwn3vd9D/Qf9nsENbKsF/ude8895/hJZVV4vvv49Hh2dnZy8ovf8fHxdDoty6Jp6qIsNG0zHo9Ho9HDw/3zc980zaquum3HJZV15fr+22w2HI36/efhcPjP1dX9/cNyuWi7Ni+KOEnZMJ5MXl5fJ9OprCiWbZdluX3fSlVTB2G4lGV1s2HBVlVVPT/QdaPpuqKqkjTNy3LN2ygKw4hy0PkCb3StP3g+Pz8/PTu7u7t7fHw6v7iYLxaGZTmet1gtJ9Px62RydXXd7z/BTtP1siqbtum2WynNs+HL8Pv37/v7v337tn9wcLC3t3f4+2HvrudHkayq1zc3F5eXR0dHh4eHf/78eXN7y4xxkrx/vEtw84Kg/0zzi5Nff5+env748cfxXydQUVTVdhxNNyZv08vLy+ubq9te7+HxYTAcrGSZcaS6bU3bns0Xy5VsOrYX+IvlEsI8rJomimPP9+M0VddrFq7nJ0kSJ3GWZ03bIlhju+5gOBwMRrKiIsxk+maYFuB228VpIssK5LHhdTxWFMUPAjRr2vrj3w8BpjCPYLLRNDBrTUN8iEAKRbK8cD1vNHp5eR0vV6uVvDJME7m3H8KqJkoS7mVds6AQYjAPhJFU0Os6FmmeZkVBT6ROs6xu6p3PojNOcoe/aVkU3ug6DkOb3UmawYtxMI+6FhY6NuUEmIYg6QnJqq6pJRafd64dnTTPwzi2XcdxXRghGNn8ArOJtsSLTUmWcYctMaI8nFGensjBD82hIDTLcwGGnhimLPETYwmprK41Q3c9V2Q7z7FaN03yTDyB0Rz9kjSBnOhMPeZASSxR1uvFasWTHTgrcuwVzlkWOovJDdP3fQLeNI1U1NVKVnhHNujMRRX606Bu66Zr266jlizLvV5vNp+zM4xCJhLxhLYieJqAVwqJUFiQC9u2a4q3Da6QDcgDsy2bD8PzPQYWM6MQkvhhCIx87Zqwm0GEYG3Dpz+fz2fzGTAulNMN/csqOqMWqgZRSG2msj61RSH8EPHIUlhEceS4DjqBZOavk+R/VxETJDlhE/ziLG23W8BExQ98Tddg63y+hQsDCzCEkyxNs1wE8zNq/IVOWVVFXXNIsZVI4j0L5hd3fkjRNhKf4ny5ZFqs3Gi6H4RwJqE4xCXSilEWVpmGYbgivybiYXgURRKfKGQMyyQJ0OargjlgtCEMhFxE2XHWaxUYYAqJmPg+OZVMUdVyRRJMZNcMg/7gbcfVP5vSwhF/bQ5dwzSYPwgDypFwaRe9XehY7M69nau04kInXnHCrDdrzEIw2rBB0Ba5B+64EObaHUC8s8R4YgSe7OoCQ/MgCMThW4oP6j+LsqjBXRqnMQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.2.14\"\n        title=\"\"\n        src=\"/CR4-DL/static/bcdba00b8e7c0fae9904dc83a279edc5/7762d/figure4-2-14.png\"\n        srcset=\"/CR4-DL/static/bcdba00b8e7c0fae9904dc83a279edc5/63868/figure4-2-14.png 250w,\n/CR4-DL/static/bcdba00b8e7c0fae9904dc83a279edc5/0b533/figure4-2-14.png 500w,\n/CR4-DL/static/bcdba00b8e7c0fae9904dc83a279edc5/7762d/figure4-2-14.png 649w\"\n        sizes=\"(max-width: 649px) 100vw, 649px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><strong>Section 4.3: Computational Approaches</strong></p>\n<ul>\n<li>One group of researchers aimed to produce a computer implementation of Marr’s raw primal sketch, another group of computational theorists took the connectionist approach.</li>\n<li>Marr’s specific proposals about the nature of image-based representations (raw primal sketch and full primal sketch) aren’t widely held anymore and we will consider alternatives.</li>\n<li>Edge detection\n<ul>\n<li>Luminance edges signify either a change in the reflectance of the surface (object part), a change in the amount of light falling on it (shadow), or a change in surface orientation relative to the light source (object edge).</li>\n<li>In computer vision, edges are detected using the convolution of an edge operator with an image.</li>\n<li>Convolution might be implemented biologically as the receptive field of retinal cells are applied to an image.</li>\n<li>The output of the retina is essentially the convolution of the cell’s receptive field with the image.</li>\n<li>No notes on the convolution operation (multiply, add, slide window).</li>\n<li>Computing the convolution of an image with an edge operator is just a sequential version of what a sheet of cells would compute in parallel.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 812px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/84bc5496b11cdfffc9aa337dd802f3cd/63ec5/figure4-3-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 105.60000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAIAAADJt1n/AAAACXBIWXMAAAsTAAALEwEAmpwYAAADLElEQVR42lVUWWsUQRDevyVKonkzios5HwxoHl0hN5JoHiIogjkgO9k5erp77nt67k38eX49sxIdKkWn6a/qq6qvdtR2LT4hcj/w4ziKkyiKo6IQxNA8z9Z1FZeO64iiyEQOy/G0EJkocpGPmraB1XVVFAXnnDHKLX5+fr76cnXn3c7m5iYhOqUmMYll22Vdw0RZSnBRLMDdfD75OFlfW9ve2t7d3X29+mpjbWPlxcr4zXhrc2tjff3D+/ee71PGbxXF9f2iqiQYtOf38yAM9/cPDo9Ojk8+Ly8tLy89f/rk2Xj89uzsy+Hh8f7B0fcfP5HQdt1fl1fcspB/AUZm23EQlVJmUrq3d3h6+nUy+XR1dY1LYpqMW3GaIhvwwIiixAE26uZd3dSoPozQqjgIo7Kq0jSFD8AnirwgCKIoE0IW3EhDlCGQBMNEIRzH0Q3dsuzpVGGcz2YqiKiaTihFtVXT4LXreZbtZLn4D4w8wHueh6kwxhzXNSnDv+BsOTbAyAzA5dX1xcU3Pwwfa4ZVGEAh+sqp7djAU/xxWTChJugDDJup2t1MRQ8ea27a9uH3g6bpoK0oikGM2+nUIOT65mamqlNFiZMEqeoGQ8VUu6GEBbjtYC0GYFITH0IwzgyD6IYBA4UkywBGwbYthxL0tFHFAoxppVmaJEkcx71P/jY/jJIE79AUHDw/cDwvjONhZgO47e47ya0sAQE9ILM8RxR4nOu2lbTbdrCB9j/geQfFl1UZJ3FVVUmaYhOQFpc4Ixw6WvbjHYa0GBWuIJKmqUUPRirpsTllmSSp3KFclP1TCS4X2sJiycx3dzNFmZpSgxxaQs1Qi91/soUm9f0A24a2GcR0XA8eEkDboPORPDAKGGZj2RbF+lETK42Ga5qGDmPchJjYR003gIP4dYMgLmASjMDYYUzI8z1gcOjHhui8lw2DALhlMzZohsKrum4C3CuR6rqOtADLR5wjA/DyaV8OtkruHOMgbPT4O1WFH7mui+QyJ2PIAxK4kT8pizrJoNNBMCCsGQb4qLqBKkaSPaXDPgz8UZfv+8NiWT1/YKACaWmK9UwhgSxLsuwP0eMUmF42dQUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.3.3\"\n        title=\"\"\n        src=\"/CR4-DL/static/84bc5496b11cdfffc9aa337dd802f3cd/63ec5/figure4-3-3.png\"\n        srcset=\"/CR4-DL/static/84bc5496b11cdfffc9aa337dd802f3cd/63868/figure4-3-3.png 250w,\n/CR4-DL/static/84bc5496b11cdfffc9aa337dd802f3cd/0b533/figure4-3-3.png 500w,\n/CR4-DL/static/84bc5496b11cdfffc9aa337dd802f3cd/63ec5/figure4-3-3.png 812w\"\n        sizes=\"(max-width: 812px) 100vw, 812px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>The visual system calculates convolutions much faster than computers by implementing the computation in parallel hardware.</li>\n<li>E.g. The retina uses the connections between layers of neurons to compute the convolution at each location.</li>\n<li>These local edge detectors are first-order differential operators, but we can use more complicated local edge detectors to produce second-order differential operators.</li>\n<li>Second-order edge detectors don’t detect edges indicated by extreme values, but instead detect zero values with extreme values flanking them.</li>\n</ul>\n</li>\n<li>Marr-Hildreth Zero-Crossing Algorithm\n<ul>\n<li>This theory isn’t a serious contender for current edge detection algorithms, but it’s here for historical interest.</li>\n<li>First-order differential operators compute the slope of luminance functions (spatial position versus luminance) and can thus detect the position of luminance edges.</li>\n<li>Marr and Hildreth argued against using only first-order operators because they must be computed at every possible orientation and position, which is computationally wasteful.</li>\n<li>Instead, they proposed that a more economical algorithm for detecting edges is to detect zero-crossings of the second derivative of the luminance function.</li>\n<li>The second derivative is just the slope of the first derivative and is calculated by simply repeating the differential procedure but on the first derivative instead of the image.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 555px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/4f44280949de99afa9b186226f41c74f/cd039/figure4-3-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 151.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAeCAIAAACjcKk8AAAACXBIWXMAAAsTAAALEwEAmpwYAAAELElEQVR42k2V2Y4bRRSG/RZAIEjzACjhFrgIiCjcIBBwH4mr8AR2BogAiUiANCYz9njrvdu9L9Xt3TN5N75T5QkplcrVVfWf/yx/lXubzWq97tbrVde1337z9ccPH7x48et2ty3KnJXj8fjH7789fPDRD99/l2VZVZV1Xa2ldQB7/HSrbrXeNEp98fmjs7Oz8/NfDsdDWVWqVcebm8Hz/of3P/jqyZOyKpVSZVkK2woE4FWn2paPqq4/+/ST9+/d6/f72/2uKKu2625ubweD/nvvvvP4y8dFWdZNg9H1xoA3va7ratXWjWLj+fn5s2c/XVwM0yyjswLJq1eXT5/+OBj8nGU5eBwEzLoBtwLW+M3ucLi5JYY4TcMo9GlBEC2Xyzg5HI8A3u6Y6K3IFAZWXaPavKyLqsalzXZr+m6/hwH/WbzDbEzMjMLc6QZzhU9tm+dFGEXEBpgFnBc8ablD/s/ctg3oLM8tyypJWqMWCysIQhK2PxzweL5YMAlCVkrA2DC0AlaqgZwEzeZzwwNyt9+xvd3tCNi2ndvXrxnrpiYKkNLhNsxdp6q68jxvNLq+vBr7QYgJ3MFtXHFdD37bcenLONa00qXO4nZbYytaxlPYF7br+bgaJwnMnOJzMpnCPJ3OrieToijgPCkMsGohUlilE1uSJCQsCAI2yB9zx3Us21pY1nQ2S9NMR94Js4SpGqJFFYA5ysg8WkZGoa7nksLJdArtaDQiOkm+KPrkdiMK1bW8qzDDlkPaQxPkqUL6ItzFbJjJUFWTpNao940kKI9RvijASMJkW++fwPg5my/A50Vh2bZhgJk8hdESpOO6ADBEN1ZWBkzCkjQZjycYJ+bJZEKetXuk2rMthywyCTETJyc9vgVWeVkuFnaSprbjUC/uDwQwh2GIZpiwBR5ywIac+cltvvHwr7//4T6S1T9fvsQK7wmync3mVIhSU2fsplneaOcJXOSJ7lASLgURPHIH8ZNHZ6WDpHIEzCIRQY549PVpxW3SXNUlGUZbGB8O/x2Pry+GQyLHNW4Yaru8uhpfo1zTRqhCJ6zrtZ0Shakmz/MkSY1C0jSNosgkhkeLrTiJM1nPCN5cTKnzbDadL2boiSCpE3s4Rte2aKk0eTcrVpiQBYovK3XVcz1nPB7ZiNdxPN8nKp4eJoQdhZG8RJ6PJKVMBO84+mkKuGpQ9PzA00ibfCAPsOxiIggDjgZSqoBXDMlH+rbE2htOEUQPJMY5ym2Uo2Fozumwl/L6kedQHkOYWWQfn+VUFPVc17EpMX744h5gWxPyCZLRgDnquq5h5pyYjJc97io+S5wSaGCupPju+/gX6QaGTJFw9rWtSCaAcZuMAIMfpB8GwNmTsPnRMcPGRNIWhryTkYClCTPgoiyoATqjDG/+VkrRjox8FrqhrQJN1aiqbJrmP6kMkRdh6xltAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.3.4\"\n        title=\"\"\n        src=\"/CR4-DL/static/4f44280949de99afa9b186226f41c74f/cd039/figure4-3-4.png\"\n        srcset=\"/CR4-DL/static/4f44280949de99afa9b186226f41c74f/63868/figure4-3-4.png 250w,\n/CR4-DL/static/4f44280949de99afa9b186226f41c74f/0b533/figure4-3-4.png 500w,\n/CR4-DL/static/4f44280949de99afa9b186226f41c74f/cd039/figure4-3-4.png 555w\"\n        sizes=\"(max-width: 555px) 100vw, 555px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>The zero-crossing of the second derivative is equivalent to the maximum of the first derivative but comes with the advantage that the second derivative of a 2D image can be computed in all orientations simultaneously by a single 2D operator.</li>\n<li>That the second derivative for all orientations can be collapsed into a single operator makes this scheme much more computationally efficient and looks similar to the center-surround receptive field of retinal ganglion and LGN cells.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/891947e38db7319c019b39332f1e243d/2f8cb/figure4-3-7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAABLUlEQVR42iWPS1LDMBBEfbVwhLDKFeAKZA9U8I5AKnFiSZas/1/merQT1agkTam733TL0trScinDMLzt97OUMSVKqRCitlpbUVrfm9FaJ8SstQ7r8j74rrW6/C1SqcPh0Pf96Xw2zl+G4Xq7pZygmZWSMDBWSDlxQdlkfUCAj6mrtaCwv4/Hr77X1iljKGM3QkKMKCGV1KsYHTZxyrhUJuUMgy7lWFsjhGy3z5vN08/vCRSIRXSIIaWkjYHGWPuInYTwMSIZ1eWSgA3O3W738vL6/vGptAEhUCEuJWM4KI1zs8LwCncXY64F4Z3WyjoLqxW+VbSc99Y5H8J9nPK4W+8wAijwRDnvYNcNwwXMjIGIcs6nCWTTejDq4WLNOI6EjI8/1+t1JCMXYlp/8n8b6xFUhxTcnQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.3.7\"\n        title=\"\"\n        src=\"/CR4-DL/static/891947e38db7319c019b39332f1e243d/00d43/figure4-3-7.png\"\n        srcset=\"/CR4-DL/static/891947e38db7319c019b39332f1e243d/63868/figure4-3-7.png 250w,\n/CR4-DL/static/891947e38db7319c019b39332f1e243d/0b533/figure4-3-7.png 500w,\n/CR4-DL/static/891947e38db7319c019b39332f1e243d/00d43/figure4-3-7.png 1000w,\n/CR4-DL/static/891947e38db7319c019b39332f1e243d/aa440/figure4-3-7.png 1500w,\n/CR4-DL/static/891947e38db7319c019b39332f1e243d/2f8cb/figure4-3-7.png 1689w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>One challenge for edge detection theories is that luminance edges occur at different scales with some changes being slow over broad regions and others being rapid changes over small regions.</li>\n<li>Marr and Hildreth also paid careful attention to how their edge and line detection algorithms might be implemented in neural hardware.</li>\n<li>They suggested that oriented edge detector cells in striate cortex are actually zero-crossing detectors.</li>\n<li>One challenge to Marr and Hildreth’s theory is that it doesn’t explain how edge information is integrated at different sizes or scales.</li>\n<li>Scale integration problem: how do different edges at different scales match up?</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 852px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/c5ab064a741b585471730a0fb28b4c0f/47ff6/figure4-3-12.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAACI0lEQVR42jWT65arIAyFff/+PNNZrRcQrYKKoqi183bzAXMQWJDLzk4i2ba5dXXbtq7rvK0Lu3ML12WZka9B4tZtdUgROWfneV6sc2iXbPMODRD7vm5+9fvmd49zhAsjYaUA3vvNh0s8bxm6ebYsgAF3McQch7Vpt+kWDFa3RPto7TLExphhGEaWMZrPmK7vOSCwdhzHgTVNbKPWGhvOgxnmZc4C2y3m/H8G2nCLzPfdM32cx3kcx77Hyfl8nxlwbdt2Xfd6vdo4et03bdukT6m2aZI2jaYJ8q4L1DKyIHWCQA/gCO5jhAOf63oToQX31b7f53ken89lpzGoPld2//qqZU3mZVkSlsSw5iqlzPO8yAtYUJKiLHQsDbwAIvla1RRsVEqWZUG0qqq+799am8fjARHCNo0i+Z+fDx14Pp5CiNvtRmFDAArmfejwcYTCQJ7koES/oAfPz88FBAeKBAp9wpMd4XWdGX15PL6lEKlg1k691tM0UaPQpOFv0L9xGon/JzSGHyabRnP/+leVJWUkQ+LWtYIe9QZOSFHXkgzJiCTJVohKSlEURaBt7VBVJbq+75VSSNs2UGClxmDKv9P3HfnTNq2DGQYUKznn+BNK1hIUjEJMIckjnYFQqk50QIELLVgWmw2DFqJARNpMzCoRGIKS/pk8f+KJQ+xlA20GYXhY2TQZnFEz6S1maAmign/JA6KEvJX4PF16qrwMJPvufwF3A3kS/VAujAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.3.12\"\n        title=\"\"\n        src=\"/CR4-DL/static/c5ab064a741b585471730a0fb28b4c0f/47ff6/figure4-3-12.png\"\n        srcset=\"/CR4-DL/static/c5ab064a741b585471730a0fb28b4c0f/63868/figure4-3-12.png 250w,\n/CR4-DL/static/c5ab064a741b585471730a0fb28b4c0f/0b533/figure4-3-12.png 500w,\n/CR4-DL/static/c5ab064a741b585471730a0fb28b4c0f/47ff6/figure4-3-12.png 852w\"\n        sizes=\"(max-width: 852px) 100vw, 852px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Illusory contour: the experience of an edge where there’s no physical luminance edge.</li>\n<li>When later stages integrate results across different sets of features, the retinotopic mapping makes it easy to access features at corresponding locations.</li>\n<li>Another computational theory to explain the cells Hubel and Wiesel discovered is the theory of texture analysis.</li>\n<li>Texture analysis: how the visual system defines regions that differ in the statistical properties of spatial structure.</li>\n<li>This theory argues that V1 cells initially segregate regions based on texture information, which we will return to later.</li>\n<li>If we combine the various theories about local spatial frequency filters, edge detection, scale space, texture analysis, and neural networks, then we can explain a lot of what the retina, LGN, and V1 are doing.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 811px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/d904f6753efa34bc78b0635edcb952ac/fd28b/figure4-3-20.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96.39999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsTAAALEwEAmpwYAAACfklEQVR42n2U6ZKjMAyEefdJgHAFA8YHNwabI7PPuG2c7MyvrVJRxPizWi0rHuf8Hvh+GNScPaLodrslWUZZnWZZnMR4YUIkacIFl41s2oaQvB8GpdS+b15RUT+MgigOo5iU1SNO4zTDYpxkQRg+SZEX5df9nhdFkj2x5+YHRUlF001KebM2iH5WhNYVE7WQWU5k14u2p4wP0/wsypIyyiUOwtdxXrBotuN4vTylzbRq8Mu2j8vaT3M3jJRzJqRoGmjmTYvFedHmOLfzZfZjv56IN4wAaY8w27yskB2EkR88oiQdZgVM78e67fiqLwyxHecP7PQrs03LWjEO0g9ClCCaFmpF22EneLPtwMx+vuH5E+7zqBbKBbJ+3f2C1k03dOM0zgppzSettuJPb/kFO9k4YpgUaq4YG9U6r9pptmk/sHnDKPKT9n3Eott+aLqewbG+BwyfHKx/4PMNq4t8v5gNDqHORxxDOQS3/YiCYcQv8mOY89CF+hiG2wS3EOgTeAm31vXy6b+wrWLV0mqWZc36cVKrPr//OFL/Um5h9NDVs14e2lOw4zgHtbTDaM6XBS6rnG36wqxhgJt+GJW6Lp3C3VqMgex/QrAbT8e7Rcsb22prWBihn0FGyryo4vQJtaSkz5zgZ1UzPDLMREVLWqMKrHAhC0oxQkI2HrxJc5IRjEpdc2FvP+N2K+aEMvtkHOsgcc8AMy6BIISUHn7jA8UwkQL3CSdjNw522TAhmCp0jl08bh7esQ64aVuvpBQYVouyojYzscddItFelECZ/R/AdmSGBARSlFXddp39M0BCEACYrafG9AOAfnQYowv/JmVnGHceXcQ7mofQxvwFF6y9quNGQ1kAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.3.20\"\n        title=\"\"\n        src=\"/CR4-DL/static/d904f6753efa34bc78b0635edcb952ac/fd28b/figure4-3-20.png\"\n        srcset=\"/CR4-DL/static/d904f6753efa34bc78b0635edcb952ac/63868/figure4-3-20.png 250w,\n/CR4-DL/static/d904f6753efa34bc78b0635edcb952ac/0b533/figure4-3-20.png 500w,\n/CR4-DL/static/d904f6753efa34bc78b0635edcb952ac/fd28b/figure4-3-20.png 811w\"\n        sizes=\"(max-width: 811px) 100vw, 811px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>But what visual properties are computed from the output of these regions to extrastriate regions remains to be discovered.</li>\n<li>Our current understanding is that the representation of image structure in V1 may be a kind of unified image-based representation in terms of the continuous output of filters at different positions, orientations, spatial scales, and phases.</li>\n<li>E.g. Each cortical hypercolumn seems to be representing something like the Fourier decomposition of a small patch of image.</li>\n<li>Local spatial frequency analysis using Gabor or wavelet filters has the advantage of preserving most of the information (up to some level of resolution) without committing the visual system prematurely to a set of specific primitives.</li>\n<li>Why would the visual system represent image information this way?</li>\n<li>One possibility is that the representation exploits some structure in natural images.</li>\n<li>We can imagine that each receptor in the retina represents some value within a range of luminances, so every possible image that can be represented by the retina matches one point in an hyperdimensional space called its state space.</li>\n<li>The actual number of possible images in the state space is mind boggling as the retina has more than a hundred million receptors that each have a nearly continuous range of luminance levels not including the three types of color receptors.</li>\n<li>What does this state space have to do with the statistical structure of natural images?</li>\n<li>An important fact is that the set of natural images constitutes only a small fraction of the total set of possible images.</li>\n<li>Thus, the visual system can exploit properties specific to the space of natural images.</li>\n<li>How are natural images distributed within the state space? Random? Clustered?</li>\n<li>If natural images tend to occupy restricted regions of the state space, then the visual system could take advantage of this structure to increase efficiency.</li>\n<li>E.g. A high correlation between the luminances of adjacent pixels.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 536px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/b7645b47d1196305fcabca70bdf55f82/2d920/figure4-3-22.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 164%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAhCAIAAABWXBxEAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD/klEQVR42mWU63KrOBCEefP1j91U6iQnNmBuNmBzsQEJAbbPA+43Eo6ztUSlYKFWT/fMyDPTaMw4TQwzz2ZepmWZH38ej8c9TZNzdWJda3W7LazPjHmSbXZ4IDV/ZnTDTEZp9fHxyfv7+3vXd3ESx1F0f9z5ZGZ4jHE7zejB/I3kwzQTg7lcL9qMeVEopQY1cDqcZp7WYfEAPXYTmDsCwLTM/dD//vpqL+3n52fdNHmRH/P82nWciCjZLxBR6j1/SDwSzDwNWqVZBjgIw6ZtTudTeTpVTV3VNfxuv3VKe4v1wDiwxaM5jhOiDcOQKOqmvnTX+basNCvYrGCnUzSPNvhRY1U/DH//89a0bZal5am8/3kst8VYU1zYotnJYBWM0hphZVkQLTpBngi45qlO57P4hy1ikPmPZgEbrcaxG3pEHg6HY36M4uRwPPLKDDKKI+yQerClIalyYJfhyWYCV5b7bbndttsdVlm1M/PtcXfR2tTocVRPZjuvmbAS2ssFSmzr+35QSj9ryVUISAc23x662lxuM+KRmud523LIBf+dtPEH0oKt9O8jABMV6cGq/X6fZeg98k7mKRiXC/0CP8vlZw5JLIUVJwlJKoqyqqqiKAjhCRbBehycZvOD3JBP4idUbKcxXMyzNVWQklCh1bp/lafjV6q/UsXXy7W7FqeStINn5hmG/idY6c6bhdAO28/tpcEnmpBUZ1kW7ve8MP/6+DifT5SksmClewF/c87PI1xjQhJFUd1UqGC79IFULzBpUpBKXT0nWJJuW9T5YXeo7HCQDktTzKex1SrV0qqrgGcHdphnGbgagsEPfMDUnE3vKljxxYGfzKNdVeKZHpq2xp6/Npu3tzcOraWZq667vvAr2ApeCaWA+DgMco19bDYbbD+fz0VZJknSNI2xl6Gl6V5gl2ck2LOFijampFYi/o2vbtd6+B/4OTgeQsKgvKWUxA6ZR+uItfMJdoa5VLFeVWe5aeOIwvbl2aVZGkXyzie5mNUwKHF7GC4exnCuZF+WZGAMVdXJPwqtZWKA7PvOtjHk6LDMnP31+7MsS9tDiVxA8tDLUmNFWdAhSZqQ80g2pFJwYViWx7oqPIouDAOKib7h6uHB2DiOA99nzo5HX164kWIwaZKQ+R1a0iTPMw/OyIqUq+so9YxCypsXwEma7vnN9zThGhcvon0QBpyVprGXWT9YhdMCImYOlnWYDwfwNAaRM8CwCHmS8ikFnAVByEWx83028Q17eXY7P7LRBoHwbLdboV3HHqXCjB9YhUMECRQeguUg32Jgc6Ik+Dji9K/tlvX9PgwC3ysKBGJtZq1CSYKfHHSQgJPtbkfkqTR2yE9ywSB1lGDb1v8CkgD5UzIiAI4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.3.22\"\n        title=\"\"\n        src=\"/CR4-DL/static/b7645b47d1196305fcabca70bdf55f82/2d920/figure4-3-22.png\"\n        srcset=\"/CR4-DL/static/b7645b47d1196305fcabca70bdf55f82/63868/figure4-3-22.png 250w,\n/CR4-DL/static/b7645b47d1196305fcabca70bdf55f82/0b533/figure4-3-22.png 500w,\n/CR4-DL/static/b7645b47d1196305fcabca70bdf55f82/2d920/figure4-3-22.png 536w\"\n        sizes=\"(max-width: 536px) 100vw, 536px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>This leads to the question of how the visual system might exploit such structure.</li>\n<li>Two possibilities\n<ul>\n<li>Compact coding: minimize the number of units needed to represent the output of the receptor array.</li>\n<li>Sparse distributed coding: minimize the number of active units.</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 533px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/7616907416bf1e5d21643b4d2d3b6654/a1aa0/figure4-3-23.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 162.39999999999998%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAIAAACdAM/hAAAACXBIWXMAAAsTAAALEwEAmpwYAAADxElEQVR42n2ViXqiOhSAffapta2jtnVDEQICruyr9u3mP8FSne/eSY+WJPw5e+xVVVnVVf0tTOMkCnaBrWzf3x6Phy3Dc/M8+3lHCw+9qiqEr0rZaOoojo/Hk/zJOB6P+9PpwBHMi7LQcH07pakFLjUsfFNvrA0yGo1//XoavLy8vr29vr6Z1sa2VZzETdPUSKu5g8uyQJgrZc8Xi6Wxmk7nyHxhzGaLtWkieZFrsm5JgQVree18kia86LqurYcfBBb/lI0Dl+u1bjXfROC8EClagSdaK2NlWzYOeJ4nZyh1zyDiOnBR5PmjZHk+eX9/wufBy3A45DsMzwJUVWtzZ2aPHORFdkdmbJibDczv0Yjvj4+PWuewzVBLdnCq+RtcFGK85/vjyftw+PttODSMFd5W9f/CKcqxv3WbjSiOdrsd+eWUMIyay+UG64z+wFkWZ1mC8g5uLg3GE7b9YRcEQZqm16+r9rn8Bywklp/DM0JupMROx3MY8pwkcf2tuavInibTe80fnx+vb68EfDafU2PT6XQ8Gc/n88uloSge4DxPOp/bpb9i0z6wBXxT3q48BKwsqN7j6YCfOlqH3X7nB/7+sN/vd3wFvh/HEc7fDq3KDs4xCfcA3K279bY8wDiuczgcCN5OmlQRi7qz6BvOdIap+5o4o1aU7/dgqA308DyfgF5atbqL/oKl3XlvtV6vzfVT/+l5MKA8nwfPLFAqbDWNoLQD8mC2BKyuzucztSn8er1kGMvVymAwbW3+TnVxB+tUYQz5xFSlHIynpfDf9z2lFB6wVdXlfaq7PAsPHEUhJWlbyvM93dVqu3UdpYhDkkT1fW0/wikw3Uf/YqzjOJAbfMCLleE4ijzpRD9UWNwVGTDmfU4/8Xk8Hvef+0/9/uR9gs/z+SwI/Kb5Dxi5wVyUBnpcd7FcUqGj8ZgrjWybG5Nsd+1xC1gHtzxh930fm8MwxGceOM6yLG5hSqAjb6mCTFOxPMsy7XbJHYh7X19XaazzqW7I3+l6vXyHqvjJMwnQkiQ0bprxIWBklThZ3OHmRjmKBxJW3i7JvL32mEqeM0FazTllyLW9WCwIErfX0jBoydFoxL1NztvCzHXzimaqn54nQ4zT6RyR6DCM41jOS2JMScSpNIxC7iZZSeXmwFA87c1m08lkvF6vNnoQLXSapunIUHyIFk1GtLn9+c2zbIs+ZZ1w9lypH7rNlnf5xxuuC8AzWxy39TwASOH1uiUzfkO9HoUkgZEyWrHBEahlSmFv9DpTuclwStwSieJQ7rnz6Q9anPXAEvAWzgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.3.23\"\n        title=\"\"\n        src=\"/CR4-DL/static/7616907416bf1e5d21643b4d2d3b6654/a1aa0/figure4-3-23.png\"\n        srcset=\"/CR4-DL/static/7616907416bf1e5d21643b4d2d3b6654/63868/figure4-3-23.png 250w,\n/CR4-DL/static/7616907416bf1e5d21643b4d2d3b6654/0b533/figure4-3-23.png 500w,\n/CR4-DL/static/7616907416bf1e5d21643b4d2d3b6654/a1aa0/figure4-3-23.png 533w\"\n        sizes=\"(max-width: 533px) 100vw, 533px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>No notes on principle component analysis (PCA) for compact coding.</li>\n<li>Sparse coding is efficient because it uses fewer, but more important, active units compared to random or dense coding.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 803px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/ae96e2f81a0f87c4b4385a58ec2637ee/e1031/figure4-3-26.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 110.80000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAIAAABPIytRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC4UlEQVR42k2Ta1faQBCG8z9FOAqSe7IESCCo9dTvlZt+qB/EBAK0tkC9/b0+s6s9nTNnmcztfXdmsa6vv159uRoOB2naT9O0o1Sv2w2CoNfrZVna1RLHcRRFSilOz/P4bDQa0+nEyvNhlmWdDlWK03PdjupEYYSddJMwDLF936edilXgB9i0Ozk5mc1mVq/XTTqSTR4tHdsOg8ButwHxPREKbNs2huM4dCf1uHY8nU4tYPv9tJtIC0OPRkHgy28QUtBNkkCLIhqGIBNqNlu3t3MLFwwlNQTS9PY4sekFTXHaDjX0govJPD09nc2mFheQC8dKw4YwDLVQI8w1juu4vhbIk0CoVqtNJmMrSzOIUcrAzEjoIrBK4QfF9AVAXymK9SxazeZ8PmdgPX1PgQLQdRyTxM3jKDYUAIw0HTMwz3Pr9fp4PLbMGs2oIOa6LkC+XqYslhVEsZBnZ1EEZ+ox2PN4fGMlelrkfSBTrBQG24Y2LRLOCOpKtuAHZoXsecIjAVlWYsQPmE34KZDRjGJHxNan0263eQtHR0eCPBgMks+BxXoNSou8MPErQ4SQ2WigL9JqtmTa/X4/1gNjvebpYvh6QmTDhZGdnZ3hNOJqYWA3N9+kONJdYWj2G+uBKT1FDDNkjI9Vu7LzRr0hxZeXF4MsG/K3yjKugJHn+flohI5G+TAf8sMTJm2U5wMdvTg/50Z3d3fWbvfr5eX5cNijz89/UD5fX1/+t//p29vr+/ubUWzr/v47WhSLonhEf/zYrlbL5bJcrVZVtVqvq6VIua6qzWaDsdmsyamqitMiVZcVpJZlQZiTvMVigY3zM1ppv1Q+Pf3EScgqS34qkwQEUCifi4eHsii22y0FdEExUaLGw2lBeCVwhWH7+CiAfBsKdDGplIEpvlJy1us1Hik2jTXnpS4WfNPOnNSjNCJaMQztF9rkY5V6WovFg9h00e3NOCg7HHb7/X63+42avWDs97u/IROa8yJqyfUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.3.26\"\n        title=\"\"\n        src=\"/CR4-DL/static/ae96e2f81a0f87c4b4385a58ec2637ee/e1031/figure4-3-26.png\"\n        srcset=\"/CR4-DL/static/ae96e2f81a0f87c4b4385a58ec2637ee/63868/figure4-3-26.png 250w,\n/CR4-DL/static/ae96e2f81a0f87c4b4385a58ec2637ee/0b533/figure4-3-26.png 500w,\n/CR4-DL/static/ae96e2f81a0f87c4b4385a58ec2637ee/e1031/figure4-3-26.png 803w\"\n        sizes=\"(max-width: 803px) 100vw, 803px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>E.g. The receptive fields show localized, oriented structure at different spatial scales that look like Gabor/wavelet functions.</li>\n<li>Two constraints that result in emergent V1-like receptive fields\n<ul>\n<li>Information in the image must be preserved by the output of the recoding units.\n<ul>\n<li>E.g. Remove redundancies in natural images.</li>\n</ul>\n</li>\n<li>The recoding must be sparse.\n<ul>\n<li>E.g. Increase the signal-to-noise ratio.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Although we’ve made progress in understanding the neural coding of images in cells of area V1, we don’t know the details of the process.</li>\n<li>Our understanding so far reinforces the conjecture that the structure of the visual system evolved to take advantage of the structure in natural images.</li>\n</ul>\n<p><strong>Section 4.4: Visual Pathways</strong></p>\n<ul>\n<li>A new architecture has been proposed that structures the visual system into different pathways that processes different visual properties in parallel.</li>\n<li>E.g. Form, color, motion, depth.</li>\n<li>Early studies suggested that area MT was specialized for processing motion information while area V4 was specialized for color information.</li>\n<li>Where does the visual system separate out different kinds of information?</li>\n<li>We already know that there are functional differences between cells in the retina and these differences are maintained in the magnocellular and parvocellular layers of the LGN.</li>\n<li>If we continue following the pathway, it splinters further in V1 and onwards.</li>\n<li>Livingstone and Hubel proposed that M retinal ganglion cells carry information about motion and stereoscopic depth, whereas P retinal ganglion cells carry information about color and form.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/da994/figure4-4-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB20lEQVR42kVSaZeiMBDkd4/j6D4/eKwieKIMIiGEK3dgZn7eFrhvN68IhHR1Vx+exXLOWNvyluT5M8s4565zOc0pzdu2lVLUTVuwgpUlpbRpWy5EO4B7xtiRyaXWMDqeL7gzzhpjhBBKKRizqgoPB8qYNAbOBox8TxmjjQETFkmawl/X99CCCHALCff7fbvbbTa/T+czzITWXCkuJeDh/EJG8vSZVXXDWCm1qtum65yU8ng67nzf3++xZ4QoyAQTfKW8QZkxXMjLNVouV+HheDyd4yRBhl3fAckj2WH5fhgOl1D0NzjIEuQx8sfH/O1tAnm0YEgBmf/8fDvnuODRLdoHATwEQZDTAsH/k7W1+OXvg9V6Qygtqyq63S7XKyEEmuumRheCMIQLuFXa/Mvc00ZDG9pyj+PH4wFfJCfr9Xr+aw4mPuI4LqsSLiAYQHWRptBj5FdJAWuNVJKVbLvdTSbv0+l0NpstFgukap3t+851mIihr6/uvMjDGb0BHOrrXBRFqxVCbrCWy+Vnknx99QNvnCVAjWRU2quqEhMCoDAoEnaBkaprjFTTNPjgr9/jAzMoRwJlXQNeniNfSouCFtiH93BiBfJEMAwpIRlsMixC0ufzkaYglLhumj/8PdHDmV/AVAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.4.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/00d43/figure4-4-1.png\"\n        srcset=\"/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/63868/figure4-4-1.png 250w,\n/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/0b533/figure4-4-1.png 500w,\n/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/00d43/figure4-4-1.png 1000w,\n/CR4-DL/static/7b85124e8904943dbaf00bcd79618185/da994/figure4-4-1.png 1487w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The M and P cells project selectively to the magno and parvo cells in the LGN continuing the functional separation of motion and depth from color and form.</li>\n<li>The magno and parvo cells project to different regions of layer 4 in V1, which themselves project to different higher-level layers of V1.</li>\n<li>Layers 2 and 3 in each hypercolumn of V1 are subdivided into blob and interblob regions.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 812px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/f1f8f62c3facb8529591d1235aa2358b/63ec5/figure4-4-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 77.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC3ElEQVR42iVTa1PaUBDN/6j2B1g7xX4pjthPpQhK7YeAzmhFneKDh1GSgCS8XyL4QDuQFyQQEggC/sae6J07O5u9u+ec3XtDjCyrp3YTiUSADLp/uB1fHH6/n02yJycngUBwbc31dWXF89Pj9Xoj0cjzvyfrxZrOpkNjOBqPiKFpnp2fJ2iaYdmj42Mk/Q2HDw+PaJqOxWIez8bWlt/tdi98WMD6vLz8fX09l8/NXufWxCK0fp+iLvOFQuv5qXZT4zOZYqnUsNdtoZgHKMdlqMvLVeeq1+fz+385nauflpaSqdT89ZWYTKfRaBSS7h/uH1utcqVSKpcLxQIcjuMTCZpNJhmG3dvbOz09PTs/Y1gGfTm/OQe6ToyscUcQDg5C8Yt4pVpBTaFYRBl8yIvGojzPQ0u1WuN4bv/PPiJo0eFYeXh8RM+GOTIwoVDo8E1zsX5b53g+n89nc9lardq8a97d37Esa4PyHHVJRWNxiEePKB4ZIxNSfb7NZCqJ5qu1WrlSKpYKXIargvGmBsRcLkdRFND5DL+zu7vh9aFHm1k3Bs1mYye4gwzAp7l01nYKaC/NXYMtEomg+Zt6HQU0w7hcroNQaDqfEbox7A8wcCocDuN6cBaNxS4u4thXiSuSDGSz2dR1qlQuMax9eZubWx8XFxvNhl08nrxogwFGkub463QaglMpsPEMw8TicQwZDwBHNENvb/8mySC6JUkS72SMe+7rAwzcmkzUfr+v6+YII7A3fLyfwXDY0zSgwyLQVVVZUXRdn8/nMzA/tFq3zabS7YqSJMky3ky70xFEERFV0xBERNXsGjhaX4NjmAYcwxgSgiQJogQLbFnpirLUFjrIAwDiPVUFIizmIivyu9WH+gB6TZNoC8iRwNMRRVGW8WnDSRL47aAgdN4S3nPanTaWqqpKV4EcAlTA7vZ6sDiGlRQFDshFSQSVZY0xCNM08Rthj60xtmkaljX+D8VsylsDur6IAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 4.4.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/f1f8f62c3facb8529591d1235aa2358b/63ec5/figure4-4-2.png\"\n        srcset=\"/CR4-DL/static/f1f8f62c3facb8529591d1235aa2358b/63868/figure4-4-2.png 250w,\n/CR4-DL/static/f1f8f62c3facb8529591d1235aa2358b/0b533/figure4-4-2.png 500w,\n/CR4-DL/static/f1f8f62c3facb8529591d1235aa2358b/63ec5/figure4-4-2.png 812w\"\n        sizes=\"(max-width: 812px) 100vw, 812px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Four functionally distinct pathways to higher levels of visual cortex\n<ul>\n<li>Color pathway\n<ul>\n<li>Retinal P cells → LGN-parvo → V1-4C<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span></span> → V1-blobs → V2-thin stripes → V4 → …</li>\n</ul>\n</li>\n<li>Form pathway\n<ul>\n<li>Retinal P cells → LGN-parvo → V1-4C<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span></span> → V1-interblobs → V2-pale stripes → V4 → IT → …</li>\n</ul>\n</li>\n<li>Binocular pathway\n<ul>\n<li>Retinal M cells → LGN-magno → V1-4C<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span> → V1-4B → V2-thick stripes → MT → …</li>\n</ul>\n</li>\n<li>Motion pathway\n<ul>\n<li>Retinal M cells → LGN-magno → V1-4C<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span> → V1-4B → MT → MST → …</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>This complex anatomical structure may be important because it defines four pathways that may have different perceptual functions.</li>\n<li>The hypothesis that there are four functional pathways is controversial, especially in its strong form, because the pathways aren’t completely separate and there’s significant cross-talk among them.</li>\n<li>Even if the functional separation of the visual system into four pathways is correct, it isn’t clear how we go from local oriented spatial frequency filters to the detection of faces and motion.</li>\n<li>Based on our visual experience, it doesn’t seem right to split experience into different aspects because we don’t have separate experiences for color, shape, and motion.</li>\n<li>But on the other hand, if people are asked to list different aspects of visual experience they likely mention color, form, and motion as being distinct and able to selectively attend to.</li>\n<li>One solution is that these different attributes are initially processed independently but are united at some later stage.</li>\n<li>One theory argues that the unification occurs through the action of selective visual attention to a location. So, attention forms a kind of glue that binds independent features together.</li>\n<li>But this doesn’t match our experience since unattended objects still appear whole.</li>\n<li>If the streams of visual information processing are truly separate, then there should be evidence for separable effects of the color, form, depth, and motion systems.</li>\n<li>One source of evidence has been patients with lesions to one of these systems.</li>\n<li>E.g. One patient lost her ability to perceive motion without any disturbance to her color, depth, or form perception.</li>\n<li>Such selective loss of function can easily be explained if there are separate brain pathways for processing these properties. Otherwise, it’s mysterious.</li>\n<li>Other pieces of evidence come from experimenting with the separation of color and motion information where motion isn’t perceived when color changes.</li>\n<li>Similar claims have been made for the separation of color and depth information.</li>\n<li>E.g. Changing light-to-dark shadows to red-to-green shadows greatly diminishes or eliminates the perception of depth.</li>\n<li>The separation isn’t precise though as color seems to contribute to the perception of motion.</li>\n<li>It seems unlikely that clean perceptual effects are likely to be obtained and that the “four pathways” conjecture is a vast simplification of the true nature of the visual cortex.</li>\n</ul>\n<h2 id=\"chapter-5-perceiving-surfaces-oriented-in-depth\" style=\"position:relative;\"><a href=\"#chapter-5-perceiving-surfaces-oriented-in-depth\" aria-label=\"chapter 5 perceiving surfaces oriented in depth permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 5: Perceiving Surfaces Oriented in Depth</h2>\n<ul>\n<li>Interpreting image structure for visible surfaces in 3D space is one of the most important steps when trying to solve the inverse problem of vision.</li>\n<li>Two of the three spatial dimensions of the environment are explicit in the 2D retinal image but the third dimension, depth, is lost in the process of optical projection.</li>\n<li>But the fact that people are very good at perceiving their 3D environment shows that depth can be accurately recovered from 2D images.</li>\n<li>How does the brain recover this depth information?</li>\n<li>Two problems when perceiving the spatial arrangement of surfaces\n<ul>\n<li>Depth: the distance from the observer to the surface.</li>\n<li>Surface orientation: the slant and tilt of the surface as seen by the observer.</li>\n</ul>\n</li>\n<li>Slant: angle between the observer’s line of sight and the surface normal.</li>\n<li>Tilt: direction of the depth gradient relative to the frontal plane.</li>\n<li>Why should we care about the nature of visible surfaces?</li>\n<li>We should care because visible surfaces are the environmental entities that actually interact with light to determine the optical information projected onto the retina.</li>\n</ul>\n<p><strong>Section 5.1: The Problem of Depth Perception</strong></p>\n<ul>\n<li>How does the brain know where light comes from?</li>\n<li>With only one eye, perfect depth information is ambiguous and impossible.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 807px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/6f150f5ecbedd56d9fff3166bcacf277/d2a60/figure5-1-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.60000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAABoUlEQVR42mWS6W7jMAyE/ebbX11gnRZo4cSnLtu6bKXP2E9y2mRbgKB1cMiZsap0TXu6R7petTFd32/7Hrbtv4ixRPAhWO9c8NVvsFSqOZ+p2NJPvP8NZsg3mPWxVUavzsU9/ZjsC9h5T64eAd/rwjlOUq7WxoN/hh3gmMHhC3xgIPwIjjvTolAShgUfH8mDDPfJKVlnobQXFQW5xdt8Ufjn+fHOPxBVnvmRHe6Hvm1bsywZn8DnoGnYdqEUACgs63JzseArSkHWp5OZZ0z+8/SkjFFak/U8k6UmVF3X/+qa1stqV2eddxkMExfC2/u7yH+oOb2+IImTUYjFWgwbxWSde/77zAKOuMj5jTZ7rMK9fhjGaaIXVInsbZGNN1kCetMOTfpydVhQQfV8ufAwMMZ6D1VCak3M67qsK1dkkyVoKJeFgT90qqZpLm07jCM8O6ZP0yQEX14odSxwUQhBQdt2NOqGnho0kiuoctR2HdWU9uMoywWZwMt5WUg0ogV9hZTIPmZUVIAfy4ZMF6m0y0LjoXYrDwEvfFFbzvltvDD/CYi8GzQ9YXttAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.1.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/6f150f5ecbedd56d9fff3166bcacf277/d2a60/figure5-1-1.png\"\n        srcset=\"/CR4-DL/static/6f150f5ecbedd56d9fff3166bcacf277/63868/figure5-1-1.png 250w,\n/CR4-DL/static/6f150f5ecbedd56d9fff3166bcacf277/0b533/figure5-1-1.png 500w,\n/CR4-DL/static/6f150f5ecbedd56d9fff3166bcacf277/d2a60/figure5-1-1.png 807w\"\n        sizes=\"(max-width: 807px) 100vw, 807px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>However, only perfect depth perception under all possible circumstances is logically impossible. Human’s depth perception isn’t perfect for all circumstances so there’s no logical contradiction.</li>\n<li>E.g. We perceive depth in videos, photographs, 3D-glasses movies, and virtual reality.</li>\n<li>The depth we perceive comes from looking at images that are completely flat.</li>\n<li>Our depth perception is possible because the visual system makes certain heuristic assumptions about the nature of the external world and the conditions of visual observation.</li>\n<li>These assumptions plus the two retinal images are enough to recover accurate depth information.</li>\n<li>The convergence of different depth cues allows depth perception to be as accurate as it is.</li>\n<li>Sources of depth information\n<ul>\n<li>The state of the eyes themselves (ocular information)</li>\n<li>The structure of the light entering the eyes (optical information)</li>\n<li>Information from both eyes (binocular information)</li>\n<li>Information from motion (dynamic information)</li>\n<li>Absolute or relative distance to objects</li>\n<li>Numerical or ordinal relations</li>\n</ul>\n</li>\n</ul>\n<p><strong>Section 5.2: Ocular Information</strong></p>\n<ul>\n<li>Important for depth perception are the focus of the lens (accommodation) and the angle between the two eyes’ line of sight (convergence).</li>\n<li>Accommodation: the process of using the ciliary muscles in the eye to control optical focus by temporarily changing the eye lens’s shape.\n<ul>\n<li>The lens of the eye can become thinner or thicker depending on the object’s distance.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 812px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/b536d53077361c6c8a834e75de145e33/63ec5/figure5-2-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 99.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAACTklEQVR42n1T2Y7kIBDL/79sT6c7nYtckHBDzv9bEzqzox3NRBZCCFO2q5JY69q2w9c0jVJq3/d1XZdlmS/4E26eI6z3xnltHZCUZUUpPY5Da51lmVQqEHDPe6zzxYm0iMgM5DzPUbBt28fz+XH/eD6zgVJIadpWSDmvq/2FXBQFNO/Hgavp4xEro2zAPPvfKzvn6Pl1fa+t3fZ9gedtW48D63+003CgKWuBBLQ0TSG+quuyqtg4UsZI0zyeWd00MaofychlHEchBGhwq42xzo3TBNvDOH6vHPifZDQGUpF2aNK2Bc3haINmpOV+8PyPHC55v540f4X0NeSvzChbWgskoJG2RWPwGWvXfffXYHyOx/chkeAbmxBCRs5Dq4S431OltfXuqmDjqo0NK6QaI7URWgNB9j1NQ+Wuy14v9DkvCryFwCjyY4xNE9aeUgCbbhj6ASvFpiIkKcoS4wG3XIg/txv6JKTiUko8rxQwCQHgRRwCI8de0PP1BI1BV2JvMF7Qj/DXY0eKX61eDZ9hOPwY3mnnkn4YXmFCCIYEwsCnbMQeld8hu/cwmmsDt8IEJEgI8wRVmKe277GBHpCRjVsWc5V6F3ROOfQJ/BB4AiYKwhuyAYELGQ2z4GqibGITB4JzKRncnoY7yijnyQtfnkM8MkdlZBjDh5AXwqzrHMt5UlRVSUhWlDd0KMtwnuAS2gCpkYAVfHDwUFHVFWnqpo38kjRV0578AhdeIONPjD0ksEyp8SEYZTAPNoyhNpgkCObomdKjVPiHAMY54/wvvz8VGsrvOrAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.2.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/b536d53077361c6c8a834e75de145e33/63ec5/figure5-2-1.png\"\n        srcset=\"/CR4-DL/static/b536d53077361c6c8a834e75de145e33/63868/figure5-2-1.png 250w,\n/CR4-DL/static/b536d53077361c6c8a834e75de145e33/0b533/figure5-2-1.png 500w,\n/CR4-DL/static/b536d53077361c6c8a834e75de145e33/63ec5/figure5-2-1.png 812w\"\n        sizes=\"(max-width: 812px) 100vw, 812px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>If the visual system has information about tension from the muscles that control the lens’s shape, then it has information about the distance to the focused object.</li>\n<li>However, accommodation is a weak source of depth information as observers are notoriously poor at using it to make direct judgments about distance.</li>\n<li>E.g. Accommodation is used to compute the perceived size of an object at close range, but beyond 6-8 feet it provides little to no depth information.</li>\n<li>This is because the muscles that control the shape of the lens are already in their most relaxed state so the lens can’t get any thicker.</li>\n<li>Accommodation provides absolute depth information.</li>\n<li>For accommodation to provide accurate depth information, the object must be in focus on the retina.</li>\n<li>This implies that the visual system must somehow “know” when an object is in focus.</li>\n<li>The best indication of proper focus is the presence of sharp edges, which rely on high spatial frequencies.</li>\n<li>It’s likely then that the visual system controls accommodation by adjusting the tension of the lens muscles so that the output of high spatial frequency channels is maximized.</li>\n<li>Unlike humans, the African chameleon relies mainly on accommodation to determine depth to catch flies at close range.</li>\n</ul>\n</li>\n<li>Convergence: the extent the two eyes are turned inward to fixate on an object.\n<ul>\n<li>With convergence, light falls on the centers of both foveae simultaneously.</li>\n<li>However, each fovea only has one center, so only one point can be precisely fixated at any moment.</li>\n<li>A crucial fact is that the angle formed by the two lines of sight varies systematically with the distance between the observer and point.</li>\n<li>E.g. A close object results in a large convergence angle, while a far object results in a small angle.</li>\n<li>Convergence is binocular and provides absolute distance information to the fixated object.</li>\n<li>Like accommodation, convergence only provides distance information up to a few meters because changes in convergence angle at far distances become increasingly small.</li>\n</ul>\n</li>\n<li>Convergence and accommodation normally covary so it’s difficult to tease apart their independent contributions to depth perception.</li>\n<li>Both are important sources of depth information for close distances and are among the few techniques used by the visual system that specify absolute distance.</li>\n</ul>\n<p><strong>Section 5.3: Stereoscopic Information</strong></p>\n<ul>\n<li>Stereopsis: perceiving the relative distance to objects based on their lateral displacement in the two retinal images.</li>\n<li>Stereopsis is possible because we have two separate eyes with overlapping visual fields.</li>\n<li>Binocular disparity: the relative lateral displacement of an object on the left and right retinae that depends on their distance from the fixation point.</li>\n<li>We normally experience binocular disparity not as lateral image displacement but as objects positioned at different distances.</li>\n<li>Information from binocular disparity\n<ul>\n<li>Direction of disparity: information about which points are closer and which are farther.</li>\n<li>Magnitude of disparity: information about how much closer or farther they are.</li>\n</ul>\n</li>\n<li>Binocular disparity, even though it only provides relative depth information, is important because it specifies ratios of distances to objects rather than just which is closer or farther.</li>\n<li>Corresponding positions: retinal positions that match for a given object.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 804px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/802e4a1fd5b655ba9a998cd513ccfcbb/27b7a/figure5-3-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 95.19999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsTAAALEwEAmpwYAAACKElEQVR42l2T23KjMAxA+fW+tA+Z6SZNw81cDRgI2Cb7hXtsE5ptqsmoko5uViJjDbJtxm4WQfE633aaxmW9o1hrnPiAp+4kCrCxv2G0QQ3TPGEM3mAPwZo/q59wyPoi22MbpxHZq724PLz+wC8+n9jaReu2k7LrVkMgFuw7GWAkstsL6TATou/rKqVsZLsavfPGHDPStoPDqkJx40hCmcdOs+t5HNV9ueMKKQ5+h49OTKip11Vreu6HoarrVrakIJd3acRjem/7IAO8wBrgtaiqumm9NNrDZPTeHzkqu5m174fiVV3xsQ/3bkII3ozncJV/wc+y+87mZRGFeHt7K8oyPOHX9fr+8S47uayLb9thfrrlv7Yff7d+6OlTFEXdNqyKI2vapiiLpqnZnrb6IHdYP4tzGGVVfv757PoOhqB5nsAwnk4njO4h9Q5zudGxulCc3V6v11Y2PreL481ykX/fbrjCRhDIAEMuXnR4bdbT9T1X7da+Ll3PmXFqkilCxkA6mDn9wvX22K+PPSdp1g0dFlILkcdxnGUpLfiBlyAOzvOMfahRMaRSioKKzcwzlkF1tIrCFpUaZNdSvx9ooyEYJTpfLl/XS12XcRIzW93Ut9s3SlkVWZ6yKuxpRisJLaRpGuY/n89JmkREUzlJY75pOBcExjh4bVxplma+tTi5FaUgEf9iJAAl4v5EkSdJTBEEBmtZVQDCf0K1uqnmOxMpJPzOx1H9A6oH62LZtyApAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.3.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/802e4a1fd5b655ba9a998cd513ccfcbb/27b7a/figure5-3-2.png\"\n        srcset=\"/CR4-DL/static/802e4a1fd5b655ba9a998cd513ccfcbb/63868/figure5-3-2.png 250w,\n/CR4-DL/static/802e4a1fd5b655ba9a998cd513ccfcbb/0b533/figure5-3-2.png 500w,\n/CR4-DL/static/802e4a1fd5b655ba9a998cd513ccfcbb/27b7a/figure5-3-2.png 804w\"\n        sizes=\"(max-width: 804px) 100vw, 804px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Stereoscopic vision is extremely sensitive to binocular disparity as most people can detect differences of just a few seconds of an angle.</li>\n<li>However, this sensitivity only works to a range of less than 30 meters as beyond this, disparities become too small to reliably detect.</li>\n<li>Stereoblindness: inability to perceive depth given both eyes.</li>\n<li>The most common cause of stereoblindness is strabismus or the misalignment of the two eyes.</li>\n<li>No notes on stereograms (pairs of images that are slightly shifted to give the perception of depth).</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 560px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/0e746b077a549297858c62ec0a3718cf/b06ae/figure5-3-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 155.99999999999997%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAfCAIAAABoLHqZAAAACXBIWXMAAAsTAAALEwEAmpwYAAADm0lEQVR42l2V23ajOBBF+fI8dz8kWVk9aTDG3K8GCSGwv292SYAzQ8qJIJy6nDpVDh6Pddswiz2em1lMWVZt27Rt3Q99mqbxNeYmy/OyKvM817M+TAUevG6rtctwv89mruoqud14b13XqqrLqh6nMYqiOL7WTa3n+TQBYzgwy8KrszGXSxyGlywv8FXVdUEmdRWFl6+vP2VV8cI0z5gyxoMl78Xa7fHgf40k3bVdx61ELitt5qKopJKm0f8Db4+N96yrW+LXbdcP7tdQVVWe5V3Xp7mgeWjsAgzTDrx5MJdd18Wu/J6XZVSKXDibxSqth/uo5pnnp3nw6/KkSwnr6rPQWjuP1lj7QhrjDwLeHGw7mubP2/PZdB0FUzbE4mhxGekj5z3yCdvkKB/eJn+Khrn3j4+iKLmHdDLfwWdkD9iOzImZZkVPo+53+vTr1+9rkpD8+/tnXTeeEeMs8MDtwNuj4Mfz2dKurr/E124YHHMLxS9H8SfbO9IT5t4g63UYR/D6oMdH89XCv5p14DAvqsB4Q7BCwfMpMR3bZ1gBO5148LqDBWZJFZ2gka7vy1K07eTRyqFufORpB8vlZ4M4yHNumpZocIOhcORdN22WyTxxmJQ+5R34mPBMPfBjZtP33aSmvChuaRpFFw5hGALu+rYfukkrqdxP1Vkt9YwT7gD3xM/y7HZLr9cEmTBnaZrfx5EJn5SAqVkJ2NGzMM6r9WmTpk++6dqUjpfVMMhsTlpTNmlLzVpjgd8EPy8XYSBb2oxxHseJ+MwjFGoX8wXejswPkT75WaXlp34oSrSNnWDlwHt7l58mw2gpj8FgGPUxjGr2HZawyrVq/akNe4CpBbCjKqN+ntACocoNxk7Y9tgV8ur2Y2Pj9D2ljm9vb0mScLbr8vn5wTKVwTjBsjedsHaT5bEqJZkSKowili5y4zn+6OV/axZKHOGn7dreaBgj6fcxCfvBmg95ClhcoBZDpHk/8KqRP+x1J8ZFuTgYLLCY7tN0nxRZBFEY/vP9XZRFlmX0klCse0jO8yJmCdyQmeiMai9xzC1qT3jEn7IK2FFFUSCAkglquFpZWwLOU/zlBTVjKWpN+RoRJC7wiooCJ8ASPB/0zNiBIR7R5HlZOhc5H/KIr1fOcZJ8//1L+ADRli4430bkVrhhwiPuXdYp+OhykXOaAYZ/wJTAbcAYOteFuJAUJHnZzPuukdGDGwQ/as03AQZhbLVuGP4FBm7bfUdpjw8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.3.6\"\n        title=\"\"\n        src=\"/CR4-DL/static/0e746b077a549297858c62ec0a3718cf/b06ae/figure5-3-6.png\"\n        srcset=\"/CR4-DL/static/0e746b077a549297858c62ec0a3718cf/63868/figure5-3-6.png 250w,\n/CR4-DL/static/0e746b077a549297858c62ec0a3718cf/0b533/figure5-3-6.png 500w,\n/CR4-DL/static/0e746b077a549297858c62ec0a3718cf/b06ae/figure5-3-6.png 560w\"\n        sizes=\"(max-width: 560px) 100vw, 560px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Correspondence problem: how does the visual system match features in one retinal image to features in the other image?</li>\n<li>Vision starts with two images and must discover which features in the left image correspond to which features in the right image.</li>\n<li>One idea to solve the correspondence problem is that shape analysis is first performed and then shapes are matched to compute disparity. But we don’t know if the visual system actually does this.</li>\n<li>An alternative idea is that stereopsis is performed before shape information is available.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 804px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/8e3501002a7637a4ee1448294768b2cf/27b7a/figure5-3-7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 82%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAACSklEQVR42iWT6ZaiMBCFeXqdduw/rbQgsiUhLEnYUeft5gvm1IFKqFu36qYI1m2Z5tGYTiqha912TdPUQkrbu2Vdpmlq21YpVQpRlmWlK0xrrSo1zVOwrgv45+v5er+fL17b+99re24gd1s/Diev94sYnPX5XDd/HizrzMtYm6ZZUZJe5HneD/2yzLM3j4SkqqooiuJ7rHXN4cc8M0FCimv4G91uUXS7Xq5t22zbBoZPkE+zDzidTofDIcuyvRxvARHWGjrpTIdjLE3X9D1O007rg9ZtG8ZRyipNc9f3e1M7uGnrOPJLSjXPs7X28XiE4S/bfhg+SACohWwUj2OdW3zZc6CU/D6fv76+qBkwSv78XI6HYxzHzjmQWF3X9PNI03tyD8PQ0+yKBKbrsiwllFK5mL53Qog0TQHQEczQ7lfYQWgs4R38NOjBaFPk+f0eU9UwDMaYsixut9+6qekTQNM2SfJADK6AdFKK8Hrl2Q8uqGsdhtfz9xm2cRzJnCT3P8djURREQ5XlGToXZel6N04jivz9e0IjtgHaMkN+rOoaZvpk2pyzlAcP0dZZ+Dtj8DkhHV+JJziA9nq5PJIEGVGrqpSUspSCAZRKcQInJ7QtlfRDKvwU+0hdBVw6SKaKnnlyiGDUTCFFkTPHtMON6Er5q9IVKXiSGlEApxi0Yl/4LBxKEDsP2uw/RgmsKJlgiqrYkj0gPZ/zvOACkB3/Y8R5nn0RChUm/M735f8srT0YY8HDc58kyW19ksKN5yXskRLhGGC04z9imf/mi3EwrOKVnAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.3.7\"\n        title=\"\"\n        src=\"/CR4-DL/static/8e3501002a7637a4ee1448294768b2cf/27b7a/figure5-3-7.png\"\n        srcset=\"/CR4-DL/static/8e3501002a7637a4ee1448294768b2cf/63868/figure5-3-7.png 250w,\n/CR4-DL/static/8e3501002a7637a4ee1448294768b2cf/0b533/figure5-3-7.png 500w,\n/CR4-DL/static/8e3501002a7637a4ee1448294768b2cf/27b7a/figure5-3-7.png 804w\"\n        sizes=\"(max-width: 804px) 100vw, 804px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Using random dot stereograms, evidence supports the alternative idea that we don’t need shape information to perceive depth because random dots don’t have any shape information.</li>\n<li>But this only supports the fact that stereoscopic depth can be perceived without monocular shape information; there might be some primitive shape or contour analysis before stereopsis that helps solve the correspondence problem.</li>\n<li>For random dot stereograms, the greater the disparity the greater the perceived depth.</li>\n<li>So far we’ve assumed that there are corresponding points because that’s normal, but what happens if different images are presented to both eyes?</li>\n<li>E.g. Horizontal stripes to the left eye and vertical stripes to the right eye.</li>\n<li>Grossly mismatched images to the two eyes result in the observer only seeing one image at a time.</li>\n<li>Binocular rivalry: how both eyes compete for visual perception.</li>\n<li>Rivalry tends to alternate between the two images as we believe the neurons responsible for one perception become fatigued after prolonged firing, leading to the perception of the inactive alternative.</li>\n<li>How does the visual system solve the correspondence problem in random dot stereograms when there’s no obvious features to match between the two images?</li>\n<li>Most dots in the left image have some corresponding dot in the right image and the visual system must somehow figure out which pairs go together.</li>\n<li>Different computational approaches solve the correspondence problem in different ways.</li>\n<li>E.g. Match individual pixels, match lines and edges, or match local regions.</li>\n<li>The inverse problem can be solved by introducing heuristic assumptions that provide correct solutions when the assumptions are true.</li>\n<li>The problem in this case is finding the correct assumptions that explain random dot stereograms.</li>\n<li>Binocularly sensitive cells were discovered by Hubel and Wiesel in area V1 of the cat cortex.</li>\n<li>The cells they found in V1 seemed to be tuned to features that appeared in corresponding retinal locations of the two eyes rather than to binocularly different locations.</li>\n<li>Evidence finds that two separate physiological systems are involved in stereoscopic depth perception, one in V1 is highly sensitive to small or zero disparities, while the other is in V2 and is sensitive to large disparities in both crossed and uncrossed directions.</li>\n<li>We have been focusing on horizontal binocular disparity but vertical disparity also exists.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 799px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/7bba57e9e79f0d1aa763e88970e021a2/76cea/figure5-3-22.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 85.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB6UlEQVR42mVTWZajMAz0/b+me8gCJBgbMEtsvEA435StDg+m9RQibJWWkmDre4W+k0R7jTaey7qQvrdNNk1ZlrWoFxwkf3Jja8LS+0fgEk/xDOvivH88nrUQO2YXRuF/UiUhgJkhdhgGISVet21bzsgIDksIABzUh9Cp7nK5qF4JKfI8X6ieQ17qlMAnPJxQZJ4X1jkUXFXcWgvfY2YKx3zwScOuuHhpPYxj16m26/p+MMbs2c5g7z74HzWzma0FyT5y+0Yp3vu95CO1zAF8wI/jaIye7TwOI/ymVxKtp+m1/hLm3Ow+YJ1gTdtUVRVCQCBrXVEWWZY550EBrcAZ7CyyExg1g2pec5yM02SdBdt/swxG7OU/sLUEdpQfPMEAH0DCVWsdoiyoP83hXPZsDaJS2bhG8rwobrcbOgV4mqbr9YrkGPhOFdYppECJ7ZQzrBjSQtNCe7imslNRsWHaWSjtQlyS8lFKKeCHVpVSaAyVa2Miw4lnLCmohoEBjtMLa4voaXln9v31536/dV1XxO9G9EOPrwdhpGx4XWOxoSC/4rwfBnxecFOqxyGEcV7JRmI8UHgDiYVElGQL/CpyaOL82rbFyeP5FEKqvmdw4Dx68vjHAUASdJH6xFKj6dgzGP+sQyAKfPD/AMJCsi9toFUXAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.3.22\"\n        title=\"\"\n        src=\"/CR4-DL/static/7bba57e9e79f0d1aa763e88970e021a2/76cea/figure5-3-22.png\"\n        srcset=\"/CR4-DL/static/7bba57e9e79f0d1aa763e88970e021a2/63868/figure5-3-22.png 250w,\n/CR4-DL/static/7bba57e9e79f0d1aa763e88970e021a2/0b533/figure5-3-22.png 500w,\n/CR4-DL/static/7bba57e9e79f0d1aa763e88970e021a2/76cea/figure5-3-22.png 799w\"\n        sizes=\"(max-width: 799px) 100vw, 799px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Vertical disparity exists for different sized objects, but our eyes aren’t vertically displaced unlike the horizontal axis.</li>\n<li>Da Vinci stereopsis: in binocular viewing of surfaces at different depths, there’s usually a part of the farther surface that’s seen by only one eye. For these regions, there’s no proper solution to the correspondence problem because there’s no corresponding point in the other eye’s image.</li>\n<li>The regions seen only by one eyes are important for determining depth because these regions are always part of the farther surface.</li>\n</ul>\n<p><strong>Section 5.4: Dynamic Information</strong></p>\n<ul>\n<li>Depth information can be obtained from motion because of motion parallax.</li>\n<li>Motion parallax: points at different distances from the observer move at different retinal velocities.</li>\n<li>E.g. When driving, the close road moves faster than the distant mountains.</li>\n<li>In the case of binocular disparity, your head is still and you simultaneously compare the left retinal image with the right retinal image.</li>\n<li>But in the case of motion parallax, you move your head over time and compare the current image with the previous image.</li>\n<li>Thus, binocular disparity involves the difference between a pair of simultaneously displaced images, while motion parallax involves the difference between a pair of sequentially displaced images.</li>\n<li>Experiments show that motion parallax is sufficient for depth perception but only when no other sources of depth information are available.</li>\n<li>Analogous to binocular disparity, what you normally experience from motion parallax is that objects are different distances from you rather than objects moving at different speeds over the retina.</li>\n<li>E.g. Stationary objects are perceived as stationary even though their retinal images move whenever you or your eyes move.</li>\n<li>Position constancy: perceiving the actual position of objects in spite of changes to their retinal position.</li>\n<li>Binocular disparity only provides relative depth information from the fixated object but can do so at great distances (unlike binocular disparity).</li>\n<li>Optic flow has information about both the distances to surfaces and the observer’s motion.</li>\n<li>The visual system appears to use a rigidity heuristic, which is a bias towards perceiving rigid motions rather than plastic deformations.</li>\n</ul>\n<p><strong>Section 5.5: Pictorial Information</strong></p>\n<ul>\n<li>The remaining sources of depth information, aside from stereopsis and motion, are known as pictorial information.</li>\n<li>Pictorial information: static and monocular information from a scene.</li>\n<li>Pictorial information allows us to perceive depth in 2D pictures.</li>\n<li>No notes on perspective projection and Alberti’s window.</li>\n<li>The sense of depth provided by pictorial information is never as compelling as that provided by stereoscopic vision or motion, but it can be just as convincing as viewing the real scene through a window with one eye fixed in position.</li>\n<li>Pictorial sources of depth information\n<ul>\n<li>Convergence of parallel lines\n<ul>\n<li>Parallel lines in a 3D environment don’t project as parallel lines in a 2D image.</li>\n<li>Instead, parallel 3D lines converge toward a vanishing point on the horizon for 2D images.</li>\n<li>E.g. The parallel lines of train tracks.</li>\n</ul>\n</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 806px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/fd9176686a9773b0098418ff798c8922/764be/figure5-5-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB4UlEQVR42kVSi5arIAz009vtY1ttq61dH4CgVhERsXZ/8A727i5n9IQkM4QET6nuK/m6XaOqKuu6bpqm6zo9aGAw2tpxmuw4mgHbQSMZi1JCKcXWQ0xrfT6fPw+HNMs454/mgXW/38MwRFLbttDtOgkDoSiK4ji2kwVAnqbnNL/mqq6SNC14QRnd7/c5IQXnSZJA5WO98Y/HnOS3OM7y/Dk/URHgyNaZI1wQBjM4nYPghCQUUlVQTIDADy6XixBiMAOSFxgPx9rpR2FyKpcw3Gx3wemUZRns1XqVZulqvY7v9/n7G8l/5OdCfgMuM45tJ8Mw2u72QRCkSer7fny7Bb7fSommLZn2j/zLf5NhDMZcwN/urrfrZrMJo1D1CkwzOpr9K3vp2y954RtIAITQz8MR5KZtlO7NuLh/cmB7r9c8v144HQ3D3/7XsgiiN4xzUZawcdzSHbvUbM0i4aGBeBtSykfTqL5HwTgHI8UnVYd7SowYL0M5yM5t4Vz80kvTFNMrikKUAoOBFisYYZTBxTmeUkZyhLgQlMHr/LR4G9QrqxJpIJRC5HmGJFSKWF1XjLFCcAyMUJJTAhXIwXAqNGeMeKjZ2ZRo3eO54nbDexmHXuuuV51yxaJsNfRou1LyjX/eru7z18HS7gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.5.5\"\n        title=\"\"\n        src=\"/CR4-DL/static/fd9176686a9773b0098418ff798c8922/764be/figure5-5-5.png\"\n        srcset=\"/CR4-DL/static/fd9176686a9773b0098418ff798c8922/63868/figure5-5-5.png 250w,\n/CR4-DL/static/fd9176686a9773b0098418ff798c8922/0b533/figure5-5-5.png 500w,\n/CR4-DL/static/fd9176686a9773b0098418ff798c8922/764be/figure5-5-5.png 806w\"\n        sizes=\"(max-width: 806px) 100vw, 806px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>Position relative to the horizon\n<ul>\n<li>Objects located higher in the picture plane (closer to the horizon) are farther away from the observation point.</li>\n<li>In general, for all objects on a level plane, the ones closer to the horizon in the picture plane are perceived as farther away.</li>\n</ul>\n</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 804px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/b434224c722376a9713d203da7aa8f9e/27b7a/figure5-5-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 80.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAACUUlEQVR42jVTi5KjIBD0/+uyye7exWh8C8hDFAQkW7cfd43eIkXxmJnunhmzEJz3m/dujyG+9hhD2L3HZXDY7Lu3diV0fBR527fWmrjvIXjn8RoyOMMHzlLJru8IGY01eEvOwePyURS3j48/9/z2fsvzO2N022xAiOS8ezXL8llerte32+3X5QLTYSR2c6Ck5nk1dvMBp2NudrM+hB/n4Nd1EVKdkws5CUEo64fxnufX9+vn709CyRHCQ0YSljwPZwh2LoGcV+C6OU8Yy6Hy8aibuijLqq4pY7CD549zSM4HnTS2czi3WquXFVQPetHvUc6zVApP4T9yesLMoP6YKeFYORcAIZQKpcy2IcrEBdbFGMYFdJ2CwTQ5n4mAbCkEn7helkRv32etu75HCkAmvoCfRAEEFIxN2U7OnHNK6QC7ic+zXo1Z1lXrFStSPRKCKJxLNglMQLOJV1U9DINe5mxJZovWMyEE5Y0xFkUppTpjQyGyBQvwBKzWGl0AdcZgtZkxSJAB0rOqwAe6lgXGK2Osbdu/399Syr7vgf/6+kIgHH0KlHKcHbC6LJ+IDYvns0Ieq6rCJQxAEqaEUKVmfHXTgA4wbCqLS9kGctt1xtjUBTGiRsdEq78ADl3YoKWxQS1QJ8AcneGysizbpkEqoHmaGFJOGQUFVAifdQY8hRScT8gzaoZuP81mrbKieOT3e12DUQOdHUbfU5oENw0aiyKxuErHtu0GlAWja7t2HMdsGHHG75TucQY+xR84jqgfrIbDuoMpGbHH4zCkDV7Nav4BfDWCeW0myVsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.5.6\"\n        title=\"\"\n        src=\"/CR4-DL/static/b434224c722376a9713d203da7aa8f9e/27b7a/figure5-5-6.png\"\n        srcset=\"/CR4-DL/static/b434224c722376a9713d203da7aa8f9e/63868/figure5-5-6.png 250w,\n/CR4-DL/static/b434224c722376a9713d203da7aa8f9e/0b533/figure5-5-6.png 500w,\n/CR4-DL/static/b434224c722376a9713d203da7aa8f9e/27b7a/figure5-5-6.png 804w\"\n        sizes=\"(max-width: 804px) 100vw, 804px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>Relative size\n<ul>\n<li>All else being equal, more distant objects project smaller images onto the retina.</li>\n<li>This requires knowing the size of the object because you can’t tell if you’re looking at a smaller nearby object or a larger far object.</li>\n</ul>\n</li>\n<li>Familiar size\n<ul>\n<li>Many objects tend to have a characteristic size or range of sizes which we can use to solve the size-distance equation.</li>\n<li>The knowledge of object size isn’t conscious but occurs unconsciously and rapidly.</li>\n</ul>\n</li>\n<li>Texture gradients\n<ul>\n<li>These are systematic changes in the size and shape of small texture elements.</li>\n<li>E.g. The fine details of a texture can’t be seen from far distances.</li>\n<li>Thus, we can use resolution as a proxy for depth.</li>\n<li>The overall size of texture elements diminishes with distance because all dimensions decrease with distance.</li>\n<li>This only applies to texture elements that are similar in size. If they aren’t, then illusions of depth and surface occur.</li>\n</ul>\n</li>\n<li>Edge interpretation\n<ul>\n<li>Occlusion: the blocking of light from an object by an opaque object closer to the viewer.</li>\n<li>Provides relative depth information at any distance.</li>\n<li>The intersection of edges is crucial to determine which edges are occluded by others.</li>\n<li>Edge: a discontinuity in image luminance.</li>\n<li>Four types of edges\n<ul>\n<li>Orientation edge: discontinuities in surface orientation.\n<ul>\n<li>E.g. A bend in the object.</li>\n</ul>\n</li>\n<li>Depth edge: discontinuities in depth.</li>\n<li>Illumination edge: discontinuities in the amount of light falling on a homogeneous surface.</li>\n<li>Reflectance edge: a change in the light-reflecting properties of the surface material.</li>\n</ul>\n</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 803px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/3bc848025852c17dec6668a7e0d4fe92/e1031/figure5-5-16.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 85.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAACm0lEQVR42k2TyVLbQBCG9XDhxJ0sd86BxCYpnIQtYAgp2/ECxtp3WZa1jqQ8Xr6RoYI11Wr19N/b31aaRlRV5fl+tI7DMAqCMAh5R2t+8Trs9DAKXxR54bhuVZVtUyt1I8qy3CRJ3TYltu5UQuRFUZQlSlXXWOqmqUS1U9ZxLMF/GwkuyoJ4XADY5feDwLIdLPFms91um7YNwshx3KILLcFCNG2jiLoiB9+irgHrurFYPNq2uwMD+HX/+/27D1dX11S7TVNgxPsPzvLcMEy6jaL44WF5eHg4OOUZLJerp6fVcDgcj/+MRuPxePK4fPI8j4h01kqwKLM8Y0gprzxfx5uDg7d7b/ZuboalENPZbH9/H+Tl5c/b2zv6ZxC+73c9t0olyjTL6JOaGdtsvsBpMpnYjsM5PR2cn19MZ/O7u/vZfF5RZ10zfAkmc1UVWZaSmcnScK/X/3h0dHz8WTdMvI+OP/X6JydfvmKbLxZ124qmAVy+Am9pl3psx/1xdtbr9wffvquqTiGGaV5f35ydX4xGEyggAaPqqKokzyUt5ymUMFvwKITAL5ck15BEO0nHVt3UYJHx5gUsoLnMGSAMc+JNsk0zZBjFIexLQ8KJ47jbuTjZchWKWsiyw9Bbr0PseFI8TD57sx5phpK8slBCJ5O8yGlbmc6m/CzLWq00VdNd1zP5sByWxPMD07TYDRrhsVgdz4MC3TAs2/IDXzFNc6VqmFRNw4+xm5ZpGAZVMC0CYQQGEWwRsC4Q4Rw6VzRN0w2dSNLVJB/ty2s+d1LXKUdaUAjKhuBDUNdzAau+Ly9kZkd62xLurlSVEp7TGvquBCQBuEKSRbEsw7a7xmwHhmzHxpvhsaxFmXd/zILxyN0tUArWMc1SZF7k/wD4Y42X7N2PnwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.5.16\"\n        title=\"\"\n        src=\"/CR4-DL/static/3bc848025852c17dec6668a7e0d4fe92/e1031/figure5-5-16.png\"\n        srcset=\"/CR4-DL/static/3bc848025852c17dec6668a7e0d4fe92/63868/figure5-5-16.png 250w,\n/CR4-DL/static/3bc848025852c17dec6668a7e0d4fe92/0b533/figure5-5-16.png 500w,\n/CR4-DL/static/3bc848025852c17dec6668a7e0d4fe92/e1031/figure5-5-16.png 803w\"\n        sizes=\"(max-width: 803px) 100vw, 803px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>How does the visual system know which type an edge is?</li>\n<li>Orientation and depth edges are mutually exclusive and a theory of edge interpretation explains a process of how to label every edge.</li>\n<li>Surprisingly little effort has been directed at testing the predictions of computational theories of edge interpretation.</li>\n</ul>\n</li>\n<li>Shading information\n<ul>\n<li>Shading: variations in the amount of light reflected from the surface due to variations in the orientation of the surface relative to a light source.</li>\n<li>Shading provides information about surface orientation.</li>\n<li>One heuristic assumption the brain makes for shading is that illumination comes from above.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 792px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/734d2fa5e3f6c0caafd568ad3fcb512c/9a86a/figure5-5-28.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 91.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsTAAALEwEAmpwYAAADPklEQVR42kVTW3PaVhDmRzRTG7CRhIwkdENIXO1cZpI+xRcMbuK6L+mbXzwDGEwneUlmmmneEjsII4HkaewCupmL+wP7CTrN4Whn92i/b7/dIyLj8TgIgqury3wuV8hr+ZxWKha3y6XtckFTNU1VCzm1XCyUSyUk7JRLxbz24cP72XzuB15kNJkE9/eXV5dJipYl6acXz9VsjmXSLMOWiuXt8g6dpFN0ShKExztPREHa3EhcdDqLhwfP9yPjycQPwVepLbpWrV5cdN68+S0jy4V8vtk8b7Xaj3eesinu5JeTt2/fvX51HI/GzlvtxcM/S7DjANzVewIvSmAWhYyckaUsHhkBL4iixHOcglgUlYyS2EyAdI7KQbACT/UewDwUxmNRIpEAkE9z8Vg8Go3SFJ3meJIgo+trJEFQJNVqt+eLBx/gieveT6e6rvNpXhTCJYmSJMqiIKY5jmVSaJ5j2aUPm05SVKvV+g88dtxgOuv1esiGclmCPjwZsKzwAKXBy4ehyIsUSTaa54v/ZXsBZF+jIJ/GYtGbFLYvQC3LMCzDcQyHVyELx5EE1Wg054uFi4GNJo7rB9f9PgoSCTS2TpHEiigWi62v/Ugnk1CLntfX1ogEAafeaMwA9vxV5aDX7wOQVVRNzWr4KZqiKDL4RMxchmZZUjAIHIKrXm/M5ovvV4Vpb23Re3v77db5rycnSiaLT+309LRerz979pRJMUe1V53O78evj5Mk2Ww2ZyvZ4bRnM/S8Ed+AbDWbxYhwZxRBoiyuN7GxibvFiabmtmj60Q+Pzs7OlrK9CKpDw9Cya7Wfq9WjysFhrXqEVa1W9/cPdnf3Dg4qh4c1+C9f7lYqFTh/fPw4nc3DnoeW9e329m40+uvbLfZ44vw9gnEdz4OFN3E9bMzF931UQo+Ouzx33MjnL1/+/PTJMIyvXf1rt2vZN7re6+p63zDA27u+xkVgIqZp9g0Tgblctn1j23YESQNrCAsY8szBYDAcwjGWGbAI+0trGGZ4eGMPBqYR5lshGKnfbeggbWCirmWvMCGfZcEiB+4wjKzbu7sIXvXDHaqCD24rLIhMCz5IIC9EDof48zpo1/NgMBE/8P8FSOG23IOHgysAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.5.28\"\n        title=\"\"\n        src=\"/CR4-DL/static/734d2fa5e3f6c0caafd568ad3fcb512c/9a86a/figure5-5-28.png\"\n        srcset=\"/CR4-DL/static/734d2fa5e3f6c0caafd568ad3fcb512c/63868/figure5-5-28.png 250w,\n/CR4-DL/static/734d2fa5e3f6c0caafd568ad3fcb512c/0b533/figure5-5-28.png 500w,\n/CR4-DL/static/734d2fa5e3f6c0caafd568ad3fcb512c/9a86a/figure5-5-28.png 792w\"\n        sizes=\"(max-width: 792px) 100vw, 792px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>This assumption makes sense because our visual environment almost always is illuminated from above.</li>\n<li>E.g. The Sun.</li>\n<li>Current computational approaches can solve simplified cases of shading but not for naturalistic images.</li>\n<li>E.g. Images with multiple sources of illumination due to reflection and surfaces with different specular properties such as glossy or matte.</li>\n<li>Depth information from shading is further complicated by cast shadows.</li>\n<li>Cast shadows: shadows of one object that fall on the surface of another.</li>\n</ul>\n</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 782px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/d4aff0de7ad323ac5610ccb1700f7b4f/2e195/figure5-5-30.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 107.60000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAIAAABPIytRAAAACXBIWXMAAAsTAAALEwEAmpwYAAADlklEQVR42kWUWVMbVxCF55fFhhCnbCep4sUYU4UhLuwX5OIhRuWwhACmQAgMQttoFo00+75pJH5evju4KqM7V31n+nTf7nPmSrNZMZ/PGItFlaXJSBkN5SGXqqkKP1XRdI1xe3vb7T6UZVGKq6iqEog0m5VVBXJGFE1TO51ufzCUR6ORonZ7vW/fzts3N9fX13t7e5ubm7Isz6oqL+p8iwpwAX4+r6IonEyMyXQK+Pj4+KHbvWq1Go3GxsaGqo1/WVn59cULwzCiKC5LgZwv5oBzMmdZdnf3/ejo8L7TOTw6WnvzZmdnp9vrLz1fWlt7OzXNy8urdvtG03XC50X+P5jknu81m/uvXr7c32+CefbTs2bzq+N6iqpNTQuw7wcM9mWatuu6i8e5qJnSPc8NwuD8/Gx56efLq5bjuOMxCUzTcmyHlYft+b5pmq7rWZZlO3aeZwsyp2kCuI7rR3GMLy8BOI7nuL7nB0EYgmF2XCeKI2KGUeR5Hh2W8jylVUHoh8xB6Hp+DfNIRYW4AvNEaP5xC7w6XhwnMCYVRVbOCjHKghsayplYQ0lRP8HErubzx8dH/lky6Ba2yJwXGSPLmQs/CFWVlGNNF4VTOlWPDYMxGAzJjg9XHaGUMsACljPSLAM4RCOKyt3r4T+EPBRzenq6u7v75ctf+BRiQ2JPUpYlWcaTtBSE+aqmgby/79B27IO/D7a3t/abzYuLi9evXjd2G7btZHn+hP8BzssiisNW6woxsGeSvFt/h8L+OTn5/bc/zs7O4a113SYcQocM0Zcir8F5Sm8Gg/7nz431t+vf7+4ODg8+/Pmh3x+alq2PDXQymU5My4Jq0IYxTdO0rjlL2LCDHFxndXV1eXnFmExOTv79+PETTbput7e3ttE5H8nW+/codFpftmUjakFVGAVI0LItvPkILdtmexgIC75leQT59J7u4+C4qMgKUFQUCapQiB8GrtBZYAl1+YiMGZ5ssSWHRtIneMLGh1p830dkUllm7J6aSzGXdDLNfoxapy7MJJT4RAlFpmnda0G1pOkqe3Zdh5awqzCKCSwa4Hp1+uBJqqQVsowiZtZxHCVJLHU69w8PHXrI6cGgsRwkSAWdQTgqwUZtnEqExk20Q1HH8DnWJWxdvB3LCseVzosapqBNVdCqD2XZgIDpFB9xmmkanNFwAUa2mgim4BAnCV9lPUfUhsGMzcOUwzGJxR1HfMWUn6bJf3HaOJVm07l2AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 5.5.30\"\n        title=\"\"\n        src=\"/CR4-DL/static/d4aff0de7ad323ac5610ccb1700f7b4f/2e195/figure5-5-30.png\"\n        srcset=\"/CR4-DL/static/d4aff0de7ad323ac5610ccb1700f7b4f/63868/figure5-5-30.png 250w,\n/CR4-DL/static/d4aff0de7ad323ac5610ccb1700f7b4f/0b533/figure5-5-30.png 500w,\n/CR4-DL/static/d4aff0de7ad323ac5610ccb1700f7b4f/2e195/figure5-5-30.png 782w\"\n        sizes=\"(max-width: 782px) 100vw, 782px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>Aerial perspective\n<ul>\n<li>Farther objects appears fuzzier and bluish because the atmosphere distorts light coming from them.</li>\n<li>Again, this provides some depth information given the assumption that the atmosphere affects light at a distance.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Given this large number and variety of sources for depth information, how does the visual system integrate all of them?</li>\n<li>In normal viewing conditions, integrating them isn’t a problem because they’re all highly correlated and therefore converge on a single, coherent, and accurate representation of the distance and orientation of surfaces relative to the viewer.</li>\n<li>In abnormal conditions, the depth cues can come into conflict resulting in one of three outcomes: one source dominates a conflicting source, a compromise is reached, or the two sources interact to arrive at an optimal solution.</li>\n<li>Dominance to solve conflict implies a hierarchy of depth sources such that those higher in the order dominate those lower down.</li>\n<li>E.g. The Ames room visual illusion pits perspective information against familiar size.</li>\n<li>A surprising example of dominance is that binocular disparity can be overridden by pictorial cues using a special device (pseudoscope) that reverses binocular disparity.</li>\n<li>Using the device, we would expect that close objects appear far and vice versa but this doesn’t happen. Instead we view the scene as normal.</li>\n<li>Compromise solutions are consistent with neither source but fall somewhere in between.</li>\n<li>E.g. If four depth sources are varied independently such as relative size, position relative to the horizon, occlusion, and motion parallax, then subjects integrated these sources of information instead of relying on only one.</li>\n<li>The exact mathematical integration function used by the visual system remains unknown, but additive and multiplicative models both fit the data.</li>\n<li>It seems likely that different kinds of depth information interact to arrive at a coherent representation of the distances to visible surfaces.</li>\n<li>E.g. Binocular disparity provides ratios of distances while convergence provides absolute distance but only for the fixated object. Combined, they can determine the absolute distance to every object in the visual field.</li>\n<li>The same applies for interactions between binocular disparity and any other sources of absolute distance.</li>\n<li>E.g. Familiar size and accommodation.</li>\n<li>How difference sources of depth information are integrated into a coherent representation of the 3D layout of visible surface is a complex and difficult problem.</li>\n<li>The evidence is lacking and unclear and there’s no systematic physiological evidence at all.</li>\n<li>Everyone acknowledges that some form of integration must occur, but no one knows precisely how.</li>\n</ul>\n<p><strong>Section 5.6: Development of Depth Perception</strong></p>\n<ul>\n<li>As adults, we perceive the distance to surfaces and their orientation with no effort.</li>\n<li>Are we born with the ability to perceive depth? Or is it learned and so well practiced that it has become automatic?</li>\n<li>Early studies demonstrate that infants have functional depth perception but the studies couldn’t tease out which sources of depth information they used.</li>\n<li>One study that manipulated binocular convergence found that altering it changed the distance babies reached for an object, suggesting that five-month-olds perceive depth using convergence.</li>\n<li>By 3.5 months of age, babies have reasonably good stereo vision.</li>\n</ul>\n<h2 id=\"chapter-6-organizing-objects-and-scenes\" style=\"position:relative;\"><a href=\"#chapter-6-organizing-objects-and-scenes\" aria-label=\"chapter 6 organizing objects and scenes permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 6: Organizing Objects and Scenes</h2>\n<ul>\n<li>What we’ve been missing so far in our study of spatial vision is large-scale perceptual organization.</li>\n<li>Perceptual organization: how all the pieces of visual information are structured into larger units of perceived objects and their interactions.</li>\n<li>We don’t perceive edges, bars, blobs, or local pieces of surfaces but objects, people, and landscapes.</li>\n<li>This perceptual organization is achieved automatically and effortlessly.</li>\n<li>Without this organization, visual experience would be messy like a snowstorm of multicolored, swirling confetti due to the output of millions of unrelated retinal receptors.</li>\n<li>The more we learn, the more it appears that even newborns have certain kinds of innate perceptual organization with the rest developing during the first six to eight months of life.</li>\n<li>Problem of perceptual organization: how people perceive a coherent visual world that’s organized into meaningful objects rather than a chaotic combination of different colors that stimulate individual retinal receptors.</li>\n<li>There are an unlimited number of possible organizations in an image but we only perceive one.</li>\n<li>Which organization we perceive and why that one rather than others requires an explanation.</li>\n<li>Converting an image into an array of intensity values and interpreting that array is similar to the problem the visual system faces in trying to organize visual input.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 491px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/ca48677bd2111ab1b341165979d7f409/13566/figure6-1-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 178.40000000000003%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAkCAIAAAAGkY33AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEoUlEQVR42m2VS1MbRxSFtXIWIRs2cTkLnGVchcN/oAqKNQYHQ+GCAgohIl7KBkMRE0Cg9+iBQOgxM5JAEtI8JAH5efm6Wwglzqmm6Znp0+fe07dbnu5DV7VOt3OTz9/c3GSz2VQqlclk0hIMLgfgtt2HxwdF8fTJ7U77Mis+ZyRYAqamadkBsJLjOopM7+k8dAT5seu0nUQi0ZuUTrMKg34v3mQvM5cZ27EHyN1OW8JxnFQqLdWSAALBJzWNKLSkltASSR5TSeeZLMJuP8N13Uw6I3PMaJJDsvCvc9epdEop/zfnQTJT4/F4LBajV7OhESqyDK6urliPaZCJV5FdBcImWpj0Ij56KcgfTzieu8lBtgfDdgRLwLIsvglOKkXYiItBUlNvriT6bveUMUA2AYzFcBl5nAGyYom0jEO2WDxm2ZYqChpkSzbBxw84uI0y3iEYjUaVfwnxIQEZ5f8nIxWN4paYCZP8w+EwZJaIRAVYlpy/JTu2bRPvxcVFOBQJhcIROZ1HZFmFsUgnHrdsu9uF+pKzAIbXG/W7u7tavUZjbJhG9bZ6D5ovjSpWyoLsuhbbxIbx/Pj0SHv6+0n1eIOCeOT105P62t8npSwSVruNhm1bsrebzSY9/FarJR5bTdm3lKaAIqMMk30+Ojqa+/RpLxBYXFzc3NxcWVlZW1v73e9fWl7a29ubm5s7ODj41jChTIX6NjeHh4c//vZxZGRkamrq1/fvx8bGJicnfn77dnZm9ofvh3w+HzmTCz2tT26jfHB4uLy8vLu7u7q2tr297d3Y8Hq9fr+fEHZ2dj4vft7f38daQW7/i+ySZPD8nDL66+SEvTk5OQmFQheh0OnpaSQS+fPrV7bq+PhYpS24nZew241GAw76TGrc31Mbpmnquk7N1mo19hkBagYNMhbkXs62Laxu3iPFJKSU4LnE2dkZpYJmJBoJngcFuSvurB6ZHVGn6sv+l5mZma2trfn5ebxZWsKBZdJeWFjg5ezsLDnbssKUbT0yyrxdX18fGhqanv7w+sfXExOT7355Nzo6Oj4+/tObNx+mp7979Wpjw6eyVfCIQkDZdVqtJvu87vUGAn+wZ3jul9jZ3d3w+QKBwOrq6uHhoTiS3ed9VmSEMYxUOfpky/EPBoMRCdLGQsxPyZcU3EuRwAQYDpmjLM5WLM6Ye6cocX19Xa/X44k4AQu3WwOG9cmUNAocRDxHkD4swTgai/HAV/aCyeoXom+YuonslkTzGaVSqVqtisMBbNEobEmTnqFcr9eazYZltQhVea4uBrF5kqVe9m5Yt3fyWYUL3ENKyaRWKhX4iWMhpMrlMoWFLBcDRVapVHRDr1QrZQnDMCg4s2wy02MYej6fz+Vyhq6zEFNLxSKPWMW8ioSoU0NX63K9qJ5LxlMqCUuRFf8KRb2kK1kI9ErKNF7GgMmMzXLZA4N5hUIBfWjFYolvRMUYji7BBFY2JfLPYIyyAFEV8gX+qyBNU6RnmqRC01FjjggBC6oVNQYeYuynDdTyMgQBlqBXOgx4L1atlCuV6u3tnYd4IZdU6vKzWlX5zIDklHl8opDYKrZa3nruPxDWIryWsRAzAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.1.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/ca48677bd2111ab1b341165979d7f409/13566/figure6-1-1.png\"\n        srcset=\"/CR4-DL/static/ca48677bd2111ab1b341165979d7f409/63868/figure6-1-1.png 250w,\n/CR4-DL/static/ca48677bd2111ab1b341165979d7f409/13566/figure6-1-1.png 491w\"\n        sizes=\"(max-width: 491px) 100vw, 491px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>It’s similar because the structure we perceive so effortlessly isn’t explicitly given in the stimulus image but must be discovered by the visual nervous system.</li>\n<li>Why is it easy to perceive the image when the numerical image is converted into luminance levels?</li>\n<li>The reason is that the human visual system evolved to detect edges, regions, objects, groups, and patterns from the structure of luminance and color in optical images.</li>\n<li>The numerical image doesn’t engage the same mechanisms even though the same information is present in both images.</li>\n<li>Thus, the visual system evolved to process a specific representation of light.</li>\n<li>A theorist trying to explain visual perception is in the same position as you are in trying to find structure in the numerical image.</li>\n<li>E.g. None of the organization that the visual system picks up automatically and effortlessly can be presupposed since it’s that very structure that must be explained.</li>\n<li>Why does visual experience have the organization it does? The naïve realism answer is that it simply reflects the structure of the external world.</li>\n<li>But how this organization is extracted from light stimulus, the mechanisms of perceptual organization, is a mystery.</li>\n<li>Also, the visual system doesn’t have direct access to facts about the environment; it only has access to facts about the image projected onto the retina.</li>\n<li>In general, an organism can’t be presumed to know how the environment is structured except through sensory information and through evolution (environmental structure passed down through DNA).</li>\n<li>Experience error: the false and implicit assumption that the structure of perceptual experience is somehow directly given to the array of light that falls on the retina.</li>\n<li>The cause of the experience error comes from starting our approach to vision using the distal stimulus rather than the proximal stimulus. The distal stimulus provides more information about the environmental structure than encoded by the proximal stimulus.</li>\n<li>The structure of the environment is more accurately regarded as the result of visual perception rather than its starting point.</li>\n</ul>\n<p><strong>Section 6.1: Perceptual Grouping</strong></p>\n<ul>\n<li>Perceptual grouping: how various elements in a complex display are perceived as “going together”.</li>\n<li>Principles of grouping\n<ul>\n<li>Proximity: elements close together.</li>\n<li>Similarity: elements similar in color, size, and orientation.</li>\n<li>Common fate: elements that move in the same way.</li>\n<li>Continuity: elements that are smooth continuations of each other.</li>\n<li>Closure: elements that form a closed figure.</li>\n</ul>\n</li>\n<li>The visual system seems to be more sensitive to certain kinds of differences than to others.</li>\n<li>The principles of grouping have no general purpose scheme for integrating multiple conflicting principles into an overall outcome.</li>\n<li>E.g. If proximity influences grouping to one outcome and similarity in color to another, the grouping that’s perceived depends heavily on the example.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 809px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/e59951817f007f98af5de43d839e82d9/e80ac/figure6-1-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAACKElEQVR42j1QaZOaUBDkJ8atWmPcUhEFEcVrq9YD8zVGMHHVXZXjcQmI2cXb/5dGKqGaVz1vpmd6HnU4RNfrefE2b7Xb3V73pfvS6/dABtIAAJeG0vA7fuk/R83r6/R43FOHwx56WZbzhfhjmFJdFEvlcoGmuUoFvMLzRYZhWbbRbHIVji4WiyVm9HMEFYUG+32kTCb5fCGd/prL5dqdDl1kMpkMNLDDclz2KUvTNO5xk8l8g16ZKIdjFE++3a6yoiCBIYBQq1UFoVavgySn2BBhIQlrosjxlfF4dDofII4ulwvE6FdmWRiGW5bleL7KV6sQIBRqApomN+gLHtu+T47CMPwxGsGV2Gh8SaXQ4vExjV7ScIi6Xr/f6TynHh6gzGafYHAgSbIiR/tP7Hy83W6YDA3WY0qlO8osx94HVmr1eAuE4HABVIWqrIyPpz11Pp9gezqb4THjle4AB2Ck2WrhzZrtVhNnqwXcU43f019H2P742OG1/WBrO471D4ZpmpZtO66z2SSIuevaMTaW4/rb7Wf0h5rPZ4u3habrOiHQENNca5qKmJiaQUCScKWqyGq6sdZ0VTd0w3Bci0K9ZVn4VV1Heq2qmmGYto1e78vVcrUGgV7TyXK1Qgqj0MK0iOOYsdi2bYhRBD3wvlzqBjEISW6gjMUGWd+HwwJsYVw8mRD0cIMwxBpeEIB4Ach2u9uBu56HhV3Pj+H7nh84G8/1Nn6wCUP/L5bxmd9zq9e7AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.1.4\"\n        title=\"\"\n        src=\"/CR4-DL/static/e59951817f007f98af5de43d839e82d9/e80ac/figure6-1-4.png\"\n        srcset=\"/CR4-DL/static/e59951817f007f98af5de43d839e82d9/63868/figure6-1-4.png 250w,\n/CR4-DL/static/e59951817f007f98af5de43d839e82d9/0b533/figure6-1-4.png 500w,\n/CR4-DL/static/e59951817f007f98af5de43d839e82d9/e80ac/figure6-1-4.png 809w\"\n        sizes=\"(max-width: 809px) 100vw, 809px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The visual system clearly integrates over many grouping factors but we don’t understand how it does so.</li>\n<li>Three new principles of grouping\n<ul>\n<li>Synchrony: events that occur simultaneously tend to be grouped.</li>\n<li>Common region: elements located within the same closed region of space are grouped.</li>\n<li>Element connectedness: elements connected by other elements tend to be grouped.</li>\n</ul>\n</li>\n<li>Connectedness differs from proximity because being connected usually results in the perception of a single unified object in contrast to the looser grouping by distance.</li>\n<li>One dramatic example of where perceptual organization goes wrong is natural camouflage.</li>\n<li>A successfully camouflaged organism is grouped with its surroundings, making it nearly invisible to observers except when it moves.</li>\n<li>Quantitative experiments on grouping have confirmed the importance of the various grouping principles.</li>\n<li>E.g. Proximity, common region, and element connectedness.</li>\n<li>Is grouping an early or late process?</li>\n<li>The consensus is that perceptual organization must occur early to provide higher-level processes with the perceptual units they require as input.</li>\n<li>Although this view matches our intuition, it’s a priori (assumed) and there’s little empirical evidence to support it.</li>\n<li>One experiment that tested grouping by proximity versus depth processing found that grouping occurs after stereoscopic depth perception.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 558px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/63aa6bba10b0758f555eaf8e863adb9b/42a8d/figure6-1-12.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 157.99999999999997%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAIAAACdAM/hAAAACXBIWXMAAAsTAAALEwEAmpwYAAAE6ElEQVR42lVVTU/bWBTNP5hOoeWbhARCwlecQICBMGJAiIoli6mQEGo1XUwXMyOENBULFsNmSgdaAmkSJ7ZjJyYF7Di28wVSC/y0Ofc5UPL0Yj3HPvecd+99x65KtWLbtmXbpmXalUpWyCZoJE/ip4nTxPGnY1xP4omPHz/JOdkoG1pJ10slna66qwVcrcqKksvlJCkniJIsKzzPS5IkihKfyarqmW4AXHqYrmq1WrkfZdOUCCmJkiQIWMqZTFZkgXg+oxa/GOWyZrBJYAPMrWCCyA4zVtlsFmsESIO5eF4qm00wmwR2ZGNWazUln5cV7E4GWFGUbFaQZIXIBal4fqEzcKn8HVyxK1WkCiEQCIJTqTTPZ9PpzOckf3KaSGEkU8fHJ0peNUyrROAmv7NngIkZA7s6OzvL5/MajZJhGLjl+XShUKCbsqkbZZ0x6w6YaBlzrV5HYpaXl+fn546ODpPJz2/f/h6bn/P7h5Z+WRQEwbTt7+Cy6arWmmAwf/36bWdnp7ura3h4eHx8bCQY9Hq9brfb5/MGAoFkKoVOIDXgZ8qbzBiWZdUbjVQ6PTE+EQmHx8fGxkZHuRAXDASH/f7V1VXLcjSbrdlmzIii63qxWPx5YSE0MQHk2MjIxPh4IDA8NkoqPnw4qNRqLGGkHJNkN3Nm242rxmkiAcowxwEGPKIEg0EuFOrt7flpdpZyZhL/fbYBrlGfAHx9fb2xseHzemdnZiJcGMhwOIxY0ampgQHPzMw02rNZZHSYYbgcJI1qFTmLxWIetyeAjPn9+Pm8Po/b7fF4enq637//l2SXmWynVJUmlrKGDovH43t7e9vb20ibu79/c3Nzd3f3xeqL9fX1S00zbQstbDzs+aGxbRJOyu/u7m5vb3HucMKQP1Th5ubm280NQhuM1gGzbN8X2WLtiYS/fv3q5ctf9/f/efPmt8FBH8eF3r37G7c45aB9aGzdATtIJ9tHR0cdz593dXZ4vQPd3V1I8rNn7e3tbW1tPyJtmq4j2y3gCjW2Re1tWUtLSz3d3cjW1ORkdCo6GZnE5gcGvG1Pn66trUGb3nIkWZFM4OhYVRYWYp2dHf6hoVAoFOG42Nz83Owc8t/X23t4+B/AaM9W8CPl+/v7Pp9vaHBwKjIJ8uloFK0K8aHQhKZdGrTnR7If2ZCN/kHOVlZWPB43djgZicxMT6PV+vv60CQ4mDA5nbWnM5t1tpnmRqMhiiIOECo86PVCKiZSgA558uSHv/78o351pdOpplIR+P48E7xar8N9FhcXh/1DaMatra14/OTg4IDjuGh0Cv3jdJhzpAlsUNFZMBbSKTXc6/z8vN64qtfrsGT17Ay3SCqzXnLfC02/1HRXEiOVyhcKcC+4D+wG9oe34YQieWAeHoorpiCKMv4tqBJcVRDEnOwqqCq8ElQEwCt5vKc4CycW1jBy3EtyTgBBQYVhIBC+CS6yO0aLAPgmIBD7nzhyZOEyrsAjkTDwjEB2ztOnQISTu2R6kXQW1AJsHmwI5wDwJE/ixQysH/9gnXPwEikXJRceq0UIzheLXxzTVYtFhX2xnI3go5FO89BAAnMymLFhRM1kBZeqqqCGQDxDRCjOMakSu2YyGWyHdGYFiBGIUoFmTEggZgeJCSWXOuzRdI4eHKtUKrFawncM1MmZ6M1zTbvQ9P8B0Yo061d0wgYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.1.12\"\n        title=\"\"\n        src=\"/CR4-DL/static/63aa6bba10b0758f555eaf8e863adb9b/42a8d/figure6-1-12.png\"\n        srcset=\"/CR4-DL/static/63aa6bba10b0758f555eaf8e863adb9b/63868/figure6-1-12.png 250w,\n/CR4-DL/static/63aa6bba10b0758f555eaf8e863adb9b/0b533/figure6-1-12.png 500w,\n/CR4-DL/static/63aa6bba10b0758f555eaf8e863adb9b/42a8d/figure6-1-12.png 558w\"\n        sizes=\"(max-width: 558px) 100vw, 558px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Another experiment that manipulated reflectance and luminance found that grouping occurs after the perception of shadows, demonstrating that color similarity groups by reflectance over luminance.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 809px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/b50e4cefe827659ee48bf5f2afc1f8ae/e80ac/figure6-1-13.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 108%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAIAAABPIytRAAAACXBIWXMAAAsTAAALEwEAmpwYAAADu0lEQVR42lVU6W7aWBj1s0SKOs8w00zUP81AWAK0QfMrwYaEpRFhGsgokdomEJZAgs3iHczmDduYvOGcaypVY322L/f6fOd8G5RpGYZJTJJFQeBlRZ5MJ+pElSQpl8syWUaUpNF49Pr6yrJsr9eVZNl1XcdxPM+lDMsE0rKty/xl+DgUCoXDeIXxOt7b29vf349EYpHIcZhsR0KhvyqVirfx13AAsLW2TdtaO2uGoU9P04/1xvcfDz8eHh4e64eHHw7//NBstuqNRqvVajw9pdN/X11dbd+23mYDo0x7bQLruhn6/OIiP9VmfZaVFZXluHg88flz+u7+nmW5b9+/QXyhUCiVSv5260K051GGvdYtC+DzDJ2hs7P5YjTmFXUy4oV4PIkLzMPRuN1ujwWByeaKxZLn+7bjgJAw65a9dj0mm81dXIqy0up0uMGw3elGo7FUKvn15is0V2u3vZd+EdDSFzCDzHZIzGsDNwHnMjStzefAT7QZL4gpcn3qs5wkyRw3nEy1bDabz+f/BwY5Es8Q6hwwzVab6Ox0opFIMpn6clVuNJqVyj+d5+fcRe4yn3c9yN6BbRuFclyPpumzs7P5crVjFiX55OQkkfjU74NZGQzHSATUFYol19sgxXYQ808wNNM0gzw1mk12MER5IpEomAvF4mO9Xi6X2+3nHLKSz6PO0Arlv5gh++w8M5vPx7wgK8poPCbEySTHDQRB5AacOplmMvQu20Du6mybAHsbhmE+fjxCYaOxaJTcsXfvfoMlEiexWDyRTJ2m00dHR6jz9u3tV5Mg2yj0U7NVrVZrtdu7u/ty5RoMBwcHf/z+Hl+Xr69vb/+t1Wo3N9VutwtSJJjIJjGD3LZR98Cfhz4RRJHn+Va73eu9IITpVDMMw9/6aEw/0OyQ3vYoYCzSLg5yAMMCI4U6ATlfLDAwzVaz03mWZXnjb3aYje9jTcCzxWKx0pe6vtL15Uo3MKK7KTVN0yILXUcHrwPv6EjiHTnCCguq9/LCckNUFY3Bi9Kut/gg4YqiYph5QVAn6C4NkwxbLJd4Yn40TaMkWeEFCQ0wHJM3j/8EHMoyAAow0yl87GDAwy/2YbuLGoNFkoEmJmMWJ6pKjlVVQXmxhk9gQB3YVFYVUZTgCEmlpIAlEKyBBP6wSzSrqhxw4ic8wiuaJIgCp6o2m4GIglQwY2M4GgXfqaL4M2ZsCiKJeecFGrGGIzBDHMgpjDjihE7MsIaxMK2lbqxI8g2kfkXSbpGHbekGdtATa9TPttFWzn+cDOAJHV43bgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.1.13\"\n        title=\"\"\n        src=\"/CR4-DL/static/b50e4cefe827659ee48bf5f2afc1f8ae/e80ac/figure6-1-13.png\"\n        srcset=\"/CR4-DL/static/b50e4cefe827659ee48bf5f2afc1f8ae/63868/figure6-1-13.png 250w,\n/CR4-DL/static/b50e4cefe827659ee48bf5f2afc1f8ae/0b533/figure6-1-13.png 500w,\n/CR4-DL/static/b50e4cefe827659ee48bf5f2afc1f8ae/e80ac/figure6-1-13.png 809w\"\n        sizes=\"(max-width: 809px) 100vw, 809px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Grouping by shape similarity also showed that grouping occurs late in visual processing as people first perceive shape and then perceive grouping.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 666px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/6fc0b1f480be3a2ecdb3800d153ec434/ace37/figure6-1-14.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 130.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAaCAIAAAA44esqAAAACXBIWXMAAAsTAAALEwEAmpwYAAADVElEQVR42mWUCZeiMBCE+e+jeHIfglyCiMqN/sH9kjizO281j0dCKtVdXR1tGAdGP/SMeZmTNNntd6ZlGvwt0+bnOI7rHI+HW1O/369lmed5muaRofUDSIHv+n6aZz/0/cAvLkVxuRyOh9PpFIbhKTpxFovLC9yokAI8itnIGMZxeb1Axue4qqq6rjebjeu6hnG0LIsXwPOysJ+dcgwKPKml1/sDrm+3+lbv9vswDIAFYUjkkvmlaFSy2vcxI8EL5jBQzNfrdbPder5nWxZZk3le/Aduu77th7bvn11HHp4fRHFUVuWVsLcbz/NMU8hmObYMe/4OW2isPSW4k3gEc1wXeS5lKZhFzo5hGOSM8nmRAx7+BcMsY1CCLcQZnMKSX3WlZkEYsOIHgWlbAjz/ZubtR7PX+w3t1+qLbBlrXUczjqBm+maDDNM8yZw/1tB+Jqpmbdc+2+fj+WAQPJozVStd3/3rKHLV1FyC0Wtu7vc0S8/JGc0ZUczzzJRF8IrpL5i5Mpm050JJ9Y2OtvhxtV6ZlmWYpmXbX6tVXmS4SnmRKBBagDtRp77tOmlPShVTpyTNOAXB9oc9YE6BXzGJvWJ7KwSTJ4lIUBuHoVlZVUmS6Dpgn2qbpgE/PTPOCtx9wBL2CRvfKzAOS7NsvV77PuDt0ThSbZiHafgwd23bPTWloVwaqITnu6LOVZUB1nXO2u12x6NBtdBP0QJr2wdDMfdyVQjmel54CrFXmmZwnqIIWsu29ocDzN0A7vkL3Al7Eko7zrMC4+00TVfrdRCE5AwtgsEMmIKpygswMePqVqqPT2ggohXN5Ni80ImWuExsygYzAQL6BVZFh5xKFOXFdh34Xd+jkx0evAY+NbvWVxFgLyJ/Cv6HNokLaZIOFwZAcJp2Wpbl/bo1DVtFJ43DtMySQIQo3NpKMOndH3cyuTW35t4Q/OPxUCsMXlhkYHKcwye23e6NeDa1JtrdsigMdwCScBZm4BfjZ/nD29wh6oZRJs/yjETOSaxxPzDhyYckS+TnrCjkNE04kb4AILZlOf7hnVMAJ+lZo8VpPZi5a1UzMQUQRdGlvCRpCh4AL4STiidNFssoYi3PAV8AcwrfmKrApMNL1Z4AsjzPxBGJlAMhhEB/AGIt1OwnQ9ngAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.1.14\"\n        title=\"\"\n        src=\"/CR4-DL/static/6fc0b1f480be3a2ecdb3800d153ec434/ace37/figure6-1-14.png\"\n        srcset=\"/CR4-DL/static/6fc0b1f480be3a2ecdb3800d153ec434/63868/figure6-1-14.png 250w,\n/CR4-DL/static/6fc0b1f480be3a2ecdb3800d153ec434/0b533/figure6-1-14.png 500w,\n/CR4-DL/static/6fc0b1f480be3a2ecdb3800d153ec434/ace37/figure6-1-14.png 666w\"\n        sizes=\"(max-width: 666px) 100vw, 666px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The middle occluded circles group to the right with the completed circles, which showed that grouping is based on similarity of the completed shape rather than the retinal shape.</li>\n<li>These three findings and more provide evidence that grouping is a relatively complex and late process in vision, which doesn’t match our intuition.</li>\n<li>A seldom mentioned factor in grouping is past experience.</li>\n<li>The idea is that if elements have been previously grouped in past viewings, then they will tend to be seen as grouped in the current viewing.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/3be36312231169ae64b7dabe6ab4da40/d74fe/figure6-1-15.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 76.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC6UlEQVR42iVS207iUBTth4hP+jA4D/ow3kafGC8ZmASNRrwm6jwpEB+0oDARIgQKOkoFpC1QWnpOL/TCDCT+3qw6JzvN6d5nnbP2WptxXDeVSs1/mZ+bnV1dWVldXc1kMul0+ubmhqtwKZY9Pzu7uLjI5XLFYtF1Xc9zh0PPQ3gOY9l2jeevr68PDg7YVCocDp+ent2wbPXxURQFHBqNxxohum5omgboaDwCDpe4rs0MHId/rR/s729ubsZi+3jw+Pg4EokcHx0tLMyvr63F9vay2SyQjuPYjiO12/hiuc6AGf4dxROJYDC4vr6+sbFxeHj4dXkZLXyemZmcCCwuLCaTyXqjIUpSp9tF1OsN07J8rA8ejRLJ5NTU1PT0dGBiYjIQmAxMBj8FIz8i8cvLZrP5/v5OKZUkiX/FqhNCfbALFhZjWJbcUwRRfH6plctcqVR+enrCr9rvgy3HcfF4nOdfeZ7PZn+hVK1Wu3LXduyBbTG6aQLvDYfuR/Q1DdygEDa7u7tA/jw/F1otQkipVKpUq41mk1AysAeWZTDmYNBTFJZNVR+fwMcwzVqt9vDwAIosy4a/h/F+r9cbj8eEUtz41mo9v7xgb5i6D1ZUFca0BAGpQqGQSMTh3O3tHbzNZjLfQqGdnZ3t7W0wr1QqgigoqgLxLUtnBrb9UqsBCT1jsdjJyUk0Gt3ait7n7gWhpShKsVjI53Ow8OrqKpFIlMtluAXBDIMy6PPu7m5udg6QpaWlUCgEPIZMakuqCk7K8A+E6KMdfNvtNoe5q1TQs2kC7Hk4ivHEbO3HYql0+vfzcz6fLxQems2G6/mDAZ5Y1ocikFY3DDTv9+x4XpnjIBguRgrkodBbqynLHUhOKYHVpmn25B7kBMy0TGyoofvgPiEAwFUUfG6EwGTLMnWdwkmqkzffGyqKIkaF6roGo5ClVNP6DKSSOp3/09eVZQScayMjioRoukFVVUUJSbkngzzqONfuYFQ7/wCu2uoOHP2F9AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.1.15\"\n        title=\"\"\n        src=\"/CR4-DL/static/3be36312231169ae64b7dabe6ab4da40/00d43/figure6-1-15.png\"\n        srcset=\"/CR4-DL/static/3be36312231169ae64b7dabe6ab4da40/63868/figure6-1-15.png 250w,\n/CR4-DL/static/3be36312231169ae64b7dabe6ab4da40/0b533/figure6-1-15.png 500w,\n/CR4-DL/static/3be36312231169ae64b7dabe6ab4da40/00d43/figure6-1-15.png 1000w,\n/CR4-DL/static/3be36312231169ae64b7dabe6ab4da40/d74fe/figure6-1-15.png 1164w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Past experience can have a dramatic effect on grouping and organization, especially if the organization of the image is highly ambiguous.</li>\n<li>The effects of familiarity and object recognition on grouping are interesting because they suggest that grouping effects occur as late as object recognition.</li>\n</ul>\n<p><strong>Section 6.2: Region Analysis</strong></p>\n<ul>\n<li>We haven’t discussed the elements that perceptual grouping act on but the obvious basis is the analysis of regions.</li>\n<li>Region: a bounded 2D area that constitutes a spatial subset of the image.</li>\n<li>Edges (1D) are useful not only for depth perception but also as boundaries that define 2D regions.</li>\n<li>Uniform connectedness: the tendency to perceive connected regions of uniform image properties (E.g. luminance, color, texture, motion, and disparity) as the units of perceptual organization.</li>\n<li>It’s been argued that uniform connectedness occurs before any principles of grouping take effect and that it forms the elements that grouping uses.</li>\n<li>In general, if an area of the retinal image is homogeneously connected, then it almost certainly comes from the light reflected from a single connected object in the environment.</li>\n<li>This isn’t true for camouflaged animals but the assumption works most of the time.</li>\n<li>Perception goes astray whenever the heuristic assumptions underlying a perceptual process fail to hold.</li>\n<li>It makes sense for the visual system to organize an image into objects by segregating it into uniformly connected regions.</li>\n<li>Region segmentation: the process of dividing an image into mutually exclusive areas based on uniformity.</li>\n<li>Two ways to segment regions\n<ul>\n<li>Detect the sameness/similarity of adjacent parts of the image.</li>\n<li>Detect the differences/gradients in local visual properties that divide one region from another.</li>\n</ul>\n</li>\n<li>One possible mechanism to detect boundaries is to detect edges.</li>\n<li>Algorithms for detecting luminance edges can be used to partition the image into connected regions of approximately uniform luminance.</li>\n<li>An alternative process is to find uniform connected regions by some global process of attraction among similar elements, and edges are defined implicitly by the boundaries between the regions.</li>\n<li>E.g. Partition the image into regions by finding the set of pixels that are both most similar within a given region and most dissimilar between regions.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 999px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/40fca6aa23fc51f4e13303ba01358b94/20c85/figure6-2-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 85.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAADJElEQVR42jVTW08TURDeXyAPgokaoYKAoCRSMSRInySRUCQSEAK+USLEJoVICDwQAVtIL1sILW3p3q/dS7s3WigRfpzfFjyZbM6ec76Zb76ZIXhBYFhWNwzXc23HsR0by/pvDwuH/pXjuI7rOt7lZaNRrzfqBM8LwNcs67Jedz0P5ri+ud5la+M2b25u7+7wxQIAyHq9fnXVaDavCQ5oUTTMqmEahmmaVVgVQSqaBjq1mpVMJre3t9MZMp1Oy4qs6Zqu63ioKAohCIIoSXiH14qq6IaOLEBYUVX48jxvZmam/XH73Nzc+vo6SZLwKyuKKIk0w/i0AVYUlaJpZEBRZZpmDENHEJ7nwfD78nJ/X9/U1NTh4SFgf29vETpDkizL+mAcIdrJyQnPc4IoGC1KFxcX5XJZ07SlpeXV1QjW5ubm4sJiZGXl7OwsND5OkmlClmVRFOGMYRlVVWtWTZJlIGmaRkbIf37+25vBwaGhty8DAWAODg4kSVpYWMiQGQKc1UqFoqlCsUgzdKFQMAwD7vAC2gM8+XmyBQsNvxtOk5lkKplKpWKxjcRRgkCRYABUqxDaBE8cqKpimgYqjRJks9l4PH50lEgk4uCSy+VACgJRFEXIispyPLQttaiWKT9PiM9xHIJXKpX8+TkY7ezszM/NQ8Xr5tXe3l6gqyscniJaukvFUikeT+AL8tADTYXKsSyDUKl0OpfPrf1Y6+vtnf06Cy+/9/fb2h6NvA8SiILgkPpPPM5yHMDwhVQRs1gswB1oo9T9ff2QbaB/YPD14MSniVAo9HFsjFA1HfKenp4eHx+XShfnhQJ67p62L7goQJ7R0dEnHR2dLzoDgcDzp89e9fSsRCLT4bAP5nghnz8ns1mUlmFRagnd47ou+gyF2N3dRcju7u7g8PD09JcPIyOxjZgoK8tLS4Rc0dAnUAtsWZ7P5fJoOZblwNxvctuORqPBYDAcDv/a2voZjYKa43mmZaNiPpjh+Aomw3Zqtm3WLAOlw96yq5Zl+WPoPoyn4+Aco9a6snH1ANbQkxaQ/ilc+PZ/YzuYc3+OW/Ps3f/CLNv+BwevQSKXBlCpAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.2.4\"\n        title=\"\"\n        src=\"/CR4-DL/static/40fca6aa23fc51f4e13303ba01358b94/20c85/figure6-2-4.png\"\n        srcset=\"/CR4-DL/static/40fca6aa23fc51f4e13303ba01358b94/63868/figure6-2-4.png 250w,\n/CR4-DL/static/40fca6aa23fc51f4e13303ba01358b94/0b533/figure6-2-4.png 500w,\n/CR4-DL/static/40fca6aa23fc51f4e13303ba01358b94/20c85/figure6-2-4.png 999w\"\n        sizes=\"(max-width: 999px) 100vw, 999px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Evidence from stabilized images leans towards detecting edges as the method of region segmentation used by the visual system.</li>\n<li>Stabilized image: an image presented so that it’s completely still on the retina.</li>\n<li>Strangely, the visual system stops responding (or adapts) to optical structure in the retinal image if there’s no change over time.</li>\n<li>E.g. The retina’s blind spot or blood vessels.</li>\n<li>Stabilized images support the claim that people experience the shape and color of regions only due to edge information.</li>\n<li>Another important perceptual process in the organization of perceptual objects is parsing.</li>\n<li>Parsing: dividing an element into parts.</li>\n<li>E.g. A cat is parsed into eyes, ears, legs, body, fur, etc.</li>\n<li>It’s natural to parse figures by deep concavities.</li>\n<li>Deep concavity: points when the contour undergoes a sharp bend towards the interior of the region.</li>\n<li>Like grouping, parsing must logically come after region segmentation because segregation forms the elements used by parsing.</li>\n<li>Parsing is essentially the opposite of grouping.</li>\n<li>Texture segregation: defining regions by texture.</li>\n<li>Texture segregation can be understood as a special case of region segmentation where edge detectors operate at a higher-level scale.</li>\n<li>Textures segregate more strongly based on differences in feature density.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 831px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/4e85cfcb8dd8aca4a662f90ced1597a6/5b4a1/figure6-2-9.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96.39999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsTAAALEwEAmpwYAAACyElEQVR42k1SiXajMAzkz9u+blvCHe77DhAMmDvp3+0YuoefQmRboxnJ4tZ13bYty/PLRdANA6ZqVz8IZUVVFFWWFUmSJUX1w9B2HD8IHMfpOjKMdKADtx5oy3I+P3nHCyRZe339para+/uHomimab+9vn99CbbjKer144PXDcu07ANMuWVZt33zvECUVFyomi6ICpzLRVSUq2Ha1yu06KblaJphO65tu0mWj/M8jCO3gHjf86LUVB3RgiiLkmK7niQrkA05giDJsmrZDhIFYQQt5a2e5oWO04/sKE4kSRFF1KjxF9FxPZ4XPT8ADw7RBdcNUBrPC1fdzIsC4IEeYDTM90NcWLYLhWDWdROEEGxZNmjRAsOwkNEPI1nWXNefFzCP3Lqs+2Ovqhp6oigJw9gw7DhOgiBM0qwoSrQjYSurqqZpWtcLiuI2zfOPbKSBnRKWhXn9QNuWkK67t6RuGtitQp3zNE3AzPOKLwMjFLsTfC7sCOnBGUVxmuae57uuF0ZR13XjNI3jhD6fxsAn83IwY50txHZ/PJ7f3/CP7QYOsPaU/gOv/4H7AS9PEXpqgVp0G0cDHSHyrGJalh88/QNG9OP55PkL3vbx/YS8eVmrusGcISni5nUtyvLl5a1pCZ2mbqBIwcB/laP7dd0cuZiPFJDSMxvOLZqAjOM89XSEcRgvVue2nu1ed1bhMAyYuTCKYZgqTEua5VB3VM7y0rPme9t2fY/cBH8HyVEkxQ6PddwOcBqU2/Vt19VNe25xwTmOC8NUgASjUt6qLMuzvPD9ADMbYzjyPE5S3OIEYQjGnMRphgni8PpleUvTDAsOupJDcXkLwhCJ4AF8pMvPRHDhoBb4HOI838c1piLJUvwhfZKm+LFvykKjGMVjZNhNiMk9GgFyrigKnAIMPobKMqiITnyaQhcaQTqCagkrvrsTUt9bfFtCfgMC4djMmcqOqQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.2.9\"\n        title=\"\"\n        src=\"/CR4-DL/static/4e85cfcb8dd8aca4a662f90ced1597a6/5b4a1/figure6-2-9.png\"\n        srcset=\"/CR4-DL/static/4e85cfcb8dd8aca4a662f90ced1597a6/63868/figure6-2-9.png 250w,\n/CR4-DL/static/4e85cfcb8dd8aca4a662f90ced1597a6/0b533/figure6-2-9.png 500w,\n/CR4-DL/static/4e85cfcb8dd8aca4a662f90ced1597a6/5b4a1/figure6-2-9.png 831w\"\n        sizes=\"(max-width: 831px) 100vw, 831px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>E.g. It’s easier to segregate by texture when the line orientation, brightness, color, size, and movement differ between textures.</li>\n<li>Malik and Perona’s Three-Stage Theory of Texture Segregation\n<ul>\n<li>Stages: Spatial Filtering → Lateral Inhibition → Edge Detection</li>\n<li>Initial center/surround receptive fields capture a textural dimension that’s used to segregate regions based on texture.</li>\n<li>Many filters are distributed densely over the entire visual field so that the output approximates the convolution of the image with every type of receptive field.</li>\n<li>The second stage imposes lateral inhibition between nearby cells that have different receptive fields, suppressing spurious weak responses.</li>\n<li>The third and final stage of the model explicitly computes the strength of texture gradients by running the output of the inhibitory stage through a set of coarse edge detection operators.</li>\n<li>The final texture gradient is defined as the maximum gradient over all filter types.</li>\n<li>Texture boundaries are located at local maxima in the final texture gradient.</li>\n</ul>\n</li>\n<li>Strengths of Malik-Perona’s theory\n<ul>\n<li>Is specified in explicit computational terms meaning it can be implemented as a computer program.</li>\n<li>Applies to any image.</li>\n<li>The computational model corresponds closely to human performance on the same textures.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Section 6.3: Figure/Ground Organization</strong></p>\n<ul>\n<li>Figure/ground organization: the separation of figures/objects from the background.</li>\n<li>Figures appear closer to the observer and contours are attributed to the figure rather than the background.</li>\n<li>Experiments show that the visual system has a strong preference to assign the contour to just one of its bordering regions and to perceive the other side as the background.</li>\n<li>Since the figure is always perceived as in front of the background, this suggests that figure/ground organization is related to depth perception, specifically occlusion.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 825px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/3a1238fa890657e15d553ca852efb2e3/d4c13/figure6-3-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 88.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsTAAALEwEAmpwYAAACqElEQVR42lVTa0/iUBDtDzRZ3SC1RQoW9JPrfjNBDZuIYVFaHmr8YJA+7m0rKNAnFH/dnrl1X5PhPqZz5pyZFinL0mwNz/J8w5hrmub9/Xg8HsJxED7C+vh4//Aw5txdr9M0jYUnUpqlaZYkaZJv8263u7Ozsy+sXJZl+QCmKAeqWlHJlF6vl+d5nBBSgMUWJ8kmzw3TBOJQWLVaq9frVWy0HzUaTWyGYWw2GySnJDiVCiQ8W6/xrFQqCdqyIFRlWalUDjWtpuu6pml9w9wQcwHOfoNjuvdub3d39wBHiVqtdnr6rXVxeXFxCRnNZhMSDMPMtx+Q+glGA1EUh1GUZWuA9/a+otX9/RJaQDxKknb7B2DHx8d6ozEYjv4HxxGQcERuBRiqZbncarVeZ7PVKhyNxugZzFhNc5Bvt39lRwQOo5gi/X4fDWNIGC+ozs/Pz86+X11dgbPRbOp6Y0jMBC7IJXCuwhCO6x2BS8WEILUiDIUwLTAfHekAbz8+CjBcAnC5gocYPaYNycgGuP5pJBgqTk5OwDwYDP8wExhAgGG43N31v3zZVYQdyOIDUVQUwmvXqpqqqKgOMHrEyyEwBgaHoYfJy0u73e50buDXwm46N93uz+vrjoh0LNvCeybaTPTsODbnbP4259xbLpdQRDJCjIJmiE3wJFQ9TRaL97f3N6zQu1gupKenx+fnZ9txJpOJ53lBEDDGLMvCmXGOuMvIsHq+77guIjhwPGWcmF3XdVwHqazIw9VxZ/O56zIcChhSPc5RGtfg9ZWSOQPYsaHc923bZpQCozNWZOBCPz/Aalk2Snm+V4gChwTAdDpFPT8IbNvxhHTO+dSygtkMHxlofUGIGgCgCuTgKeISNALskHI2tWxkRmJIYRjG/xjF6P8TF08jEfwFv5FwAALJq8kAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.3.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/3a1238fa890657e15d553ca852efb2e3/d4c13/figure6-3-2.png\"\n        srcset=\"/CR4-DL/static/3a1238fa890657e15d553ca852efb2e3/63868/figure6-3-2.png 250w,\n/CR4-DL/static/3a1238fa890657e15d553ca852efb2e3/0b533/figure6-3-2.png 500w,\n/CR4-DL/static/3a1238fa890657e15d553ca852efb2e3/d4c13/figure6-3-2.png 825w\"\n        sizes=\"(max-width: 825px) 100vw, 825px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Factors affecting figure/ground organization\n<ul>\n<li>Surroundedness: if one region is completely surrounded by another, the surrounded region is perceived as figure and the surrounding region as ground.</li>\n<li>Size: the smaller region tends to be perceived as the figure.</li>\n<li>Orientation</li>\n<li>Contrast</li>\n<li>Symmetry</li>\n<li>Convexity</li>\n<li>Parallelism</li>\n</ul>\n</li>\n<li>These factors, like the principles of grouping, can’t predict the outcome when several competing factors are present in the same image.</li>\n<li>Figure/ground organization likely comes before grouping because grouping operates on elements already differentiated from the background.</li>\n<li>Attention tends to be the driver of figure/ground organization rather than the other way around.</li>\n<li>Figure/ground processing is influenced by knowledge of specific object shapes and by object recognition processes.</li>\n<li>A convex figure can be perceived as a figure or as a hole in the background that reveals another surface behind.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 824px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/a66a149e498f00572420086584f4224a/c1c45/figure6-3-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 82.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAACZ0lEQVR42lWTzVLUQBSF8xiAWiWgW0sXapXKIDBUOe6kcIN7ZBTcYlHyBLiWFSbpdLozmfx3Ms/n190O6K1U0kn63HvOvaeDYej91femKIr5fJ7n+dwFi9kytNa8djZMZwwP9ge3YGOM0plKVZqmQogoFlEUE2EYRXYl+G6hvQ1j7HUH5ruUqZSSepSdzfIsm3FppXU2A84vag7DsFgsuLMOyGITGdO2rVKKCt/Pz7+dnU1PTj4fH3+ZTllcXFwIkaSposBicMjO1E0bmGU0DWANwzevXz24vzaZvPt4eLi/P763ujLe23NgDVXKwrNuuqpuAyvDRd005Ebhzs7bjfWHV1c/+364vPzx+NHm+8kkSRJS/wNuawduXQ+7pqkBQ3u0tbW2unL69RTGz54+2dxYH4/HiaRfqoX2YkGlCnRngtaFAzc0jD4fHHx4+eL5HrG7u729PdoaHR19grZ0mn2rmoZRDXdg7qlMoecnxD2KSCX9kJKEzGnrwNYUg+3zf2CcwBadZciDpLS5gDAETRpLG3Q/+IBFgNRbsNtNV+204QlTa5UYYEJ9EjER57CeC1DgrIYATNJVVVWWVVUTTVGU87IsymrOovi7aN1knEcNeYIwDGErYoGBS3YXRV4Uyjq5wONImOU5tqUuRrMKlMoy+8oBCK6vf1knR5HrirURZrQUpWQ3nfNC2CPi+ObmBgn8RAUWDlDnRFqdtjNaczasSik9jI9S+qcCzChIzxfqQ/t36IcTC1dKYLLYHSnhhhTZidlBcVR4JTtSGIKvLKMwImtqg7umvBuV52KHRJHlCej9w/f4D3OvgNSSmgKtAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.3.6\"\n        title=\"\"\n        src=\"/CR4-DL/static/a66a149e498f00572420086584f4224a/c1c45/figure6-3-6.png\"\n        srcset=\"/CR4-DL/static/a66a149e498f00572420086584f4224a/63868/figure6-3-6.png 250w,\n/CR4-DL/static/a66a149e498f00572420086584f4224a/0b533/figure6-3-6.png 500w,\n/CR4-DL/static/a66a149e498f00572420086584f4224a/c1c45/figure6-3-6.png 824w\"\n        sizes=\"(max-width: 824px) 100vw, 824px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><strong>Section 6.4: Visual Interpolation</strong></p>\n<ul>\n<li>One complicating factor in perceptual organization is that most surfaces are opaque, which blocks/occludes surfaces behind it.</li>\n<li>What the visual system needs is a way to infer the nature of the hidden parts from visible ones.</li>\n<li>The process isn’t perfect, however, as the system makes its best guess from incomplete information.</li>\n<li>We are remarkably adept at perceiving the nature of things that are only partially seen.</li>\n<li>Visual interpolation doesn’t include visual experiences of the completed surface, but only perceptual knowledge or beliefs about its properties.</li>\n<li>Visual completion: how the visual system automatically perceives partly occluded surfaces and objects as whole and complete.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 826px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/36d4a03b9589d66b9cd597bca7ffe94a/6a6e9/figure6-4-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 89.20000000000002%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC4ElEQVR42j2USU8UQRTH+0M4cCPRC14UwwGDZzxwwRAI8RNISDSgMS4EUGQ07ESjDAPT2/R0VS9V1bV0T3PR+NH8Vw/YefNS1fV+b60eRxsllSqkFBBltTJ6QMjh4dHBwb4feNWwyjlnooDOcpbcSsqYIy2gGpFcKlZIpU3Yj7a2d3a/7G1ufuxcdKQxKRNJziF2kXGSgecOjIXSshG44NibMogGO592t3c+z8/PP5mdZYLDKbBcFFzKjBeNI+EUYJRWSmtjRMNLrXNeRDFdXFwaGxufm3uKDJEUGCQPA7iAo4zfwtBgbvNXMJVl1e15r9+87Xm+NCVIG7nhR+uMCUc08IjnoxL0Tf6FNrxxhJwhYBAN2H9xlDGF1hzNZDxjHIzUBsJgyniaMyYsiYCssG9s25s3sHRMWZZVleV5USCSqevhcFgzLkBhMBhPfV2jidGAxITGhCAYt2XbGVkYgpDVcLi1tbW2+kIIhsmjNb///G232wf7X+GdpPnLV+tr9lmNSIyckYiFbZ+lPD45uXf3bqt1Z2NjfVhfx5SedzoLC8+mph4gWVi3vx08npkZH2u9+/BelXWObo9gJNO9cqcfTU9MTLTbe8O6RkkrK89brbHl5SX0DiUdHp1OTt6fejh19v2HUAYDc0BqU6IqzONXp3tyenbjThuascPjU0ITrDEkLvXP8+5F9woTwZGF6+trxEHBpipt56WqqtFaN1voUpflaGBgaJanmAu3t8WJoogQkjQPGp6kKSTH1U9TQgmuFqEUa5pmYRTj2kGHaH2S0DR1LrqXgR+E+BSiQWznQWL8KPX9wHW9MOrjzA/8aDAI+/2e63YvLz3fdz3/8qrnWENKwzBEbdA0SWANgTX89ft9uAQckzgIA4jnea7n+WHo+r6DF/AEryMGWzA4Bgw7H0FcmPkBDsJwEMc9wLCPIvAOjBATGgdhw6NUeLCV3D5NzhGSYUIUzR8GL+xt/Qdwdaof8//hEwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.4.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/36d4a03b9589d66b9cd597bca7ffe94a/6a6e9/figure6-4-1.png\"\n        srcset=\"/CR4-DL/static/36d4a03b9589d66b9cd597bca7ffe94a/63868/figure6-4-1.png 250w,\n/CR4-DL/static/36d4a03b9589d66b9cd597bca7ffe94a/0b533/figure6-4-1.png 500w,\n/CR4-DL/static/36d4a03b9589d66b9cd597bca7ffe94a/6a6e9/figure6-4-1.png 826w\"\n        sizes=\"(max-width: 826px) 100vw, 826px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Knowledge of visual completion falls somewhere on the spectrum between full sensory experience of visible objects and purely cognitive knowledge of completely unseen objects.</li>\n<li>Three theories of visual completion\n<ul>\n<li>Figural familiarity\n<ul>\n<li>We complete figures from experience using what we remember about the entire figure.</li>\n<li>The hypothesis is that people complete partly occluded figures according to the most frequently encountered shape that’s compatible with the visible stimulus information.</li>\n<li>One issue is that we can complete objects we’ve never seen before just as easily as familiar ones.</li>\n<li>This doesn’t mean that familiarity has no effect on completion but that something more than familiarity is involved.</li>\n</ul>\n</li>\n<li>Figural simplicity\n<ul>\n<li>We complete figures using the simplest possible figure, thus maximizing simplicity or minimizing complexity.</li>\n<li>But how do we measure simplicity? Without a quantity, we can’t minimize it.</li>\n<li>This theory fails to account for experimental results.</li>\n</ul>\n</li>\n<li>Ecological constraints\n<ul>\n<li>We assume that the occluded edge somehow connects with another occluded edge in the scene.</li>\n<li>Edge discontinuity: an abrupt change in the direction of a contour.</li>\n<li>E.g. The corners of a square.</li>\n<li>Hidden contours are perceived when the edges leading into discontinuities are relatable to others.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 831px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/1440c57a24ceb0f5cf15582a89823106/5b4a1/figure6-4-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 92%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsTAAALEwEAmpwYAAACoUlEQVR42k2UW2/TQBCF/bsgQZVQKZfHVkjACzQqD+Wh/AgKpQ1UQON4fb/bu74n+YF8a0cBax7i2T1zZs4ZxyhleQippBBWXhR12+WlflWVIoRt50Xedm3bNkTXtVMYGqZkqZSsVBhFcZyszNXP+/sky6qmbtomSdMgim3XTbNUyrKdkH3X950BbERWWZ77QdAPgyXE8fHz9x8WqqoIx3WjJBWOE8VJmmVSqU4/7T+wquswjv0w/PrtZrFYnJ29Pjp6SrfVmDctQVPc4hW8Ju0m5qqCtpCS+ZqutYT98sWr2ezJm7fvuMd81I3TDFaQcZJoZrC0PQyGHMGaX+rm0WltiZub2zBOGI4MGC8IXc9nLkSctNqDYSYoT3AVeZhzu9txXjco1qBvVhQc7SVu2z1z3++ZQcJQNQ2zFchf1+U4IUjwtI2K/WbYbjcafFB7AhNcwmGm8hA9jJIsVyMYWuG4a8v6cn3tOC7dTh3s1T6M7fkMVnAvwPAkKUoJQxiF5CH6eHExn8+Xy7vtbov/3f/MMHhBYNm27bgRk4/aIg2FHkzz/Pz89PT08aPZp8vLzW47rcoeXEhFqFqP5/p+WpQorK1qW9U0aHF19Xn2eH7y7GS5XG6YfAJX49nYNlVwJSCiJEnzrO33S0wLWZabeGg7wzC0hw1jgTghWGB8pPnJGFZdavOlGr1gCTabDZOTwZbRssa4W/74fntHzftfv1F1tbZW5tp2PUjM9RqpUNjx9CvbihG24/i+F2JIGBqk6IdjAGshVqbJzJSgRywjSPJJgcEFPhLP96IodF0njiPD1CkPvB7I9ajNnjzALmx+WdgWhpTzAh9AAiJi0Mj1XCobeENQECr4qc3zZ/VAIdiEEHQu9AWPvKcftywL/T+h5F+ERaa1sqgHmgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.4.4\"\n        title=\"\"\n        src=\"/CR4-DL/static/1440c57a24ceb0f5cf15582a89823106/5b4a1/figure6-4-4.png\"\n        srcset=\"/CR4-DL/static/1440c57a24ceb0f5cf15582a89823106/63868/figure6-4-4.png 250w,\n/CR4-DL/static/1440c57a24ceb0f5cf15582a89823106/0b533/figure6-4-4.png 500w,\n/CR4-DL/static/1440c57a24ceb0f5cf15582a89823106/5b4a1/figure6-4-4.png 831w\"\n        sizes=\"(max-width: 831px) 100vw, 831px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>Edges can fail to relate if their extensions don’t intersect at all.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Illusory contours: when visual interpolation results in contours that don’t exist in the stimulus image.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 818px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/78cc6465ff8ecbb588535810c019dc12/64d87/figure6-4-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.19999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAACF0lEQVR42j1S227TQBD1r7QF3iMBElB+IJV4AAEP+QukPhaoEiH1sXEjUTnxfX2P7b0n9Oc4s466GtmzO3POzJldzxhpjDJGT2YPuu87KcX19Y9XLy/u7v4YazkfrFUu05k95XvaKK2V0rTHVxm97/dxkszn89lstlgscsaGcdQUnQooiXTnew6KrUQYPyFBbNMs8x82TdP5/gZgnAiphHIFrXHQU2UHmQgcBVJA0Q+jVJAwINNaY4+Hw/EIBC3ak3laCweWMC7EyLmQAg64Ri5QBPoNkqRA84QhsJkcT1E7ZKiG1AffXy1XICA5ThmIbn//8tfrrttT0xbrpN4DsVNKYC742zevv3/7ihAwylFiQl8+f/rw/t3T079TZScYAj10KgTnfHSNDR8vL8/PzpbLVVU3SG3a7ubm54vzi6v5laGK+vlSaWDjOEDMwMeRj7gqxor7+83fx2Df9/aAIcswirbbcN8PpF+RHP08bWpY0ITcnDiCEFyU9STY0gsRmuaP+fHpkC7czdsLgkfGsizPcpbXTZ1meZ6zpuuKsmzbrmlbVmCVjJV5jrOqcyFcfte1nu+vwyjc7XbBNmBYRZGkyS4MkyRN06yqiA4+YHGcRHFc1TXoi4Lh60VRiH2apXEST9llVaE4OoBacMEBD0pNX0Rx0jQ18F4cR2lKlGgOqWgefg6vLJHET+OQ7iKnh0iveHoF/wFvICj0pbEtOAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.4.6\"\n        title=\"\"\n        src=\"/CR4-DL/static/78cc6465ff8ecbb588535810c019dc12/64d87/figure6-4-6.png\"\n        srcset=\"/CR4-DL/static/78cc6465ff8ecbb588535810c019dc12/63868/figure6-4-6.png 250w,\n/CR4-DL/static/78cc6465ff8ecbb588535810c019dc12/0b533/figure6-4-6.png 500w,\n/CR4-DL/static/78cc6465ff8ecbb588535810c019dc12/64d87/figure6-4-6.png 818w\"\n        sizes=\"(max-width: 818px) 100vw, 818px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Factors such as symmetry and parallel edges appear to block the perception of illusory contours.</li>\n<li>If the inducing elements are perceived as complete, illusory contours or figures aren’t seen.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 623px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/3a6d7162e4814c019759732a18624be5/6114d/figure6-4-7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 138.39999999999998%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAIAAADuuAg3AAAACXBIWXMAAAsTAAALEwEAmpwYAAAED0lEQVR42mWVy28bVRTGR3SLuqQPuqBkUTZNhRKEClQoQCRQWxb0QeOExEhRWSYVQkkAE6lKuqlDWjvGcT2e94zHM/ad98PPv43vzp3YKYyOJsd35nfOued8d8KFURCGAbt834/j6OSk8tntT/dLu8Ph4PXrVx/fuvnbr1tB6Nu9Xs8hxCFd4nQJ6RHCgZzCruf1h4MnPz658M6F1cJK2u/v7Px+8d2Li4uLjuv2iMvgHjX4LhcEQEPGu74PV1a0ra3tbq9Hf0bRzs5etVr1grAHjjK4E3IehoHD21EcN5v8+k9rmqb6QYAQxY3i7t6u6/moliXMzfVmMMzxvMF4VFhd4zju+/v3RpPx9vYz+Lfm522EOUcS1yMMDqMwq5tmDqNYFJUHPzw+OvqbuK4gyoWVtZflI1q2mzFn5FtlM/N8H30ajyej0cjsWFGcDIdDrDiuh7pgjDzLHIbnekZbb5pmobBSqRynaWoYxqOHD8rll34YopD/wqxmeo+QNojTtFT66+qVqxvr60j4/PnhtfevFQqF/6dF/RzDkBC9JR7dSRzHhwcvul0bi3GSHh4cWpaFPZNsz9NuZXCWlrWarWKrvzx9Wq/XwigyO52fi8VXx8foBSMpzCwvO8uMx5gk2v1naf/K5ctrq6uD4eDg4MWl9y49fvTQ87INOx6kRs74HGbJwUMMUMve3h/tdhvvwS+V9o22AamxhmUh8mnn2matZg7w0Xic9geSJHeszmQyCeMYqmQknVmWls05JzOFBl5A5ywIPOQZJUmSJs1mUxSFmPIuTe7mJPbIAZgaTlWcJNWT2kc3bty/dxc6r1Sqc3NzX3x+26G69rKz4cxgFDk1wNjbmyZ//YMPNzc38VOS1e++vbu8vGyabTQg26rDyLcyMx6FDcfjcrmM045Ag9Ho9LTRarUGg343P5LuDD7XMJTto+xa7Z+FhYXixjoyq5r25Z07nywu0HxUCLM55Q2batsPKNzkUfb1b77+CplkRZm/Ob+0tATBT+U9HfXsS8LMz05V/fQUHywPyvB9Xmh1bBtbcLycdKYiSRJMgR6OKIqyGDR/kqZotWXZhmniqHhB4Pg0EB1yNi26bcCGrlsdC9+JtmliDHAs2+5YdAWGL2bH7rYhd8vS9XYHorEszcCCbVo2V0N/6nU0Bl3FQ90wGg0MqyWJkqKoLUHkeTS7JYrUwQNBFBtNXsQMdYNTVRU5cZckEUuSLEOVooSbjG4hBEiZLtInWEcaaI5v8YqqcnL2kqbrCi5VzXmsqSqSCIIAHw7ego/8WG+8QQW8IEkcSMRDaYqqgdQ0HYFYOACapslZFF034KM6WUFdMFAKh68UzkB2KTiG9G1FJoQEYa452tuz8eIvbSph/3GcfwHbISry7ORu5wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.4.7\"\n        title=\"\"\n        src=\"/CR4-DL/static/3a6d7162e4814c019759732a18624be5/6114d/figure6-4-7.png\"\n        srcset=\"/CR4-DL/static/3a6d7162e4814c019759732a18624be5/63868/figure6-4-7.png 250w,\n/CR4-DL/static/3a6d7162e4814c019759732a18624be5/0b533/figure6-4-7.png 500w,\n/CR4-DL/static/3a6d7162e4814c019759732a18624be5/6114d/figure6-4-7.png 623w\"\n        sizes=\"(max-width: 623px) 100vw, 623px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>This shows how what’s perceived depends on the structure of the entire configuration and not only the structure of local parts.</li>\n<li>Perhaps illusory contours and visual completion are just two different manifestations of the same underlying unit formation processes.</li>\n<li>They differ in that if the missing contours are part of the closer occluding figure, then illusory contours are perceived. If they’re part of the farther occluding figure, then they’re visually completed behind the closer figure.</li>\n<li>It seems that some cells in V2 respond to illusory contours the same way they respond to real contours.</li>\n<li>But no physiological explanation has been proposed that fully explains illusory contours.</li>\n<li>Perceived transparency: the perception of an object as being viewed through a closer, translucent object.</li>\n<li>With translucency, the light striking the retina provides information about two different external points in the same direction from the observer’s viewpoint.</li>\n<li>E.g. The translucent surface itself and the surface visible through it.</li>\n<li>No notes on the perception of transparency (transparency isn’t natural and our visual systems didn’t evolve to handle it).</li>\n<li>Figural scission: when a single homogeneous region is split into two figures with one in front and occluding the other.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 830px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/41370dc3932b2e42dba1f5dc096e075b/715a3/figure6-4-16.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 65.60000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAACHElEQVR42m2SS2/TQBSF3QVVk2waml1UtUEB/gBBAtYhLRKlVAJSsgJll7KmgFS20AXQSpVwHnViO37GnpedRJB0h/q3OGOnEpUYHV3NjO93557xKFEcRREXgiHGsfB8d2/v+dbW42bzzdnZKeeMUBKS4L9SAAs5JMwFjcfjDx8/ZbO55eUbd++UzaHJo4hxgfKU0ZCEqQiVMYEjCDlCcA74z+Vlo9HIZDKVSoVxCunGoNvreZ5PKIVCAppgAji+4gXIo89HzebbWq2WX10tlTbvV+49evhgf7/+5eux51/BSVzAIjXN+K/ZrFqtKoqSy+bW1m4Wi8V6/fW37yfwx1NXjEmMsZBKXYN/z2YvXr4qlW5tbz8pFArr6+uO687m86RVgPCwwK7BuBImDcedbs8Pwh8np/l8PrOysrPzdDIdJ6A8NoFloQWcuE0kZGOoNb+42H22u6QsbWxslsu33x8e4i5wbyghMfIPPJlOx5MJFENxzDnHTqt1cNB6Z5rDoW3/VNtoKmlN0KRzGTmHc8UwTdt1HNdxPQ8OgzBERCNIGgUBlv5olHyWsmwbyZbjoC7ylXano7bbWr+faqDrqtrWtL6qqvi3hmF0ut3+YAAhDTvnmpbugFf8kT+0LB3vQNexNk0TCYaBji1koC8IC5xxrvXxCTQmKAFEGWJYtmVZOBbZ6A0NY8A8TQfsUfkaKWVwMQoxxcsOA0L+AqOV12Hh2Y89AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.4.16\"\n        title=\"\"\n        src=\"/CR4-DL/static/41370dc3932b2e42dba1f5dc096e075b/715a3/figure6-4-16.png\"\n        srcset=\"/CR4-DL/static/41370dc3932b2e42dba1f5dc096e075b/63868/figure6-4-16.png 250w,\n/CR4-DL/static/41370dc3932b2e42dba1f5dc096e075b/0b533/figure6-4-16.png 500w,\n/CR4-DL/static/41370dc3932b2e42dba1f5dc096e075b/715a3/figure6-4-16.png 830w\"\n        sizes=\"(max-width: 830px) 100vw, 830px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Figural scission is interesting because there’s no sensory information that requires the uniform connected region to be split, which results in ambiguity in how to complete the figures and which figure is in front and which is behind.</li>\n<li>After starting at an ambiguous figural scission figure, the depth relations of the two parts spontaneously reverse.</li>\n<li>Many phenomena of perceptual organization can be understood within a general framework of non-accidentalness.</li>\n<li>Non-accidentalness hypothesis: that the visual system avoids interpreting structural regularities from unlikely accidents of viewing.</li>\n<li>E.g. In grouping, it could be a coincidence that two regions that move in the same direction at the same rate are two different objects, but it’s more likely that both regions move so because they’re part of a single object.</li>\n<li>The visual system prefers the nonaccidental interpretation; that coincidences are rare in nature.</li>\n<li>The perceptually preferred interpretation requires far fewer accidents of alignment than the alternatives and is the most likely given the particular stimulus pattern on the retina.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 826px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/c899a9ee48cd9e356e2d7f0000772d56/6a6e9/figure6-4-19.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.60000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAADX0lEQVR42j1TW28UNxT2ryJBfUCQtH1Zkt2AhIpaLtksAcSiCInrCxUVqaryUthkk53dudqeqy8znsuGEH4bnzdSPTMe+/ic79w+k6YxdV01TVWZUiolpJRKSq2yPFtbu7I3fGjqSmmpSiXwKHn5SiWEFGRlXGMuK50LWUCmdFbIJC8ODl7cvXsvjpOysrhphnOptLq0x5q0bd22DUZlKqnLytR4pNbaGFVWjCfA1mVZSBlnWQZspbBdvZrkeV4UeZalQMI+z7M0TXgcRxGDqzTNsWA8DhmDMYvTiHHKYoD6QUQQZlmWXdcdHh4Odx+EUVhIleYF9HiSJlnG4wSJ+CHt9wfj8XO4KlbRR5QSWCLmi4tvr16+Xl//6ePf/5i2k7pCzjBGmgi0bloWJ9vbg53BLcZZaUxV1zxOiTFmueyWZx3wECSkDWrQtrBhHIIcFULaTdfaTS7qtkH9TLMyfry/v/vwPpwPd4d//P5bXZunT55u3eyFYYjjvBCDfn88fial2N7aGu3tJUny8+bG/qMhpZw8Hx/0etsQvX3z7tq165PJl78+fLxxfWM8PhBSw//t23ce3LtfN/Wj0ePNG5unp6evX725tbODmSCGJE3brkGdj6fHKPjZ13NjGlRYCEsZ+KeMI5H5YjGZHJVVtTz7mmbC9QKCjnbL7uLi+6A/+PWXjaZtQAVjGVfldhRK67rt0PP1tauj0QglQS6QAJ2AaqhZt1z+++lTr7f13+dJVTfWo5QwzQpoFqCE0OX7Pz/0+zszZw4mAcu2Cp5RXdiffzufOW4KZuoVgbS2fgWoJaFtOSvk5HhaCODgIhhKGUHLQTgwFaqMMeQPVqR4ktjeB6UgBKXCiDtzPwgpDhEw5iCMCE7ASAQYxxwW4A8onBeWpMDFKATIm3HO0yyLk5hSitZgG9GIHE2OptOTMAjn83m0Go7j+IHv+R7KixGEoe8HHobvOXNnNpu5rud63vTkhCAoiBEHIEGMIAj8APohtpRRrAGKm8E4tyhBgDpB4vtA98nlD/qQYo0Z24XrLtwFjIGCtev5+GaO41r/OPX8MMKWwBVcQEY5AwZU4QRh43phwKW7ihguI6j5Vg5jKwgDgujwszHjFDCMgR5o1uq1PQNJ/l9I2wBwFs0BceUP0Tjx89i8vJ4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.4.19\"\n        title=\"\"\n        src=\"/CR4-DL/static/c899a9ee48cd9e356e2d7f0000772d56/6a6e9/figure6-4-19.png\"\n        srcset=\"/CR4-DL/static/c899a9ee48cd9e356e2d7f0000772d56/63868/figure6-4-19.png 250w,\n/CR4-DL/static/c899a9ee48cd9e356e2d7f0000772d56/0b533/figure6-4-19.png 500w,\n/CR4-DL/static/c899a9ee48cd9e356e2d7f0000772d56/6a6e9/figure6-4-19.png 826w\"\n        sizes=\"(max-width: 826px) 100vw, 826px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><strong>Section 6.5: Multistability</strong></p>\n<ul>\n<li>Multistable perceptions: perceptions that spontaneously alternate between two or more different interpretations.</li>\n<li>Why does multistability occur?</li>\n<li>The most widely accepted theory is the neural fatigue hypothesis.</li>\n<li>Neural fatigue hypothesis: the perception of ambiguous figures alternates because different sets of neurons become tired of firing after a long time.</li>\n<li>Assumptions of the hypothesis\n<ul>\n<li>Different interpretations are represented by different patterns of neural activity.</li>\n<li>Perception corresponds to whichever pattern is most active in the brain.</li>\n<li>Neural fatigue causes different patterns of activation to dominate at different times.</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 558px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/f12866005db76b348ea248eddfd1d7d6/42a8d/figure6-5-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 145.20000000000002%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAdCAIAAAAl5NuSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEE0lEQVR42lWU6XrjNgxF9dbTTjt2Jt7lXbuofd9lJfOCPZSSH1UYfbSIC1xcAFSEEPf7w7Ksj8+Pj4/p9Rr7vuv6th+6uq7quqyqsqorPg5DP4w9NkkSO46TJoniOK5p2qYpwRimaVrXDX++H0RRXJZlURZxnORF0Q9D27ZN25iWdblcLMtU4jh1XRHjzHXDMOqHHsz98UiStChL3ngcxtHz/CAM2bAej6crBN+V8fXi9+mk/lqtYPHUtB8//tpudxNJfH5OHxMupmm6XK7b7V7TDF03Vr9Wtm1zpGRZfjqqP3/+u9nuHs/n+u1tv9+XdfWaJpx+/vlj2fbhcNjt9qp63my2+CXHJE2HcVDarrveblmeB0Fo2w70+NKPA2BI8S6KAp55XoRRRGp+ECBd3TQSDL2yrDgjLNniRYpv20s6y7uqGwQzdIN3mmaapgtPjK9RIQw/dvvDnrU7bN637+/v7NuuBQmacj1JZ73WdOPx1H7+/c96/eYHPtIqcZIcTyrRbMc5Hk8cYF019SLykjN5Ihh6bzYbbJCJsBJMVhyTD9pC+3a7w5y00yyjcthRAlW9wJY8VVX1fB9G3dDRRbJUWBCnKMoojikMx4J/P4jjmBag1LhDSxRm4Rep2r5r+1ZGhnnX9zQQSnLWtG1RVnO3ZShEFviFGpb46Xpi9l9gIsNwAWONhVwf06JzLxu6R/V0jknk/4HnSpZEW4YADABgskmkKrJebGCH/kmWdp3MVoK7RiGIJEatv8ELksrPH5s8z5uuIWZRlRI8gJZhZ/A0IaO0lLMnu5IU6PvD8Ug6Qnir1Rpk3TZBFEp14Ny186q/BgOdkRQYDUdJ1fNZ0zR646k9ZcyyaPrWME28S0UlsvkCE03W03XHeRgQVoqXZVESy4apSKdkVOgwwkKzkVPN1M8juaynpnNAZGBkez6f6Tnheyf1BGHX85jnb3BTSZ/f8zzXMF1G/HK9nk4nErFs6/fv9+v1yncv8DlCKjjP+JoFeFymJy9K6gxzxp0rDQlu9xsbtAHghwGatTN4oS1zfk2AhwUfRRGcSZj8uYmynH2PWsLzlsaE8IL8ok1rjbKTaBwux5FswaMGpnlZpFma5pmM2XWwXdTiekREKRh3AEEQw7IdepnupcMJJeSVF4Rx5HoyYZQjc8wg5QrXtq04iRQsSAzCXODpjPR4fLQNbMfGTjcNx2UupWqc8iUIfI7wrjB6uq6D5PalDRg9RMaCmQyj0DAN+dGxqZam69zYQroWEixcJQxDdjxJmsAIJHvAwOI04Ye0hrbnmaYpJHOYsjWCIFCgRFaw8iQZx5RhhTlHw5Fu6DNe6IZ8APPWdY22JTWFAwBCJhne7nfei56soq7K78VIMTwsypEXOVXI8+w/HadDpy5GAL8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.5.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/f12866005db76b348ea248eddfd1d7d6/42a8d/figure6-5-1.png\"\n        srcset=\"/CR4-DL/static/f12866005db76b348ea248eddfd1d7d6/63868/figure6-5-1.png 250w,\n/CR4-DL/static/f12866005db76b348ea248eddfd1d7d6/0b533/figure6-5-1.png 500w,\n/CR4-DL/static/f12866005db76b348ea248eddfd1d7d6/42a8d/figure6-5-1.png 558w\"\n        sizes=\"(max-width: 558px) 100vw, 558px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Neural fatigue may occur because neurons have depleted their biochemical resources for continued firing.</li>\n<li>The bistability of the network is due to its architecture.</li>\n<li>E.g. Some connections produce competition (mutual inhibition) whereas other connections produce cooperation (mutual excitation).</li>\n<li>The mutual exclusivity in the behavior of the network plus the mechanism of neural fatigue mimics our experience of multistable perceptions.</li>\n<li>To test the neural fatigue hypothesis, we have subjects view an unambiguous version of the Necker cube for some time and then have them view the Necker cube. The subjects should tend to see the other interpretation in the ambiguous version, which is what we find.</li>\n<li>Another prediction that’s been supported by experimental evidence is that the rate of alternation between the two interpretations should accelerate over time.</li>\n<li>One challenge to the neural fatigue hypothesis is that alternations are caused by eye movements to different fixation points.</li>\n<li>E.g. The fixated vertex tends to be perceived as nearer.</li>\n<li>But afterimages and even stabilized images of the Necker cube alternate, so this can’t be the full explanation.</li>\n<li>The explanation might be reversed as eye movements are the result of perceptual reversals rather than their cause.</li>\n<li>Local information around the position of the fixation point does exert strong influence on what’s perceived though.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 826px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/9766bbe7fc89df87097269592dca83b9/6a6e9/figure6-5-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAABsklEQVR42l2S6XaCMBSEef9/1eOpypJAFkC2sGjbt+uXgGg75sRwmcnchWhZcd+wPFGW5fl8uVyuUkjn3PbiHtZTEe3sJSgJdV2XxEnTNFVVFUUhMjEMwxvtxYzm5Q8INW1nS2Sqrmsp5efnWQg5juOWFn8v8TxP8wZ/WJa264wtjbVumtDcQNNgfn88NuU/8b4Qd32fZlld3x5fX0TIv21bh/O7OKxo93wXk2eWZtSMrLQlXVjF73rvPI4+t3fAK6vqcDh8fBxomDH2co3/O4e8EbsAr3Grepo4UDnF4ky1OM/PcWzwKS7RtGPNfJk50uQ4Thgy+62uv3++9w/hHpx9fTjXddV1bd/3TWhpPwzYrv0nCa6fQiITP+c8ofeUwfkso9PpdDwepRRxHNMbrY1Smm7TMK21tTbPKVxZW6pCceTRAK2hRWQIqQhRmRfax7XSKpCsgmKM0lrK3DzBjT6uvVjE12uaJDwQVmHP8zxNU3h84bixJ0mCYHcSQnhxFgAVO0gohZTca42/AhuuWNncywjRMH/MOUcwEEORUK3VL5hQbQEvE4KZrSMJY5nX6fwCgEdai9w2axoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.5.3\"\n        title=\"\"\n        src=\"/CR4-DL/static/9766bbe7fc89df87097269592dca83b9/6a6e9/figure6-5-3.png\"\n        srcset=\"/CR4-DL/static/9766bbe7fc89df87097269592dca83b9/63868/figure6-5-3.png 250w,\n/CR4-DL/static/9766bbe7fc89df87097269592dca83b9/0b533/figure6-5-3.png 500w,\n/CR4-DL/static/9766bbe7fc89df87097269592dca83b9/6a6e9/figure6-5-3.png 826w\"\n        sizes=\"(max-width: 826px) 100vw, 826px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Interestingly, when people are shown novel ambiguous figures but aren’t told that they can be perceived differently, about one third of the subjects reported never seeing any reversal until told. After being told, all of them reported seeing frequent reversals.</li>\n<li>These results challenge the idea that neural fatigue alone is sufficient to explain multistability.</li>\n<li>Furthermore, it was found in children that without explicit instructions about the alternative interpretation, three- to four-year-old children never reported any spontaneous reversals.</li>\n</ul>\n<p><strong>Section 6.6: Development of Perceptual Organization</strong></p>\n<ul>\n<li>Is perceptual organization innate or learned?</li>\n<li>Habituation paradigm: the time babies spend looking at an object decreases with repeated viewings, presumably because its novelty wears off.</li>\n<li>When a novel display is presented with the habituated display, young infants spend more time looking at the novel display.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 668px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/87b9c2e0ee16c15ea37424e85a708926/74866/figure6-6-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 119.19999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAIAAAB1KUohAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC/UlEQVR42nWU6XbTMBBG/djQhSQUOJQftIW0SbzLju3YjlfZaRP6cFxJTSic4jNVvOjOfLOo1jAMcpBc/T+XlLvdrum6r9fXbdd1fd+0bd00r806webqtRduyu3W8bzbux9n5xe26xRliQvF4OLoxZIKfkVLSRDeiDi+uJycX55fXF5i6yTppGw0aVbMGsbhhT+Z4qUvotnV1WQ6/TCZza4+x0mqYIID6/W/MLrLqvEC8e79eZJtNnmBSBS1HZl3wIZ/A0Z0PwzjOJLn7e3t09NTr90Bnwwv2BswXFXX63Xiup7vB57rhqFYrexNnuP0RL4F6yA6vtT7umOoTt1LeSL/wKwYdSKluu2qpqGk8m+pWFlVpGrIxuSMkSErHa7qppPD9fW3IAzJttWFqZpWjuPKcT59/jLoyXkp2Dhq0MC6Gdu6OTs7q+saQYiGLKlBJ+cPy5/zOV5MtzADawfDoF/1Il5PphNVZD1rBEcOLlBUmSFhm+mziWlgmjk+Pn6/ubm7u9vv92ZOWalTXpaErY9wfYJVkUfTYfm0389mH+M4PhwOL9Pe9/vDgVZNp9Nfz89G8EvOcnw9HpLs7uf3eZEXRYFgzgxVpvSO4zquW5TbvCiPkRurPx4mYBqUplmSpIyHbTtMBfy2qtZJ6gch/Mp2QhEB00vMGncqWz1XakIGXfrd4yOKPDVgIQBrP6gAfCL/umtN/6wcgWWJ8UdjiIMhj/NcbFmrbJPro6G2pZtNluebokg3rKU1n98vFktGwnYcNIdhGEURsgNeCcHDcrl6WCx4QgfKWXnPr+P5VpapJOP1Wm0VIopjMmQrv7G6Ubz5RBWoGTshV446NRZdUUgUB34QRdwLSF8lG0ax2sfK90AIYooo4r3reYul7fqBlaQpQcwVqoBxqC4RBPiKkiwjhBAxRSYTnEL6QbBcrdCiYDZ5nqezhVUucBSrgAiOlrbNdlUDITjkCNH5B+y2eCZnpTYIuNOwUB7JPcsafZ5bY/pIt536B0ifqqb9Dd8ouyMfXbThAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 6.6.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/87b9c2e0ee16c15ea37424e85a708926/74866/figure6-6-1.png\"\n        srcset=\"/CR4-DL/static/87b9c2e0ee16c15ea37424e85a708926/63868/figure6-6-1.png 250w,\n/CR4-DL/static/87b9c2e0ee16c15ea37424e85a708926/0b533/figure6-6-1.png 500w,\n/CR4-DL/static/87b9c2e0ee16c15ea37424e85a708926/74866/figure6-6-1.png 668w\"\n        sizes=\"(max-width: 668px) 100vw, 668px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>This suggests that infants perceive the common fate principle as early as four months old, but this doesn’t argue in favor of perceptual organization being innate or learned.</li>\n<li>One idea is that infants organize perception around motion and from there learn that objects have other grouping properties.</li>\n<li>E.g. Uniformity, continuous contours, symmetry, etc.</li>\n<li>Organizational processes are present much earlier than we thought but they don’t appear to be innate.</li>\n</ul>\n<h2 id=\"chapter-7-perceiving-object-properties-and-parts\" style=\"position:relative;\"><a href=\"#chapter-7-perceiving-object-properties-and-parts\" aria-label=\"chapter 7 perceiving object properties and parts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 7: Perceiving Object Properties and Parts</h2>\n<ul>\n<li>Important object properties include color, texture, size, position, orientation, shape, motion, and parts.</li>\n<li>These properties are themselves complex entities, having further internal structure.</li>\n<li>We perceive the location of objects not from their position on the retina, but from their position in the world.</li>\n<li>How does the brain know to project location (or any other object property) from the retina to the real world?</li>\n<li>We can also distinguish between object location and the object’s image location on the retina.</li>\n<li>E.g. The cat is far away and is at the upper left quadrant of the visual field.</li>\n<li>The same processes that result in perceptual constancy sometimes produce illusions.</li>\n<li>In the perception of object properties, constancy and illusion are opposite sides of the same perceptual coin.</li>\n<li>Two theorized modes of visual perception\n<ul>\n<li>Proximal mode: reflects the properties of the retinal image.</li>\n<li>Distal mode: reflects the properties of the environmental object.</li>\n</ul>\n</li>\n<li>We can switch between the modes based on intention.</li>\n<li>E.g. If two identical disks are placed at different distances, distal mode reports both disks as the same size while proximal mode reports the closer disk as larger.</li>\n</ul>\n<p><strong>Section 7.1: Size</strong></p>\n<ul>\n<li>Our first constancy problem is how the size of an object is perceived from its retinal image.</li>\n<li>Retinal size can’t be the main determinant of perceived object size because retinal size depends critically on the distance to the object in addition to its physical size.</li>\n<li>The visual system must somehow disentangle the effects of object size and object distance when interpreting retinal size.</li>\n<li>Size-distance relation: the mathematical relation among objective size, retinal size, and distance.</li>\n<li>If the distance to an object is known or guessed, it can be used with the object’s retinal size to determine its objective size.</li>\n<li>E.g. The trigonometry operation “tan” can calculate the objective size given the distance and retinal size.</li>\n<li>Factors that make proximal mode perception more likely\n<ul>\n<li>The difference in retinal size is large.</li>\n<li>The two objects are close together in the retinal image.</li>\n<li>You reduce the information about depth.</li>\n<li>You consciously attend to image-based differences.</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/3bf68ab61f1862852e024822421f67a7/28a89/figure7-1-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 53.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB00lEQVR42lVSa3OaQBTl16Yf7TQzJMOqgA+ImSZq0oeCoMDy0H3xUDoT++N6wH5omWVhz73nPs5d7Xy5VHUthEhoYtndY5rm02IhlWrbC+PsxNji+ZkMyWg0MgzD8z0hpegfrTmfVVnmeSFV6Xn+w+Pjl/v75XIlpLq07fF0guuJc9d1df3BnthJQpmQQingWlnXoOXF8cQ43vX6bTD4/Pb+DZlBZoxzIdIsg+n7j58oCp6ICwr4WveRiqZZmuXYN5vtYDBYr9+B3zLnRVEcj4jl7wJChsjyH5kLSWmW5UWW56Zlgfz6uoRH+6sFDWBZVQmly9VqbJppmoGiygo7yAqV4RAEIQQzCIEqtj1hXIAMKxTZBYHjugYxQN56XlnV/2YWSEsMcnf3CV2BrOu67+8+rh+UUt/3hwT1krE5hglRQOmX0hAGVblP7sSezGYzx3Wms5llWUEYXn9fN9stGplMp3PHmTtz/Hx9eRFdOX3ZdVPXTfN3dFLeZoOoGD6uAOLe5IHq6P9mQj5V9WTOuewh2Y8O2naBeqQ5NzccB1wVmNAgJooFzY+MafvDPkmSQxTtD4c4jqM4hrBxkmC2Vd2J3B3jzgFgFEVhCMdoF4Y0Tf8AE+icsa+ml/IAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 7.1.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/3bf68ab61f1862852e024822421f67a7/00d43/figure7-1-2.png\"\n        srcset=\"/CR4-DL/static/3bf68ab61f1862852e024822421f67a7/63868/figure7-1-2.png 250w,\n/CR4-DL/static/3bf68ab61f1862852e024822421f67a7/0b533/figure7-1-2.png 500w,\n/CR4-DL/static/3bf68ab61f1862852e024822421f67a7/00d43/figure7-1-2.png 1000w,\n/CR4-DL/static/3bf68ab61f1862852e024822421f67a7/aa440/figure7-1-2.png 1500w,\n/CR4-DL/static/3bf68ab61f1862852e024822421f67a7/28a89/figure7-1-2.png 1573w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Most people find that no matter how hard they try, they can’t see the retinal size of objects accurately in the presence of large depth differences.</li>\n<li>Experiments show that size constancy isn’t perfect but is close to true constancy. As depth information becomes weaker, so to does size constancy.</li>\n<li>But distance isn’t the only determining factor of size because cases have been reported where people overestimate size while simultaneously underestimating distance.</li>\n<li>E.g. Uniform texture elements occluded by an object can provide a uniform scale relative to the object’s size, thus allowing it to be perceived.</li>\n<li>Using texture occlusion to determine size is limited to niche conditions that rarely appear in nature.</li>\n<li>It seems likely that the visual system uses whatever information it can get about the distance to an object to compute its intrinsic size.</li>\n<li>The development of size constancy appears innate as infants attend to novel objects that have identical retinal size but different intrinsic size.</li>\n<li>When distance perception is incorrect, size perception should also be incorrect leading to size illusions.</li>\n<li>E.g. The Ames room illusion and the Moon illusion.</li>\n<li>Moon illusion: when the moon is perceived as larger when its low in the sky near the horizon.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 828px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/ea1f32a6e1b0b5d76855c80e496d9df2/8efc2/figure7-1-8.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 76.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAACI0lEQVR42j2Sh27kMAxE/f/J5pCyzV3FTd32Jvd79+RFDiBk0SI5wyGLddtiWkNKPkYsrqtxrqzqy+1W1fU0Lz4m4zxmvfchBsJCeFpBckpbSKtPidOF6HxQeng9vQk9zMZOizE+WB/IdzH+h8GKbX+kdUvrCoJQw8fnV9N2ZdW0vRzmxYZI/jDNnNxdBCAd+dmKbdvXbYcPDK2Pt7KC82Js14sFDuvmoRMThSZjoRYP+03eHzjLYuOamzfWDcNEoZfXt7YTepwVuMDGNObq4Rn2BM/IzgVyhFJS6mGcAtxiquqGbnOr1qthpKNhWqgSf8HJz8jjPJN2u5Wn05+m6dATFWg7bTu0gQrbRsN6nBCSt3SAZ2Q+tIdPFWC57I/vtG0gEy2UlmrohOqVVnp8f//8Ol/ePz4pnZHpou160oTUWg9KZcN9eTl9na+Idy/r6+1+vlw5+Xm9lfd71QsJo+Lx/UPDcK7rhueyrK3Ly4DSWdJDGMbLqF3ekMzs++fvuu8oXzBCopEGQYxx82KYPktG8mJc3hDrOcfZLEfY8d8yDsQrLpfr+XylbXajl1IqXTctDbdtj3+vas666cq6haqQkigiiedSCKmU1qjNBZ+ITkieye9/Q9u+rxg6hfXY95IYnnALIXIEM8ARQjUA6YGETIRCvajbtmKAQpDd5wIDBhfIFXj8knkkmvgnOJyPE0xR1e0d0duWosgRQl4h472x7h97S0lphm8WcwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 7.1.8\"\n        title=\"\"\n        src=\"/CR4-DL/static/ea1f32a6e1b0b5d76855c80e496d9df2/8efc2/figure7-1-8.png\"\n        srcset=\"/CR4-DL/static/ea1f32a6e1b0b5d76855c80e496d9df2/63868/figure7-1-8.png 250w,\n/CR4-DL/static/ea1f32a6e1b0b5d76855c80e496d9df2/0b533/figure7-1-8.png 500w,\n/CR4-DL/static/ea1f32a6e1b0b5d76855c80e496d9df2/8efc2/figure7-1-8.png 828w\"\n        sizes=\"(max-width: 828px) 100vw, 828px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>When the moon is near the horizon, it’s seen in the context of the ground extending into the distance. When the moon is directly overhead, it’s seen in the context of stars and empty space.</li>\n<li>Emmert’s law: the perceived size of a constant retinal image is proportional to its perceived distance.</li>\n<li>Some size illusions may be due to depth processing that occurs automatically and unconsciously but that disagree with the conscious interpretation of depth.</li>\n<li>E.g. For the moon illusion, perhaps the moon near the horizon is unconsciously perceived as farther away and thus smaller. But we consciously know that the moon doesn’t change in size so we perceive it as larger to compensate.</li>\n<li>There are many different theories of why illusions exist, but no theory can fully explain all of them.</li>\n</ul>\n<p><strong>Section 7.2: Shape</strong></p>\n<ul>\n<li>Shape is the most complex of all visual properties because of its possible combinations.</li>\n<li>There is no accepted theory of what shape is or how it’s perceived.</li>\n<li>Shape constancy: perceiving the same object as having the same shape when observed from different viewpoints.</li>\n<li>No notes on shape constancy for 2D figures.</li>\n<li>Surprisingly, observers have poor shape constancy for 3D wire objects.</li>\n<li>Why do people have poor shape constancy for experimental objects but not for everyday objects?</li>\n<li>One possible reason is that we perceive the everyday object from different viewpoints by continuously moving from one view to another.</li>\n<li>Under these conditions, the perspective changes from moment-to-moment are gradual and easily perceived.</li>\n<li>Another possibility is that shape constancy occurs because we’re able to recognize the same object from different perspectives using different features.</li>\n<li>E.g. You recognize a cat from in front and behind because you know it’s a cat.</li>\n<li>If an ellipse is viewed monocularly or from far enough away, observers generally see it as a circle slanted in depth.</li>\n<li>The Ames room is a compelling example of the tendency to perceive the shapes of objects (such as the room) using simple, regular, and frequently encountered alternatives.</li>\n</ul>\n<p><strong>Section 7.3: Orientation</strong></p>\n<ul>\n<li>Despite changes in viewing conditions, orientation remains constant with respect to the environment.</li>\n<li>E.g. Lines and edges aligned with gravity are usually perceived as vertical and those aligned to the horizon are perceived as horizontal.</li>\n<li>Orientation constancy: the tendency to perceive the gravitational orientation of objects as constant despite changes in head orientation.</li>\n<li>The visual system seems to determine the orientation of objects by taking both head orientation and the object’s retinal orientation into account.</li>\n<li>Head orientation can be obtained from the proprioceptive system while retinal orientation can be obtained from line and edge detectors.</li>\n<li>The vestibular system provides information about head orientation with respect to gravity and with respect to changes in head orientation.</li>\n<li>Vestibular information comes from the three inner ear organs: utricle, saccule, and semicircular canals.</li>\n<li>It also comes from your body, specifically the pressure exerted on your skin where your body is supported against the force of gravity.</li>\n<li>Sensations of skin pressure are a source of gravity information independent of the vestibular system.</li>\n<li>Frame of reference: a set of standards that objects are perceived in.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 833px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/a8487666f6d522203974fc321fa08733/5205c/figure7-3-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 101.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAADUklEQVR42j1T21LaUBTNW31EwQuEBEK4BeRi+1QfnIKlWMcHqzIKD36C4x84VWfqVFHHcgnkQgQSE0CIf9h1DrUnyT455+y119p7J0yz1by5+XV7+/vu7vb+vv7wcP/4+ID3y8ufV1eXFxcX19dXT0+PzWYDd6vVbLVa7XZLltv9fp85OPix6PGwAQw/HpZluWAwxPOhUCiXzWYzGUEQwuEwbEQQxIgQiQiiKAZZ9vT0lIIXPRwXDPj9bMDPsgEE8vvXYrFoOpWCayaT4XkeWzS+n7ixrMfjOT4+Zg4PD73eJbAhGB68cBwXDofSgEqpqCjm83lwY5vHBZIgi2nZ56vVaszR0ZF3aYmnu3MwuGOx2MdPGzzPQbaUlJAQF+S5IDmELkw+r7dWq76Def4dTETgNRqNbtBBtunz74AWxQtw9R+zF2v/2to7mLoHSPokF5rO/xvlADkBQ3alcoRqI7eUJBFVbGAOhkHO8Xgs8F7FuWApmUQ6i55FCgazdymXyybiCbQG5aWubFSM5rI54NGueaqQlkwkcK+vp4GvVmnOqFY+l4vHYslEMp1KozwoATxwo9tIG9WGWrQbDUjE42IkghDVkxPm5ORYkpIojyhGwI+qQzpU5HN5ISygX+gZSzuMygMDpkxmHUsAmf39fXwkiAc/HKAH6OGybxnfE9KTJGl1dWVlGXs+TFExgirg+1lY+FCpVJjz8/NCoVAul/f29kqlUrm8U6ajWCzu7Ozsft/dLm5/xbW9Dbetra1vpVLhS2Hz8+bZ2Rkznkyc0WgwHHa6XUVRR+Px63Q6nbnWi61pWrvdNi3Ldd3pbDZzXcdxeoahqupkMsEmA29nNLadESyiYIkDHHfoIHi5LcuyaQ7f3tzp9HU2m86t684Y47n/3O8PTQvkLy+29QJjW5Y1HA5N07Qd7FimZY7IcMbjMZnHIxAgBFOv39/V64qq/mk0FFXT9B5Rr6qwXQxFJUZRjGcDVtd1RVX0nt7r9fr9Z6aJf7std+Cl6XKngwMkD51Gz4BmhYYYDAcEbxjAIJfBYIAXXdcYADsUCkuS63RUilFxEUqYLghJoC5hRiysIA14BqTgbkCALFMRMlCY4Q0/TdcAhisUodS2bdOakNlx7L+w05iG3QuDcwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 7.3.4\"\n        title=\"\"\n        src=\"/CR4-DL/static/a8487666f6d522203974fc321fa08733/5205c/figure7-3-4.png\"\n        srcset=\"/CR4-DL/static/a8487666f6d522203974fc321fa08733/63868/figure7-3-4.png 250w,\n/CR4-DL/static/a8487666f6d522203974fc321fa08733/0b533/figure7-3-4.png 500w,\n/CR4-DL/static/a8487666f6d522203974fc321fa08733/5205c/figure7-3-4.png 833w\"\n        sizes=\"(max-width: 833px) 100vw, 833px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><strong>Section 7.4: Position</strong></p>\n<ul>\n<li>Object position can be specified relative to the observer’s body (egocentric position) and relative to other objects in the environment.</li>\n<li>Egocentric position is specified using polar coordinates.</li>\n<li>E.g. The radial direction from observer to object and the distance from observer to object.</li>\n<li>Perception of an object’s radial direction starts with information about the position of its image on the retina.</li>\n<li>The direction you perceive an object to be located isn’t affected by the direction the eyes are pointed.</li>\n<li>This strongly implies that your visual system determines the radial direction of an object by integrating eye direction and retinal position.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 830px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/b1353e1a640a3e466b42b0dec8eebe7a/715a3/figure7-4-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 93.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsTAAALEwEAmpwYAAACSElEQVR42m2TiW4iMRBE54ezH5FsIiUQBQJzee7TJwOfuM9jQoiyUsnygKuru1yOZiVvkFopo5XWUqmirHb7/eF4jJNUSmmcs6eTO53CCk7LEgXOrJRcOV3fN23L2g0Daz+Om832GMfDOI7TpI2B4774kedorYwZpqms63Geu2GEjX7bdUVZPjz82R8OdLH/PFBulnI5nynhlQMZTlXX2lrtXNv3cZLwOc2SKmmeU4i/OAYBfRoM/CjIcmiSMk7Tx6fnj90+SVJRlLv95+vbBnIuCutcUDPO9uPwg8yEjJcXRVXV7uSatkvSHK+k0W/b7cduZ6xlyPPlwtjDMLL5bhvUbYsCrfre5lmUZT+Ms1Te5y9Z7Xvsbp9XZfx4en7evr+jXFZ1VTevm83fl5c0y/mXibgUb34/rMyzuyfjM00ekwRBbGcEGsHqGqF+CPdE50HwhkgaA/SaAdqmynK5+MLnM7+EUbEnOBSycUcOM6+9YUyWC5hcDO3dwnCL1C/la0h02w/IZkJgktd0LsD+LHGPK3lexcumwRVa/Wb+0v9BJtWByYos5Jvsfzu/HyHykbRWWTsp5SHl9eksi103/tx5WRZvWHCOhIRNdIhjTOZKiqruxrFqGl7VMM0knMxybeGqWFjDBvA8QPT+QZY/MTlNM1EUGUkuCkA8KMpVA1EWQnjwWpI0reumbjwi5uQM2eDZ52ItUXGafNQEneNiDTztrAQiLMgNnxSOeDRJlnkp/3pEuipzSBtL8gCOmBXWu+g32Igvyuh/wVXcgIsOMXUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 7.4.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/b1353e1a640a3e466b42b0dec8eebe7a/715a3/figure7-4-1.png\"\n        srcset=\"/CR4-DL/static/b1353e1a640a3e466b42b0dec8eebe7a/63868/figure7-4-1.png 250w,\n/CR4-DL/static/b1353e1a640a3e466b42b0dec8eebe7a/0b533/figure7-4-1.png 500w,\n/CR4-DL/static/b1353e1a640a3e466b42b0dec8eebe7a/715a3/figure7-4-1.png 830w\"\n        sizes=\"(max-width: 830px) 100vw, 830px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Egocentric position constancy: the ability of the visual system to perceive stationary objects as stationary in spite of eye and head movements.</li>\n<li>An object is stationary if the eye movement and retinal displacement exactly cancel each other.</li>\n<li>E.g. Eye movement + Image displacement = Object displacement.</li>\n<li>E.g. If you move your eyes 5 degrees to the left, a stationary object’s image will displace 5 degrees to the right. If the object isn’t stationary, the image will displace not 5 degrees.</li>\n<li>Where does the information about eye displacement come from?</li>\n<li>Two sources of eye displacement\n<ul>\n<li>Afferent theory: receptors in eye muscles get information about muscle tension, which is sent to the brain to provide feedback about the position of the eye.</li>\n<li>Efferent copy theory: whenever the brain sends an outgoing (efferent) command to move the eye, it also sends a copy of that command to the brain regions that compute the new positions of environmental objects.</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/15d455894cdde4a0bc72ecc23c5b1b03/ab40b/figure7-4-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.199999999999996%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAABFUlEQVR42iWQ6bKDIAyFff+ft9YWAVE22cGlfb4evZkzwhi+nCRd2xB7rY2Q8fHoGWPG2jeCEG0sIaTve6W0MWZ4vShjMeVcW8ol5NwBbtsO+RBX5zjn40ittVIqpJzzUsr5DsY5ysVcUykxZ59SV294P85c6puMzjk4T9MEk8/3i9eUsuM40RqcxSJLbfG2veHWAG/7gebBw3YYXqXUlDKY62lKf48ejYSYgCWkSoVzSLm7sP24rM9PuWEM/0+iodo2pVT/fFLGjV1R/RoYcCkX7HxASecCDmhdvfcBw8eYMDMuSmt80b/URlur7QpJY/Cnw4QGCaWxUcgip69t4VprxbYWqeZFinmBuJiZEJRzOnEmxA9xUhHu5WQW+QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 7.4.4\"\n        title=\"\"\n        src=\"/CR4-DL/static/15d455894cdde4a0bc72ecc23c5b1b03/00d43/figure7-4-4.png\"\n        srcset=\"/CR4-DL/static/15d455894cdde4a0bc72ecc23c5b1b03/63868/figure7-4-4.png 250w,\n/CR4-DL/static/15d455894cdde4a0bc72ecc23c5b1b03/0b533/figure7-4-4.png 500w,\n/CR4-DL/static/15d455894cdde4a0bc72ecc23c5b1b03/00d43/figure7-4-4.png 1000w,\n/CR4-DL/static/15d455894cdde4a0bc72ecc23c5b1b03/aa440/figure7-4-4.png 1500w,\n/CR4-DL/static/15d455894cdde4a0bc72ecc23c5b1b03/ab40b/figure7-4-4.png 1736w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>One experiment to confirm which source or theory the brain uses is to move the eye by an external force.</li>\n<li>E.g. Someone else moving your eye or using your finger to move your eye.</li>\n<li>If the eye is moved externally, object constancy either remains (afferent theory) or breaks (efferent theory).</li>\n<li>You can perform the experiment yourself by closing one eye and moving your other eye gently with your finger. You’ll find that the world wiggles noticeably indicating that position constancy has failed.</li>\n<li>More rigorous experiments have found the same result which supports the efferent copy theory.</li>\n<li>Position constancy during eye movements is maintained by the brain sending a copy of the efferent eye movement command to the center where image displacements are corrected for eye displacements.</li>\n<li>We see that in many cases, the global properties of objects remain constant under many, but not all, viewing conditions.</li>\n<li>E.g. Size, shape, orientation, and position.</li>\n<li>The same mechanisms that normally lead to accurate perception cause substantial illusions.</li>\n<li>This is because the mechanisms underlying perceptual constancy are heuristic; being based on assumptions that are usually, but not always, true.</li>\n<li>When the assumptions are false, we experience an illusion.</li>\n</ul>\n<p><strong>Section 7.5: Perceptual Adaptation</strong></p>\n<ul>\n<li>The visual system can use complementary information that changes over viewing conditions to recover intrinsic object properties.</li>\n<li>E.g. Combining distance information with retinal size to get true size. Combining head orientation with retinal orientation to get true orientation.</li>\n<li>Perceptual adaptation: semi-permanent changes in perception that reduce sensory discrepancies caused by stimulus transformation.</li>\n<li>Hallmarks of perceptual adaptation include overcompensation when perception is changed followed by a reduction in motor errors (assuming feedback).</li>\n<li>E.g. Visual transformations include inversion, rotation, enlargement, shifts, and changes in curvature.</li>\n<li>Findings show that the degree of adaptation and its time course vary substantially for different transformations.</li>\n<li>E.g. Adaptation depends on the type of activities performed as active observers adapted more fully to the transforming effects of distortion goggles than passive observers.</li>\n</ul>\n<p><strong>Section 7.6: Parts</strong></p>\n<ul>\n<li>Part: a restricted section of an object that has semi-autonomous, object-like status in visual perception.</li>\n<li>In addition to parts, object perception include the relations between parts.</li>\n<li>The structure of language seems to reflect the underlying structure of perception.</li>\n<li>E.g. Words like “leg” and “seat” reflect the different parts of a chair.</li>\n<li>However, the link between language and perception might also be flipped as we might structure our perception around our language.</li>\n<li>Evidence supports that language is structured around perception as the name for parts is common across different languages and that people spontaneously perceive part structure in novel objects.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 814px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/536dfa3fdd02d7a12f925b16506c43be/a4262/figure7-6-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 90.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsTAAALEwEAmpwYAAADhUlEQVR42i1TC1MaVxjlhyQKphpJq9Y2AZLWiNiZ0o6hozyqGXktWJZipcogCmooFhOx45ooCgssD9lddheWV0z+Xs6u/eabu/fu3HO/xzmfhuN5llOc51sc1pbAcazNtvSbzeb1eH0+H0VdyLIsSWK7LSne6XTkjqy6hgOm1eIFAbCWKPYHfYANz575fb50+s3u7u75OdUfDIBpd9qqY6OCu7IGSA7RWi1BlCpMdW9v/4K6MBmNLqczkUiEQqHl5eVcLie1JYQUYZKEBGAKmBdExGxyvCC1r67yRoPRYXcS/kD6TTqVOrAsLBKEPxrdOj4+rjfqAPT7/W6v1+11FXCFYYo0jbSHd3elcsVkMD03mRYtPwWIQDhE2lfs4XB4Y2MjGAiEyXA0Gj3OZLLZf2m6OBgMNB6PV6/X2+0r2Wz24ODw11+WrFbrDy9eLFosr9fWPIp5/X6/e93tdnuCwY2jw6P1dTcuD4dDDUlGpr6Zem4yrq29drl+X4CZzfMv55eWXgWDwWQyRYZIZJHJZCJ/RhD50+fPJBnOnZ0NhgMNGnV9U6DpstzrFYpFw1PDy7n5BfOC9Wfr5uYmRVEkSRIEQZ1TSDsSiXy8+4hCzs7+UyKDnk63Kyncdcvl8uTjidlvvzebLbZXNmSLe4gfCASwIu3t7W1oIRgIIu1ev4dugyeB5VugAp0bHR15+ODh5MTjuR/nVldXHXbHyvKK36e8giPS3t/fT+wljo4OK5WKSpXqCF6gS2M6nU6rezQ2Nv5oHLAn+kntiDaZSjVZzu32bkb+Qi3JZHI3Hj95+/Z/MCpvy3KhWELYkQcjOq12/KsJp9P5tf6JdlT7Tzrd6w8IIvju3Wm1Ws3n84UizXKckrbqotjuVGt1kDQ9NfXd7NOZ6VmXywUWZqZnTk5O0Fufj0CfQG+330PB0Mm9PKFtkRMEQZJumyx6Xm80wLnD4QArodAfsVgMpfp9xOlpDrK+1yharBHUL7SpOH5C+7I8GA7r9frW1t/xeDym2s5ObGc7hncxJMBDm0rk95eX+ZubUpm5KdK1xm2jccswVaZaw5CgEbfNJstyzSa0zytTy3Msy+KIDcdzmg+XV+fUBfX+A7qVv8YroIChaeyvS+UyyCuV4CXEpKGkSqVYLDKMulYZDW4ztRrLKSOJ2UbjMH2CWlZLBAeSUhbGWNmI9yuOoqRc+wI+qC5NmZ1fSwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 7.6.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/536dfa3fdd02d7a12f925b16506c43be/a4262/figure7-6-1.png\"\n        srcset=\"/CR4-DL/static/536dfa3fdd02d7a12f925b16506c43be/63868/figure7-6-1.png 250w,\n/CR4-DL/static/536dfa3fdd02d7a12f925b16506c43be/0b533/figure7-6-1.png 500w,\n/CR4-DL/static/536dfa3fdd02d7a12f925b16506c43be/a4262/figure7-6-1.png 814w\"\n        sizes=\"(max-width: 814px) 100vw, 814px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>This ability to automatically perceive object parts can only be explained if perceiving parts is an innate function of perception.</li>\n<li>How does the visual system determine the parts of an object?</li>\n<li>Parsing: the process of dividing an object into parts.</li>\n<li>Two methods of parsing\n<ul>\n<li>Shape primitives\n<ul>\n<li>Assumes that a set of simple, indivisible shapes are the most basic parts.</li>\n<li>Complex objects are analyzed as configurations of these primitive parts.</li>\n<li>However, it isn’t obvious what are the primitive units for 3D objects.</li>\n<li>If we have a set of shape primitives and some way of detecting them in images, then complex 3D objects can be segmented into parts made of these primitives.</li>\n<li>E.g. A set of cylinders can be used to analyze everyday objects.</li>\n<li>Problems arise when we consider complex part/whole hierarchies and that parts switch wholeness depending on context.</li>\n<li>The most challenging problem for a primitive-based theory of part segmentation is the most obvious one: what set of primitives capture the huge diversity of shapes that we perceive?</li>\n</ul>\n</li>\n<li>Boundary rules\n<ul>\n<li>Assumes that complex objects are divided along boundaries into parts that aren’t specified beforehand.</li>\n<li>Boundaries are primary and parts are secondary.</li>\n<li>One theory applies the fact that when one object is inserted into another, they meet to form concave discontinuities.</li>\n<li>E.g. Sticking a finger in a ball of clay or pushing a candle into a birthday cake.</li>\n<li>Concave discontinuity rule: the visual system divides objects into parts where they have abrupt changes in surface orientation toward the interior of the object.</li>\n<li>This rule doesn’t really apply to most cases where people perceive parts.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Are parts perceived before wholes or are wholes perceived before parts?</li>\n<li>From a physiological perspective, tracing the path of neural information processing leads to receptive fields that become larger and more responsive to more complex stimuli.</li>\n<li>So information processing proceeds from local stimulation to global structure, but this doesn’t directly imply that perception proceeds this way.</li>\n<li>E.g. Maybe completion over initialization is more important for perception. And what about feedback loops?</li>\n<li>Does the pattern of firing on the retina being the first neural activity in the visual system imply that this information is the first to be experienced?</li>\n<li>Compelling arguments have been made that we have no direct perceptual experience of retinal information.</li>\n<li>Exactly where conscious experience occurs within the human visual system is an open question, but it’s possible that perceptual experience follows a very different time source from neural information processing.</li>\n<li>Conscious perception might even run in the opposite direction from wholes to parts.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 807px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/8d41b89309ad5b32314ff3510267decc/d2a60/figure7-6-9.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsTAAALEwEAmpwYAAACn0lEQVR42kWTjXKkIBCEff/EzWU3p6KgyAr4L7rJ+92HVuWmtrYAp5nuniFZt+36bWGb5mma52VZvPccsJjP7TzNy7oO4zCO47yuY9xysCQncl23QJZ5Pjtr86LI87zVepqmrrOtNmBYVFJWlTTm2XXd05hfMGUjmDpSyTRNVV2TdxzHOE3W2uPYPz4+87wQQmit9z1wSHoS6a1U3vph8N455+9nSKnCvg/DaK0LexCifDwenDtnQ9gpfoJDLAseNnyIApG0rX/+fEIbla0x3IJyyrD++vraj8OemUkIgWuu+hRn/f6WQjLLcj71/YBIKmdZdr9/Eo3W3OWsuypHSusSYxiin1VVgTyTAlvkvb5fxpi/RJax/a8ZtXXdnNhF46wxWFWWVdNo6xzOYRIk61pxaVmWSsm21VLKy+2VG2kv4fueKyF/vA7AWE3nsDeEDcA4DiQ3tZqXmcN5nmJlSMY+LQuwummQ/fPzjUg04D504E9NwLS1KAomROtmnseENtEhtDnvI1ip97d35uSWpnCxzlMEX+6PR3rG7XaLAnUzjn3CNVgNB/rU977GgKbG+QwtF+22hS0TolvT2Q5GTHHzC0YDfEhFM1LJeL1ekSfj5Ry9jeAi970fpzHLc8B1U5/gGEz7giTeA/iyqpjt6Oe2UpmBX9aNLjCcIIEBVkoNQ58wLvhB4Jw+g2ao+AZKnDembbDQWlUrUQrcKkvRtg0uTtOYkM3d52zZc7ypDcHz18dZpwL/fHLeoch723UgWmufCVODnmsY6jOoKWNUeIftMIzjgZaiyPIMXuzyPINHggamh1wonGvAFQ5LpfCdN8y98SYpyzhgohA5CyGKshIJM8wRSF4/RxeerWBEpcR8kIUoKuC1vHjRS/Okbc9/vHrYfd6PZlsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 7.6.9\"\n        title=\"\"\n        src=\"/CR4-DL/static/8d41b89309ad5b32314ff3510267decc/d2a60/figure7-6-9.png\"\n        srcset=\"/CR4-DL/static/8d41b89309ad5b32314ff3510267decc/63868/figure7-6-9.png 250w,\n/CR4-DL/static/8d41b89309ad5b32314ff3510267decc/0b533/figure7-6-9.png 500w,\n/CR4-DL/static/8d41b89309ad5b32314ff3510267decc/d2a60/figure7-6-9.png 807w\"\n        sizes=\"(max-width: 807px) 100vw, 807px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>One experiment testing congruence between global and local features found that response times are faster for global features, suggesting that perceptual processes start from global processing and then go to fine processing.</li>\n<li>However, other experiments present conflicting results and there’s no consensus.</li>\n<li>Global and local features may be processed in parallel in different size (spatial frequency) channels with some being processed faster than others.</li>\n<li>Lesion studies found that global information is processed more effectively in the right temporal-parietal lobe while local information is processed more effectively in the left temporal-parietal lobe.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 812px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/91f97b2a6a0562c5309d5e1f47dddfba/63ec5/figure7-6-11.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 105.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAIAAADJt1n/AAAACXBIWXMAAAsTAAALEwEAmpwYAAACz0lEQVR42k2U2ZqbMAyFefK2F53JLJksQNhss4MBsyTzhP1tZtryKUQQHekcSY43GlO37flyvlwuQqlpNmY2Uqn343sQBFff5yetu21bmrry/WsURUHgp2nyeGweYGwwRo8jNk7WGabJPg48Ttzm2czLMhE5TWaeJ+7GbPe/YKKJc34PZFlgAb5p27brlnUFAx4HWzd33e/eX5genUNachsTJSk1Va6apt1hlCUR2LZp1x1MaJJlVdOWdR2nGTV5Qxcuvp9m2eXqk4KygHut87yASts0pLNgsyxhFIe3yA/4TgBTPJPy+YXr9XB4UUVBKKaHARYD12iv++PhjcsCQ6IzqaAAuNOaLE/PB0WhouBDZXhSGTmPz0+y8MaB57moait4HPOqAtzrgWxxkn58nD5OpyAIIV9WNVNEO2KN4/zVMPC0ij6jlvtOEr/r+7qx3ebGbGyrv2FfYDvJyc55stP8MsCW6rotmx0Mtk/LNXn7B97H81XZGUEl/I2hIKmRunPBul6TeXPpMI+VYBSHw+s1CMy6ohxJiAQW+CGNzTKx10QFzaubZvnOxYbNqZBXPzydL03fj1bcQrfhovKC+lLlxq1NUZZQpdVw2VfN64fxx89f1yD8/fT88vZmnLZMCLRwPPCFVBRBCAZSa02WnbzHkFIhbm5PZF4MrjITBsyeAavqerbbue762ZHJ7v5iNedl1WptzwMHD0rktmdr2s+Q9f4/TN+zmF0+73g6vx2P7FYQ3m5RlJclKx0niZQKepmQKc9C0KoktS5veOSb9fPgHMYxG0qT2CqahEgLVoogojGW1OYqijiOOWdSkjGt68ojEwciJsgqj4QUYECyoUS7SvbK3Z7jCMFJV3CwYPqE3eIkSpJbHGM80jDHTezgIAzB2KRkVTkvkyStq9pLMwFbaCeEShmnaWz9DIW7SCrSDByZqzC8dV2//w0ZY/4ACUkihySQ1ZAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 7.6.11\"\n        title=\"\"\n        src=\"/CR4-DL/static/91f97b2a6a0562c5309d5e1f47dddfba/63ec5/figure7-6-11.png\"\n        srcset=\"/CR4-DL/static/91f97b2a6a0562c5309d5e1f47dddfba/63868/figure7-6-11.png 250w,\n/CR4-DL/static/91f97b2a6a0562c5309d5e1f47dddfba/0b533/figure7-6-11.png 500w,\n/CR4-DL/static/91f97b2a6a0562c5309d5e1f47dddfba/63ec5/figure7-6-11.png 812w\"\n        sizes=\"(max-width: 812px) 100vw, 812px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Patients with a severed corpus callosum have difficulties in producing interference effects between local and global visual features.</li>\n</ul>\n<h2 id=\"chapter-8-representing-shapes-and-structure\" style=\"position:relative;\"><a href=\"#chapter-8-representing-shapes-and-structure\" aria-label=\"chapter 8 representing shapes and structure permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 8: Representing Shapes and Structure</h2>\n<ul>\n<li>Shape is the most informative visible property in the sense that it allows a perceiver to predict more facts about an object than any other property.</li>\n<li>E.g. The shape of an apple is more informative than the color of an apple.</li>\n<li>Shape is the most significant property we perceive but it’s also the most complex.</li>\n</ul>\n<p><strong>Section 8.1: Shape Equivalence</strong></p>\n<ul>\n<li>Shape equivalence: the problem of perceiving two distinct objects as having the same shape.</li>\n<li>E.g. A real car and a toy car. An upright chair has the same shape as a knocked over chair.</li>\n<li>Equivalence differs from constancy in that constancy is about perceiving the same object despite differences in viewing condition, while equivalence deals with different objects.</li>\n<li>Objective shape: the spatial structure of an object that doesn’t change when spatial transformations are applied to it (except for deformation).</li>\n<li>E.g. Translation, rotation, dilation, and reflection.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 808px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/dec8ddeaf44f4573fbcea1a82b3d40f8/3534c/figure8-1-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 87.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsTAAALEwEAmpwYAAACnUlEQVR42l2T207bQBCG84Q8QQ+oNwXKQUW9KFwgtYXETpqoNILYIT6tDxsfdu21Y5yEtEhtH6z/2gkRrFYrez3fzD+emVZeryzL5vNq0O+rirpaLRWlc319XZYzxhilASEOFnFJnMRRkoQxtnxoNbAQory/Pzs7b3eU+WJxevppMPgxK8uE8WkYWpZlmKZlO8BixnFOJczWcJEXmRDtq85I09NMXFx80fRxMSs5JIkcOxUiFTlLMwknbBonUcJbEtysqkLUBW5+Pz4ulwt4EXkhpN88ywueiYSngCPGGxctUcOwWy4fut2eoqjQT2l49vk8YQyy6TR0XR8ZQy2recDQHDVwI+y+qk5OPqrdXhjFu2/ffft6yWEXM9+jpuUYhhWGcRP8aTdwUcxm0IifZJgWIpsWzrIo5KdGeV7AIOdpxuq9hetvkocFTuR9ddW+ubmpPeYbXqb2EgaWywhi9WulKN12u/Pn77+jw5PXr3alNc+QBXE9zw8YT1/waxgbVT08PFK7SjWv9t7vD38O8dvjhAfBFDlbNmHw9Fz5GoZC3A6Ht4jT7w92dnY+HOxDLKybhHEirIycia1sYHW2MjKXwjIkHkWhWHfeFpatspH9DIbpw2p1edlW1W6apsdHx6gwPKLUnk8b2dD/En7KGY739g56ve/oyLdvdjVNQ82iOAlo6BDPtonvU2QtYb6B0RsQDBiSRyMtCCiU3440DFOjaFvnulqN+PUP03XdsmzbccIQQ8YIIZ4XoCVxojyI5gfUMG3IxoNpO7ZDUDmMGIxaGDeMKhrLdT14ka/EvZsY4/EEduhKJDwx5LVp2qb04ujjO3088Xy/FWJFUUApXuTEY/BlcB9aiId58BCHuD5g+JpMgDtwr+l3AaX/ATg6wZyKXHjlAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 8.1.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/dec8ddeaf44f4573fbcea1a82b3d40f8/3534c/figure8-1-1.png\"\n        srcset=\"/CR4-DL/static/dec8ddeaf44f4573fbcea1a82b3d40f8/63868/figure8-1-1.png 250w,\n/CR4-DL/static/dec8ddeaf44f4573fbcea1a82b3d40f8/0b533/figure8-1-1.png 500w,\n/CR4-DL/static/dec8ddeaf44f4573fbcea1a82b3d40f8/3534c/figure8-1-1.png 808w\"\n        sizes=\"(max-width: 808px) 100vw, 808px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>These transformations provide a way to test whether a pair of objects have the same shape.</li>\n<li>Three hypotheses of shape equivalence\n<ul>\n<li>Invariant features: invariant object features are compared and if identical are perceived to have the same shape.</li>\n<li>Transformational alignment: if the objects can be transformed into each other, they’re perceived as identical.</li>\n<li>Object-centered reference frames: if the objects are the same with respect to their own reference frames, they’re perceived as having an identical shape.</li>\n</ul>\n</li>\n<li>Some features are invariant/unchanged under transformation while others are variant/change.</li>\n<li>E.g. Rotation doesn’t change the relative position of object features unlike translation.</li>\n<li>Perception seems to be dominated by relative relations among properties and parts rather than absolute properties.</li>\n<li>Unfortunately, there’s persuasive evidence that the invariant features approach is flawed as a theory of shape equivalence.</li>\n<li>The damaging evidence is that some transformations aren’t perceived as the same shape even though they are.</li>\n<li>E.g. A square versus a square rotated by 45 degrees. The rotated square is perceived as a diamond and not a tilted square.</li>\n<li>Another way to assess whether two objects are equivalent in shape is to find a transformation that brings one object into alignment with the other.</li>\n<li>The first step is to find matching features (edges, surfaces, texture, anchor points) between the two objects. The second step is to transform the objects so that the matching features align.</li>\n<li>The transformation hypothesis runs into problems of needing to try many different matching features and that the same set of matching features must be visible on both objects.</li>\n<li>The hypothesis also fails on the square/diamond experiment because the diamond can be transformed into the square.</li>\n<li>The third hypothesis, object-centered reference frames, argues that imposing an intrinsic reference frame insulates the shape representation from transformations.</li>\n<li>No notes on the Cartesian coordinate system.</li>\n<li>Each object has its own coordinate system that represents its shape, but the equations representing the shapes are identical across coordinate systems.</li>\n<li>Thus, by choosing the right coordinate system, variations between two different shapes can be eliminated and their equations compared.</li>\n<li>Two types of reference frames\n<ul>\n<li>Viewer-centered: references are chosen relative to the viewer.</li>\n<li>Object-centered: references are chosen relative to the object.</li>\n</ul>\n</li>\n<li>Identical objects where one is transformed will have the same shape description relative to their object-centered frames, assuming the reference frames compensate for transformational differences.</li>\n<li>A weakness of the object-centered reference frame hypothesis is that it only works if the same frame is used for the same shape in all cases.</li>\n<li>The perceptual system may use heuristics based on the intrinsic structure of the object for assigning an object-centered reference frame.</li>\n<li>E.g. Axes of symmetry or elongation are used to establish the reference frame. A flat bottom edge suggests a stable gravitational base.</li>\n<li>When there are objects with multiple symmetry axes, such as squares and diamonds, the ambiguity can lead to shape equivalence errors.</li>\n<li>Heuristics may also involve non-object orientations.</li>\n<li>E.g. Gravitational vertical, orientation of the observer’s body, orientation of nearby objects, and the object’s motion.</li>\n<li>Three assumptions that influence object-centered reference frames\n<ul>\n<li>The object’s intrinsic structure</li>\n<li>The object’s orientation relative to the observer</li>\n<li>The object’s orientation relative to the environment</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 810px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/2cc9ee884ab83e9835751918532f18a8/d7542/figure8-1-7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 89.20000000000002%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAAsTAAALEwEAmpwYAAACO0lEQVR42k2T6ZqqMAxAef9fMyIIyr7J1rIUnNebk3b0Xr5YQ5qTpSnevu+blb4f8ryI43gYht2YddusrKxt12VZFkVRfL+jK61npRDPGPPh9bIgTnfGFdm2ZdvcltKI/ojAzskcB1G7Z3+cJ8AnIsH0us5aP5J0nPWk9KyXWYl4VKjXzZyvNM3CMLp8+3XTvH5+4P8y7wZvKv769qM40euullV4vQqMEw18fV+atkuS1L9cp3nGbrcMPK1ib7tnEEa3KIaER/7gzez+NSjL6pEk9/tDysZuhGTNsiJJ82GcgvBWlBVd2OTA0tW2H0ddN6ENPE7T+XrZhgWmZwT44gdV06K7tGpZ3pnBz5MzA0MB29xBWnIzB52PkyLHm1xnB8tYrOswTOM4EcI17HqmLkZAw0jdtDTMAVOz9GyskwMUY1Aa/cNL5Qx5Xbt+ePY9B/lueJncqATGb9vHae6H8RPLwTLx43j2A/kl0PIffNoHK4EZ2DSLHCfwAfmeBQ3P8Ivcts+BrV7btiONyl5P2VI5j1Lq332UKykehFYLCsWzEs4LgsD3r3mWF0XZth3fBleNUfNaVXUY3rCwF96i+yPBlBdlUZY4ZFnuNU1b2YcSuNoAXE+28eOX8jVB29c8LzMJVLGmmaTz6rqGbIjRtuh4oxUF21IMXCq0KHwYubVHcUx16B5F4Eo2YuNW1Q0FZ/LkVEEdOLnkwK6QVP5zQnlSUll2zycKri4KCmK3Kqxk4VVCWx10mOZhUr+4r8lhocWIZAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 8.1.7\"\n        title=\"\"\n        src=\"/CR4-DL/static/2cc9ee884ab83e9835751918532f18a8/d7542/figure8-1-7.png\"\n        srcset=\"/CR4-DL/static/2cc9ee884ab83e9835751918532f18a8/63868/figure8-1-7.png 250w,\n/CR4-DL/static/2cc9ee884ab83e9835751918532f18a8/0b533/figure8-1-7.png 500w,\n/CR4-DL/static/2cc9ee884ab83e9835751918532f18a8/d7542/figure8-1-7.png 810w\"\n        sizes=\"(max-width: 810px) 100vw, 810px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The goal of selecting a reference frame is to give the observer different sets of geometric relations contained in the structure of the figure.</li>\n<li>Evidence disproves the hypothesis that shape recognition is best when figures are presented and tested in the same orientation.</li>\n<li>E.g. When a new but rotated figure is seen and then recalled, the recalled figure is stored in memory as though it were upright relative to its object-centered reference frame.</li>\n<li>Unfortunately, we don’t have any theory of how these factors of object-centered reference frames interact and combine to form our perception of shape equivalence.</li>\n</ul>\n<p><strong>Section 8.2: Theories of Shape Representation</strong></p>\n<ul>\n<li>Unlike the problem of shape equivalence, shape similarity is a much more difficult problem because similarity is a graded measure versus equivalence’s yes or no.</li>\n<li>There’s no consensus on a solution to the shape representation problem, but we will learn as much as possible from the strengths and weaknesses of the major shape representation theories.</li>\n<li>Templates\n<ul>\n<li>Represents shape as shape.</li>\n<li>E.g. A template of a square is created by associating all receptors that a square stimulates plus the receptors that it doesn’t stimulate.</li>\n<li>Measuring similarity is a pointwise correspondence or degree of fit.</li>\n<li>E.g. Correlation.</li>\n<li>A powerful argument in favor of templates is that they must be used at some point in visual processing to convert spatially structured images into symbolic representations.</li>\n<li>There’s a strong case that they’re used by receptive fields to detect lines and edges, but it also might extend to shapes and objects.</li>\n<li>But templates run into problems because they’re defined for each visual feature and thus difficult to generalize.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 809px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/7ccd02139cc6c6ae1b30688ad0746673/e80ac/figure8-2-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAACaElEQVR42j1Ta2+iQBTlr26b7IcttXE3IIpQrdamTR9qBQYYkOH9fqht/94e6maTYXLnzrnnnDszcMfjoayKNEs+vz7LuskLTHWSJsgkaZRmMUbT1k3beszfbHebzZZSu6zy7tBwQRD8+T30g0CWp5ZNi7pO81yeyIIoDIdDQRAkaVS3LQvCu8W9qs4Xy9Vicb9er8HIJWmqKIqq3oqi9Pzy5ocRtEYjied5URQHgxtREKqmXT08TZW5os7Gk+nibnmrzlzX5YhpLhZLnr9ZrR4DFOZFkuUTeYri8Xh8MxhI0hidrB4eZVlVlFtBlERxPJvNKaXcTtMuflx4LLi8/LnTdeCgDLf8FX/16+r6moeLuu0en15Hojy/W46kCcxPZaUvbruuOxz2HnM9VlQ1ZFGs6YZuGJvNRtM1Qkzk0Q5kR6Px/ephOlXfXtdFUXB10+AkMcdpWpRVVpQYTdcdT6fTx8fxdDwcj2VVlXUdxPH2XVuvt4RYuKC6rrgKU1WHUYR6gPKy6qFVlWYZrgBJAM5b5yDLi7ZrUARgb/usjPnf+I4BzvL8nPm/C4pepiepe2XbplEUU8cNwjBKEuaHuDzmsyhG71kUJ0AjZr6Pqw4ASTPkGWNxHHPPL682dSzL1g3Toi4+izqmZZuW5ew907YJIupg932nI6DOHkqEEI8xDqdqUyz3mkEMAhzdAWhTJJFFgKWOx2D3pAYxSS9DHNeBO852HMAgBWLowAW2UW31lA5IUYnxvtN6R99cAIDEYx5nENLb7o0RSFF3v/tugLouus2KAq0HUcLCCO8PgR/G/U+TJnmR/wVFjvxq84BZWgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 8.2.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/7ccd02139cc6c6ae1b30688ad0746673/e80ac/figure8-2-2.png\"\n        srcset=\"/CR4-DL/static/7ccd02139cc6c6ae1b30688ad0746673/63868/figure8-2-2.png 250w,\n/CR4-DL/static/7ccd02139cc6c6ae1b30688ad0746673/0b533/figure8-2-2.png 500w,\n/CR4-DL/static/7ccd02139cc6c6ae1b30688ad0746673/e80ac/figure8-2-2.png 809w\"\n        sizes=\"(max-width: 809px) 100vw, 809px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>The general problem is that there are many different visual channels through which the contours of a square can be presented.</li>\n<li>One inelegant solution is to create a template for each visual feature. Another solution is to define templates at a more abstract level in the visual system.</li>\n<li>This runs into the second problem where templates aren’t invariant to spatial transformations.</li>\n<li>There are no good solutions to the second problem; either we replicate the template for every possible position, orientation, size, and reflection, or we somehow normalize the shape before comparison with the template.</li>\n<li>A third problem with templates is the inability to represent parts of an object, but this can be addressed by using hierarchical templates.</li>\n<li>Hierarchical templates: complex templates made by combining simpler templates.</li>\n<li>Given our current understanding of the structure of the visual nervous system, hierarchical templates seem to be the only viable version of the template proposal.</li>\n<li>How templates apply to 3D objects is another problem without a good solution.</li>\n</ul>\n</li>\n<li>Fourier spectrum\n<ul>\n<li>If the early stages of visual analysis perform a Fourier-like analysis of decomposing the image into its spatial frequency components, then maybe shape and other higher-level properties of vision are described in terms of their spatial frequency content.</li>\n<li>However, this approach isn’t much better than standard templates for representing shape because without significant changes, it can’t handle the transformational aspects of shape equivalence.</li>\n<li>E.g. The Fourier spectrum of a big and little square aren’t identical even though the shape is identical.</li>\n<li>This approach stands alone in the field of theories of shape representation for its formal mathematical foundations.</li>\n<li>One issue with using the Fourier spectrum for shape representation is that the Fourier analysis applies to an entire image rather than to individual objects.</li>\n<li>This means that the representation of shape will be intertwined with its background, making the same object on different backgrounds have different spectra.</li>\n</ul>\n</li>\n<li>Features and dimensions\n<ul>\n<li>For several decades, the most popular class of shape representation was feature lists.</li>\n<li>Feature list: a symbolic description of shape using a simple set of attributes.</li>\n<li>An object’s perceived shape is defined by a feature list and the degree of similarity between two shapes can be measured by the degree of correspondence between the two feature lists.</li>\n<li>No notes on a multidimensional approach to object features.</li>\n<li>Instead of having continuous dimensions, another approach is to analyze objects as a structure of discrete features.</li>\n<li>Problems with the features theory include extracting features, specifying what the proper features are, and how detailed the feature lists should be.</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 668px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/4a23ce616750667351f3720a4fd4348b/74866/figure8-2-13.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 118.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAIAAAB1KUohAAAACXBIWXMAAAsTAAALEwEAmpwYAAACWElEQVR42pWUyXKjQBBE+XfHeA6OOVgLq4Sg2cQOYrE/b143CGGkOUxHBTTQWVlLFlrX97euu7Ha27LY8l7XjTTL2NdNU9X1ZGVVTVaUldYNwwxmsVF7kEVevL//NkyLNwtyxt9daP0wTORcFxvGMQwjaD1fcLq9dXXTYhJMFPdAtDnsu/FY1U1dN3meZ3meXNNrmpUcVODZhcKz0SaqCdlA0Q9hFIsgtG1nvz8YhmWYNm/4sAZPmx9gakMWSXIlWsuyj0fdNK3DQc/yAre4XmxyMYfNN2USHIax5/mK0+L68fGHyF+D75wrcBRfLh7Io65DTuXI+Rn8CHsJnjrHhC2ZTdKm1aZln8+uBD/h51atmxTFseteTNK1nOPR2O32SZJumF+AYZYFu6ZCBGAd53Q6naHdwB45b5iXEtAb6lTX9dTC/wBPaiF5BLOUcwVu7+An2CJycmhV859plVSbrTzX1qrxkvaE3II3UpM973uFbNWQdkvAL8CqSLMN4/D9/XVyThRs/BrBbGnlYP0D7IsAbb+9/frc7elXXpZyKn/SlrWS51TMW/f4H6hhvNJndE7bOTrH3K7BtYak0FWv5NGpO+kPLKnTRKl9nIKa0+avJF3I/4HGDKCnmBkWgmEsy+qKOLKca8o9K8IoYh8EIVEQBhlFauCDMNLO7gUB+r4f8IKDUcze9/xYLQb77KJ0ljfB/CDwfJ+ZQbdaFCeAAiHwTJLylIxFCF+orYdPBM4naHi0bAc3aJ69RmGgE4qWSVTh4YwQY+lUVizBJL/r8UjxOcuEF2X5FxyvyPNNlTw4AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 8.2.13\"\n        title=\"\"\n        src=\"/CR4-DL/static/4a23ce616750667351f3720a4fd4348b/74866/figure8-2-13.png\"\n        srcset=\"/CR4-DL/static/4a23ce616750667351f3720a4fd4348b/63868/figure8-2-13.png 250w,\n/CR4-DL/static/4a23ce616750667351f3720a4fd4348b/0b533/figure8-2-13.png 500w,\n/CR4-DL/static/4a23ce616750667351f3720a4fd4348b/74866/figure8-2-13.png 668w\"\n        sizes=\"(max-width: 668px) 100vw, 668px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Structural descriptions\n<ul>\n<li>Representations contain parts and their relationships to each other.</li>\n<li>This runs into the issue of how to represent the global shapes of the components.</li>\n<li>No notes on generalized cylinders that can have a different base shape, different axis, and different curvature.</li>\n<li>It isn’t clear if building descriptions out of primitive shapes or volumes is the best way to construct shape representations.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Section 8.3: Figural Goodness and Pragnanz</strong></p>\n<ul>\n<li>Figural goodness: a composite of simplicity, order, and regularity of an object.</li>\n<li>Why might figural goodness be important?</li>\n<li>One possibility is that good figures might be processed more efficiently than bad figures.</li>\n<li>One experiment found that people can match pairs of good figures more quickly, remember good figures more accurately, describe good figures using fewer words, and learn good figures more quickly than bad ones.</li>\n<li>Principle of Pragnanz: perception will be as good as the prevailing conditions allow.</li>\n<li>If many different perceptions are possible for a given figure, then figural goodness determines which perception is actually perceived.</li>\n<li>Two questions of figural goodness\n<ul>\n<li>What factors determine how “good” a shape appears to be?</li>\n<li>How can figural goodness be related to people’s perception of shape?</li>\n</ul>\n</li>\n<li>Factors affecting figural goodness\n<ul>\n<li>Complexity</li>\n<li>Number of parts</li>\n<li>Arrangement of parts</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 819px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/0c6f59196d7c4bc5f4befae5d44b18a2/97655/figure8-3-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.19999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAACBElEQVR42i2SaXPaMBiE/WvbfmlKoEmBTCFToMMRLuNbPrCNZVs2mOQX9nFcZkdeSbvvJbS8KFuUZX29qaqGVHUdJ2kYRcL3/SAQQkTnc5wkMs/r65XbW9NcbzeghdE5CKP0ko1fXra7PSEQcliUZXN/v93f3z8+rk3DNpN5c78HYTibzR3XLZXSeo/8+n4Qfvn6bbZYRHH8o9cDIgjWm81kOv09mcwXf6evr7+GwyRNjrr+/eGhPxjM5wtt8PPp6fmZ5JDdfr9arR/7/bftdrlao+CSPH9ms+FoNBqPl8vVdrsjHEEt29YoEmehFDmzPJd5AaHzvFRZXlykpJF2CimdyUJVpaqYSJrJmp4xoPg/NqWIgpN+znHMnCI+aZJJWVbYGGgNIKqqIJosSlVfySOLoht75w+iiFFTlB+GaZYpPJXqQhSfaAe2PxxX643tuBCkUZzAXU8IPwDtW0QRH0/4rhCW7TBIx/MghNZOhontqBu6bnBkWvbJND3fN0zLsCzLcQiEmhUl/jZuEHCORuOSVKbtWKyW3UrdNjArnFclJ4IuOYSyOO+q0zAc9JPjicNRZw88zzdtW4RBl4GeURIL4Cch7VDUZ2ZP4G+LNEyu9ZNBeXiIDXTD6KpAAwF46KI1C19zhX88Gei6O97mnCSMl1dNZcYrwrstf8+WSAmSCwfyH8iC10GR7xFbAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 8.3.3\"\n        title=\"\"\n        src=\"/CR4-DL/static/0c6f59196d7c4bc5f4befae5d44b18a2/97655/figure8-3-3.png\"\n        srcset=\"/CR4-DL/static/0c6f59196d7c4bc5f4befae5d44b18a2/63868/figure8-3-3.png 250w,\n/CR4-DL/static/0c6f59196d7c4bc5f4befae5d44b18a2/0b533/figure8-3-3.png 500w,\n/CR4-DL/static/0c6f59196d7c4bc5f4befae5d44b18a2/97655/figure8-3-3.png 819w\"\n        sizes=\"(max-width: 819px) 100vw, 819px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Figural goodness roughly corresponds to the subjective simplicity/complexity of an object.</li>\n<li>The first significant progress in objectifying figural goodness started with Claude Shannon’s information theory.</li>\n<li>E.g. Maybe good figures can be described using fewer bits of information.</li>\n<li>Another idea to measure figural goodness is the extent to which figures are the same after being transformed.</li>\n<li>This is called the theory of rotation and reflection (R &#x26; R) subsets.</li>\n<li>The fewer the transformations to create non-identical versions of the figure, the better the figure is.</li>\n<li>This is equivalent to the idea that better figures have more symmetries and are thus invariant to more transformations.</li>\n<li>Structural information theory: a method for constructing different shape descriptions of the same object and for relating them to perception using the Gestalt principle of Pragnanz.</li>\n<li>General outline of structural information theory\n<ol>\n<li>Construct a primitive code by tracing the contour of the figure and describing it as a sequence of line segment lengths and angles between them.</li>\n<li>Use a set of semantic operators to simplify the primitive code by removing structural redundancies resulting in reduced codes.</li>\n<li>Compute the information load of each reduced code by counting the number of parameters it has (roughly matches the figural goodness of perception).</li>\n<li>Find the reduced code with the lowest information load, that code is the minimum code and should be perceived most often.</li>\n</ol>\n</li>\n<li>Structural information theory can explain visual completion as it completes the occluded shape using the shape with the lowest information load.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 809px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/116638694ee2e0abdacf00abcdfdbfcf/e80ac/figure8-3-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 94%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC7klEQVR42l1Ta0/UQBTdn6MxrCAPjUAQIx8l0QTIqn8AvyGBSABBMAqi+370/dq20870scu6hZ/nue1iiM1Jdzp7zz13zp1b8lngMwawIAjCkIUhF0KSZVXTVHq00WgkIoEniqMCcRIDSZKU/CD0GXGAgPOQc5AVVUX0n3F2cHC4t7ePQKKBWDxETiZkFvJ/AF/Esayosqwoqrb+en1pcRHxUXyneIf/yX6OPIWQFPXFymp5anrt1RoLUFQUonpKcI98T1MAjAuPTsH7Hjv+crq3//n063mxT2ZERJoUXZDxH7ICPIoRQfqQElGUDpLBEG/s0GfhUpImyT0y4PrM9XyCz/yAuyxwXN/qu47rYY1cWAC+H4BfICdz0fd8TTfwNm3HdlzoK5qhGZZu2uhYEApkVFTdg4DrooA0HRTmlVCzQ4J+rV6vVD68fbNxcXHp9F2c/HqcHR4dvau8l2TFD4Kb29uTk5OtrUq3J12PRkQOySGGknZ3P5Wnph4+eLSzs4M7o2h6o9Ha3Nicm51vdzpByM/Ozp4uLDwul7//uMiyG1QOMp05TgdHx8do6eyTeZDH48y0rJerqzPTM8tLK1CG7Pb2x/m5Z4vPl39eXmVZRsoTt6PYsOxavXn1q6rqRpIOIHX+7RwZIQinBsOhaZrVaq1Wq3ueXxybzow2MOph4qC5eRXop4iTdDiEvdRb2okGg2GSprCNU9fSCGWjPGoJrLbsvB9oGIO91D9qHqzE2AS4ZDigB1F8cF5MQeny8up3tdaTlEazhcus60aj2W53evlU6Wr+0+tJkqKYtt3pdinCMHHzdV0vYQY6XQnug4kgmgbDUFSl2WpZjmOYFsYT8diXsMK0IAAJdB0WlDB9rXZXklXa1Q1k0SCWQyFdUsKHrlH0ZANbIBtmCRVJ+fRhgRw9SdZJWUUs9mVVlfOyUQKQV2vYto0cpmmVINXqdFXD8Fjg5de4AN15mBSKfB55KAT5RK7B7AhTz4X4CxA8ukt2n04yAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 8.3.6\"\n        title=\"\"\n        src=\"/CR4-DL/static/116638694ee2e0abdacf00abcdfdbfcf/e80ac/figure8-3-6.png\"\n        srcset=\"/CR4-DL/static/116638694ee2e0abdacf00abcdfdbfcf/63868/figure8-3-6.png 250w,\n/CR4-DL/static/116638694ee2e0abdacf00abcdfdbfcf/0b533/figure8-3-6.png 500w,\n/CR4-DL/static/116638694ee2e0abdacf00abcdfdbfcf/e80ac/figure8-3-6.png 809w\"\n        sizes=\"(max-width: 809px) 100vw, 809px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Structural information theory can easily deal with the problem of similarity transformations because the minimum code is independent of the figure’s position, size, orientation, and reflection.</li>\n<li>However, the theory runs into issues with gray-scale images since it only works on idealized outline drawings. It also doesn’t specify explicit parts and hasn’t been extended to 3D volumes.</li>\n<li>To find the interpretation with the lowest information load, the theory must compare all possible codes which is computationally challenging.</li>\n<li>E.g. In the case of the partly occluded square, there’s an infinite number of logical possible completions for the square and they can’t all be computed and compared.</li>\n</ul>\n<h2 id=\"chapter-9-perceiving-function-and-category\" style=\"position:relative;\"><a href=\"#chapter-9-perceiving-function-and-category\" aria-label=\"chapter 9 perceiving function and category permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 9: Perceiving Function and Category</h2>\n<ul>\n<li>What’s missing from our analysis of vision is how it assesses the function of perceived objects.</li>\n</ul>\n<p><strong>Section 9.1: The Perception of Function</strong></p>\n<ul>\n<li>Simply by looking, vision allows us to know what objects in the environment might be useful to reach our goals.</li>\n<li>Vision without function is like being transported to an alien planet populated with unfamiliar objects; you could perceive shapes, colors, positions, orientations, and other physical properties but you wouldn’t know what the objects do.</li>\n<li>All of the processes we’ve covered so far are ultimately to perceive the function of objects because that’s the evolutionary utility of vision.</li>\n<li>Two approaches to the visual perception of function\n<ul>\n<li>Affordances: opportunities for action or interaction provided by an object to observers that can perceive them.</li>\n<li>Categorization: opportunities for action or interaction provided by an object to observers that can first perceive the object’s category and then determine function based on category.</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 664px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/d240c10a19819ec55b67be8dc49db238/31493/figure9-1-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 122%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAIAAAB1KUohAAAACXBIWXMAAAsTAAALEwEAmpwYAAADUElEQVR42mWUiXIaMQyG/fK0gYSkEBbYy3t474u9IW/XzybtdKbKjnFsHb9+SRbdrbVt+3A47Pf74+F4vVyrqlqWZf5PhmE4HI6ovb9/nM+Xuq5E3/dJkiqlsizP0ixRSVlWk5Hxz6f3o/6No9h1XM/3gyDEl+A0kAGncZzEkYqimNN1XQk1mW9knSZO8iyPYx0jSTKlEgCKtm1VrMIg/Pj45bpunhdSBk3b1nXD2nZdP2ghhpQyDKOiKJu6USqNokigkqZZmqZvr3tWbBzHxT2+Yy2J4lNJFEa27fieT3D0pS9JVtxunev6eHIdjwu813W9rMukIc+LEX4fjy+u8IUlroMgTtNcjNMIOkhqmoY0wDkacpZ1xWzVcr+v/N3JvOuIdWOtqppUhMlo4AJ/MghPpxNlgNDTyQJ/IEPPk7AbhbH5Ingu8kJDmyZtfMNd39eQZII3hik25A9jdVWz5YwVgGVRdl2nqzfPus7D0A/POv6Vf7I1sn7/rPfH40ESzy7CuItjwARUCAJx/2wJru/3+7eLZRmNxyzP0yTV8ctK56xUvN+/bbfbzebH/u2dDs3zHD24tawzzWQ7Dn4H44522mx+vrxsLetCAErVNnVF2vxD/p0mtKcEXdtS/aLUQvEwnnXsGU1IIVnAi7oubfsKvWfronl2vX7o4clxPGV6HpJ9X1K4qqxo3qIosqwAPE0pqqoIw5Buf929UhvuAHm5OnkBryV7PQ+wEkWXi40xTZokuvkc2xFlWcAWedLbNDYbqkonsqFg2NBpeMcMXATkhC7G2PN8BqMmIBiuVxuSHNslK8wY8lAPme4KfM3LnGY5LiCCsaUutDeluukiGzKf/fgcfTbLfX2W+fH1wHg1zWp0Rzi79b0Yh566kiBTYVkWQYBHqownWMDmeR4s8kUaqgeQ0LwE+BVZmvCglFXF3JEnjwkrjmkDCCuM6DkyAvkkDGjyAq/4/DzudjtCwR46UOKZoSVbzAIdSPPFg8EenhhJM7kBGwEjUvpGg4K5XKMEW5SE8jJGnIOWaJzoW6ndGS+pQIf0MINCmhGXvn7f6POEU/SwwZjgaAPHcRxW/ZKoRJC+70NDAFoXbjyPa9+Iq4Eo7ZBPytZIY6Q0k/sbxiqn6loXJvQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 9.1.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/d240c10a19819ec55b67be8dc49db238/31493/figure9-1-1.png\"\n        srcset=\"/CR4-DL/static/d240c10a19819ec55b67be8dc49db238/63868/figure9-1-1.png 250w,\n/CR4-DL/static/d240c10a19819ec55b67be8dc49db238/0b533/figure9-1-1.png 500w,\n/CR4-DL/static/d240c10a19819ec55b67be8dc49db238/31493/figure9-1-1.png 664w\"\n        sizes=\"(max-width: 664px) 100vw, 664px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Two conditions of affordances\n<ul>\n<li>Functional form. There must be some mapping between visible properties and function.</li>\n<li>Observer relativity. Not all functions apply to all observers.</li>\n</ul>\n</li>\n<li>Not all affordances fit these conditions however.</li>\n<li>E.g. It isn’t obvious from the form of an apple that it affords eating or that a pencil affords writing.</li>\n<li>None of these functions appear to be available solely from visible information.</li>\n<li>Affordances are direct because they don’t require mediation through categorization and are the only perceivable functions for novel objects on an alien planet.</li>\n<li>It’s hypothesized that affordances are handled by the dorsal visual pathway, while categorization is handled by the ventral visual pathway.</li>\n<li>This makes sense because the dorsal pathway handles actions and may base those actions on visual features, while the ventral pathway underlies recognition and categorization that requires accessing internal representations of category in memory.</li>\n<li>Affordances don’t account for all cases of deriving function from form because some functions can’t be determined directly from physical properties.</li>\n<li>E.g. CD, keyboard, or sunscreen.</li>\n<li>For categorization, there’s virtually no limit on the link between an object’s category and its function since the link is arbitrary. It depends only on the associations established by prior experiences.</li>\n<li>Both affordances and categorization are likely used in everyday perception and there’s a spectrum from strong to weak links between form and function.</li>\n<li>Four components of categorization\n<ul>\n<li>Object representations. Object features must be perceived and represented.</li>\n<li>Category representations. The set of possible categories must be represented in memory.</li>\n<li>Comparison process. There must be some way of comparing the object’s representation against the set of category representations.</li>\n<li>Decision process. There must be some method for deciding which category the object belongs to.</li>\n</ul>\n</li>\n<li>Shape is the most important feature for categorization but it isn’t the only feature.</li>\n<li>Whatever type the object representation is, the category representation must be the same type for comparison to occur or else you need some conversion process between types.</li>\n<li>E.g. You can’t compare a template to a feature list.</li>\n<li>Does the comparison process happen serially (sequentially) or in parallel (simultaneously)?</li>\n<li>Given the estimated 30,000 categories that people store, the process is probably parallel or else it would take too long to be useful.</li>\n<li>E.g. The lion would have eaten you long before you figured out that it was a lion.</li>\n<li>But are the features compared serially or in parallel?</li>\n<li>E.g. Does it take twice as long to compare 20 features versus 10 features?</li>\n<li>We don’t know.</li>\n<li>If we assume that the comparison process results in a number representing the degree of fit/similarity of the object representation to the category representation, then the decision process can pick the category with the highest value or best fit or it can pick based on a threshold.</li>\n<li>In the case of novel objects, a new category can be made for it rather than incorrectly assigning it to a known category.</li>\n<li>Some categories are mutually exclusive and can’t share entities.</li>\n<li>E.g. An animal can’t be both a cat and dog.</li>\n<li>Thresholding can decide that an object fits into multiple categories but fails to mutually exclude them if needed.</li>\n<li>Best-fit can mutually exclude categories but can’t select multiple categories.</li>\n<li>Both decision rules can be combined as a maximum-over-threshold rule.</li>\n<li>This rule sets a threshold and then picks the category with the highest value, allowing for categorization into multiple classes while maintaining mutual exclusivity.</li>\n<li>The maximum-over-threshold rule can be implemented neurally using a winner-take-all network of units that all mutually inhibit each other once a winner is found.</li>\n</ul>\n<p><strong>Section 9.2: Phenomena of Perceptual Categorization</strong></p>\n<ul>\n<li>Objects can be members of more than one category.</li>\n<li>E.g. John is a person, animal, living being, and man.</li>\n<li>Categories are organized into a hierarchy and there are two theories of categorization: one based on binary rules (classical set theory) and the other based on prototype-graded membership (fuzzy set theory).</li>\n<li>E.g. “If an animal has four legs and long tail it’s a cat” versus “That animal looks like the average cat so it’s probably a cat”.</li>\n<li>No notes on the basic, subordinate, and superordinate levels of the categorization hierarchy.</li>\n<li>We reserve the term “object recognition” for when we realize that we’ve seen a given object before, regardless of whether we know what kind of object it is.</li>\n<li>We also reserve the term “object identification” for recognizing a particular known object rather than a member of a category.</li>\n<li>How does the perspective of 3D objects affect our ability to categorize it?</li>\n<li>Some viewpoints make it faster and more accurate to classify object.</li>\n<li>E.g. In one experiment, pictures that were rated as the best views were also categorized the fastest, while poor views were categorized the slowest.</li>\n<li>The existence of a best perspective and perspective effects in general show the error in our belief that we can recognize objects equally well from all possible perspectives.</li>\n<li>Two hypotheses of how we learn the best perspective\n<ul>\n<li>Frequency: the speed of naming is a function of the number of times we see the object from those viewpoints.</li>\n<li>Maximal information: perspective effects reflect the amount of information different views reveal about the shape and use of the object.</li>\n</ul>\n</li>\n<li>It’s likely that both hypotheses play a role in determining perspective effects as the most informative views are the views most sought after and thus the most seen.</li>\n<li>Review of priming experiments.</li>\n<li>Object priming doesn’t diminish with different object positions or reflection, but it does diminish with a different perspective, specifically for when the perspective reveals new parts.</li>\n<li>Findings suggest that people store multiple representations of the same object at different orientations rather than a single representation at one orientation or an orientation-invariant representation.</li>\n<li>Contextual effects play a role in visual categorization by affecting the efficiency of categorization.</li>\n<li>Review of visual agnosia (inability to categorize common objects).</li>\n<li>The fact that a case of pure visual agnosia for object classification exists is important evidence that classification is a separate physiological process that can be selectively impaired.</li>\n<li>Agnosics may be unable to overcome the difficulties presented by poor perspectives whereas normals are merely impaired.</li>\n</ul>\n<p><strong>Section 9.3: Theories of Object Categorization</strong></p>\n<ul>\n<li>Recognition by components (RBC) theory\n<ul>\n<li>Objects are specified as spatial arrangements of primitive volumetric components.</li>\n<li>The primitive components are then matched to the primitive components of object categories.</li>\n<li>Geons (geometric ions): a small set of generalized cylinders that can represent a huge number of objects.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 808px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/272c335da231cbb73030ebc9908f1802/3534c/figure9-3-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 107.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAIAAADJt1n/AAAACXBIWXMAAAsTAAALEwEAmpwYAAACc0lEQVR42nWU6XbbIBCF9f7/msTyrhXQAkhIsuK4ebl+QB27aTtHB4M0d+7MncHJ5T/mnDufz2VRzvN8Wdd/Pgl+y7KwruuKX9yzcnSji++/w6IBXu4GVd/11loXrG3buqqLvNS9Xt/fcY0MfzADg5DwUspGKVI9Z3mvtZCSELxs2265eNcY4jvY22UZxrFpGmuMGYbRua7r+G2aVhsDjAcHT/0UIpmDAZ+WxVhvpOemCTJrfAnvAUk4IRWBePMAT8HAQ1JVNWvX926aKbDrevjH0XW9btrW2EEpJYRYo2ZfYKyqqrKs4JRSkUlZloB9LsPAG1SAf7m3JtqDGTdP2ms2fLABRrYrWV+v67c+/WZ2bvoKELUL4Sn15+cnz/oMeLYAHgFGyee78YGYb2/pfn+A3yv8FzKqDaUnHuzAGj/EJCm1rsXopgBen2GX0PlE604qWRQlUjNV4FmFkNvtDuTH7eMBC2GpChWkkLQ9yfMzKqfpdrNJT6cTbezQTeu6rjmyUkVMChWOxxNxfc+FpIvJ+XQs8uJwOGL7/d6rbSx+t9stpCAAZFlGh42xcHhOO2RZziYxuufeMdFpmuJH/uAxXOk84fMsPxwOTC6XDICS6nq9QuOZ59nfIQarKAq+ITK1keQ4jv6G0YjLEuZspB2MAbQ4cKScpG2V9bfBMCAUgwyYuzefDUc/LOEQpyHeWQ9+ff3x+vKy2+3IHOaqrrmT3GRRC/TnYWZZ63BEJ4acFobqetTOlBR5nlfBvJfwfojEHie05Fb7oqSsSj//gCHgmKTp5nQ8Qitib4TgPiABqlEIuinFf4Rq49xrzYYag6LmF0nzUNe7v2mqAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 9.3.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/272c335da231cbb73030ebc9908f1802/3534c/figure9-3-1.png\"\n        srcset=\"/CR4-DL/static/272c335da231cbb73030ebc9908f1802/63868/figure9-3-1.png 250w,\n/CR4-DL/static/272c335da231cbb73030ebc9908f1802/0b533/figure9-3-1.png 500w,\n/CR4-DL/static/272c335da231cbb73030ebc9908f1802/3534c/figure9-3-1.png 808w\"\n        sizes=\"(max-width: 808px) 100vw, 808px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>RBC proposes that geons are identified directly from image-based features such as edges and vertices.</li>\n<li>Complex objects are thought of as configurations of two or more geons in a particular spatial arrangement, which encodes both the geons present and their spatial relations.</li>\n<li>Once an object has been represented by its component geons and spatial relations, the problem of object categorization reduces down to matching the structural description of that object with entries in a category.</li>\n</ul>\n</li>\n<li>Stages of object categorization in RBC\n<ol>\n<li>Edge extraction. Uses luminance gradients to obtain a clean line drawing of the edges.</li>\n<li>Feature detection. Uses edges, vertices, parallelism, and symmetry to identify geons.</li>\n<li>Object parsing. In parallel with feature detection, the system uses regions of concavity to divide the object into component geons.</li>\n<li>Geon categorization. Results from feature detection and object parsing are used to identify geons, their position, and their spatial relation.</li>\n<li>Category matching. Geons are automatically matched to similar geons in memory.</li>\n<li>Object categorization. The object is categorized by the most strongly activated category in memory that exceeds some threshold.</li>\n</ol>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 606px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/5fa81633ba10f10e786bb835ce8d02b2/4d4a2/figure9-3-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 139.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAIAAADuuAg3AAAACXBIWXMAAAsTAAALEwEAmpwYAAADF0lEQVR42m2UCZeaQBCE/dtJXuJu9lJQVG5kZLhBETf5h/mGUTyy77U46FRXdXfNTJr9Qcf+0BHdsRe7JEnkLpGbjR1G2yTNoijmr2Pfj9EdefSTL8FZXi6W1vTp+f39I83ybSz2h8MD+DiC20PXgt93EBRlBS0Ac7EMwmg3CGE3mXWMKRS4bkEd2VeWFf+UVS3TrKob1/PzoshyFSRNpHQ9LxZC015lt10ndrJpSdLHAjIJrWmYhmFSNoWgC3yaZhqpwKfTtWZ2wIl+IVPH9T4+ZsZiOZsbG9uhBMDH/qQX4K81A0jzAkAQRDEp6oYOF1Vtrda24+ZFSYBA1DaOtQT9+thtggqRShY/CFerdRhGvA274T5p5BVMwx5G5Tjuy+vbfG7MDXOxWMKv58xzBB/vwJd8RVEhGIfwZNrrjc0bpIeO8ar4gpki66ZlQmhGcNW0IpG8UjC9SGQKLRuaoWO6inPNNAke1/WYJO31/ZA1KWgvEl7f3r99/0H/8cJ2G4/iJyMtW5ntx2xOUDB+ptRYJLbtTqdPT79ffv6aCpHggrHhZzAiw1hgY9pDkYxqJ1M/VGpxm7XaTKfP0FKCdtvjqLS9KQlpjufJPF9aK6w2NxFkNc2+P33StqFzX855GIgQO1qQFeV8sBfptD11qaMdzrLRrMXrgUWxyMqKds0M01xa6IR/rFPjdS4F1jFKgJaCYUb28F1pe/4fkwrme7DSUjeMFFdyVOphfbhIvQOXTUto5QRtQzbGxiF403Y9meWeH1J5e+P/s+wHMMyAGRWWxBtYm4KDaFsNzrqNM/OI1Mx5WQl9AdoOytV6l9xiHmWP4PGQnP78RQK5+s8/zG+Uesdc1A1Bh+GvLnhaDKUKriSZNmPSm4JVzQrAeDgul1Zr5cwZMIanYIScLTgEr/r8TjaOw/Wj7i0/oE+JzPAT3cYn/CjzQieKhx/pvPa8uhNlOlnb9tKyOH+uH4B3/ZD/1AWw4TipU8mHbxbusOCYoogLj15OuKE4o37Agd8SnGDCD9S9xYKnowBRyNUoUzbgNpTr+v8BZkol0aYB1DkAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 9.3.5\"\n        title=\"\"\n        src=\"/CR4-DL/static/5fa81633ba10f10e786bb835ce8d02b2/4d4a2/figure9-3-5.png\"\n        srcset=\"/CR4-DL/static/5fa81633ba10f10e786bb835ce8d02b2/63868/figure9-3-5.png 250w,\n/CR4-DL/static/5fa81633ba10f10e786bb835ce8d02b2/0b533/figure9-3-5.png 500w,\n/CR4-DL/static/5fa81633ba10f10e786bb835ce8d02b2/4d4a2/figure9-3-5.png 606w\"\n        sizes=\"(max-width: 606px) 100vw, 606px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>No notes on a neural network model of RBC theory.</li>\n<li>The biggest problem with RBC theory is the lack of representational power, specifically the limitations from using generalized cylinders as primitives for shape representation.</li>\n<li>E.g. Discriminating between dogs and cats or between donkeys and horses requires fine discriminations both in their shapes and in the spatial relations among them. Such discriminations appear beyond the representational power of RBC theory.</li>\n<li>Since recognition and categorization performance isn’t invariant over different views, this raises the possibility that objects might be identified by matching 2D input views directly to some kind of view-specific category representation.</li>\n<li>The simplest idea is that objects are represented in memory by a single, canonical 2D view, and that 2D input views are matched directly against it.</li>\n<li>Three problems with this simple idea\n<ul>\n<li>Logically insufficient. No single 2D view can support accurate 3D object recognition from multiple perspectives without further information.</li>\n<li>Rotation. If we assume that other views are recognized by rotation, then this presupposes the functional equivalent of the 3D representation it was designed to eliminate.</li>\n<li>The hypothesis of rotation from a single canonical view is inconsistent with the data from perspective experiments.</li>\n</ul>\n</li>\n<li>A more realistic idea is that there are multiple 2D representations from several different viewpoints.</li>\n<li>These multiple views are likely from common past perspectives of seeing the object.</li>\n<li>One experiment found that recognition was mediated by a complete catalog of different view-specific 2D representations.</li>\n<li>There must be some way to extrapolate 3D structure from just a few views of an object.</li>\n<li>Our current theories of how this could be done fall short of providing a satisfying explanation.</li>\n<li>We can combine both view-based and part-based theories to explain categorization.</li>\n<li>E.g. If the current view matches a view-based representation in memory, then recognition is fast and accurate. However, if it doesn’t match, then categorization relies on the slower and more complex process of matching against structural descriptions.</li>\n</ul>\n<p><strong>Section 9.4: Identifying Letters and Words</strong></p>\n<ul>\n<li>We now turn to the problem of identifying letters and words from text.</li>\n<li>This isn’t the same as reading because we don’t need to understand the meaning of words to recognize them.</li>\n<li>Important simplifications of text\n<ul>\n<li>2D</li>\n<li>Combinatorial structure</li>\n</ul>\n</li>\n<li>We start with the analysis of how letters are identified.</li>\n<li>Templates are highly implausible because we would need too many of them to recognize all possible letters.</li>\n<li>E.g. Letters in different positions, sizes, orientations, and fonts.</li>\n<li>There’s little agreement on what type of representation people use to identify letters.</li>\n<li>No notes on the fuzzy logical model of perception (FLMP) as a continuous dimensional representation of letters.</li>\n<li>The FLMP does explain results on ambiguous letters that vary continuously between two known letters.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 822px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/09506a11e91f57a11ac92e9e2efe5009/f73a1/figure9-4-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 94%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsTAAALEwEAmpwYAAACR0lEQVR42m2TiZKiMBRF/f8vGBWykQ2CyGr/3pwXnJ62qi2KSjDn3rflsh/Hdhyvr69SSgzRh5Bzr40t49i2ivf93uSctdb82zTtsiyv12vf4Y7Ltu887B/TpLVJKbuOn48xIYFQ0yg+KmVSzm2rgQ/g44D/B+M8jiFEmL4f4NlqZUoZm1YhoXBOuVXmOc/A235sb3gTfihj22h4Y5xzXYipuvUEn1LP+oxi/nYm7HXb1gX2wMTZLoYUY+66kPtBaSOKrSIWAj7l5vknvK7LisJeyuN2a2PstbbA3gdtHM4AAiv9m/MJS9jFOakT5izYAo/jQ/yHorSNKeH/AS/rwoO1dKXBOZ/OqBjresEMbymbBK9w+gEvaM0kjrwT6J0zWlTuMT2V0sCskUOIasto1B694RW4lNu9ITacSRjr6ixly31/5syBX+Bt24A7H5gHigQ8Ph5anCc6jAQDg5ySPi//4bn+qBqtut2anKTaGPsQrXW4fdcZ3dv1/uH8fLJ9btsig0W1pc8JmA6TLZlXjFHRhEPBPmBKxcOgIqKVJXjSJU4esgDGn6Cs62rzzEe1GUViHMoQQoCfpieeMCxImD6zHAbZsqb4Rd4T/qwvf67X5n7nQqB6TkjTtkbbFDOe1nSeVGoh3/0PKfjIIPsuXCxHug6YYnIf6u2T9BBie15Sbqg1BBh9VZHJ1YbtxRrD9aUflrP1GlvbUTnkOWTMKe55TO2/tKDvIVG5eM9R573nax0NYBmmE3CQNRxUZP5q/ygNOS/L+hd7gu3u7s1NgAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 9.4.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/09506a11e91f57a11ac92e9e2efe5009/f73a1/figure9-4-2.png\"\n        srcset=\"/CR4-DL/static/09506a11e91f57a11ac92e9e2efe5009/63868/figure9-4-2.png 250w,\n/CR4-DL/static/09506a11e91f57a11ac92e9e2efe5009/0b533/figure9-4-2.png 500w,\n/CR4-DL/static/09506a11e91f57a11ac92e9e2efe5009/f73a1/figure9-4-2.png 822w\"\n        sizes=\"(max-width: 822px) 100vw, 822px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>But the FLMP doesn’t account for human performance in identifying letters.</li>\n<li>Letter categorization depends on the context of the other letters around it.</li>\n<li>E.g. The “THE” versus “CAT” example where the “H” and “A” are identical but are categorized differently due to the surrounding words.</li>\n<li>Letters are more quickly and easily reported when they appear in meaningful words compared to meaningless words.</li>\n<li>This is known as the word superiority effect.</li>\n<li>Interactive activation (IA) model\n<ul>\n<li>Starts with a multilayer connectionist network of artificial neurons and synapses.</li>\n</ul>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/cbfd83396e666d3beb9c517362711944/21482/figure9-4-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 65.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB6ElEQVR42kWS2Y6bMBSGeevcdpROBSEhhGHYF5MAxpgtfcJ+hpGKjozB/pezWJ2Ur7ZN0sT3/YcfNM/nMAx6Ms+6Ltu2rdv+/N22N/uVWNaFWNfV6gbV9n1elJ7nB8H38/UCSYDRWvdStl3Xtlzpp3na3m94DuplWax+UHLUUORFVRRV38tpmad5zoviEQRfX+H59+fn5x/3ert5XpbnHCHKShhwJ4esKMPvKAhCbM/LguCVu97dsZ3Lxb1eb7bt8HLsCxyj1ugibvUK2zLNckSiOAYM5aAU+DhJHo8HP4MggIPPjGthyOn8A8a20qOeatHUtTD5zDMr4K7rqqq63318ow+yqms1jmSrsX0ogxfNM0mzOEk7UxiTD9y1EI5t//r4OJ1OrOfzuawqeLG9x4jygO0sL+iUd78DOKpNVamz45Cxe/NuruteXDfLM0gRx+sBVrv4kKZpGIYcHMp4G8eR8pIwpMxAFEVSDntS2uCVMspSKRIl56Isj4QPPBvRNKRd13W1B/NDt/G8gwdLUjv63PeIAGbaph1spBkkM1LbT6wrP//nPCorimJAhrgy3LSqLMt630gpqfmhSS3M0jSiEVwsDKSy6B6dTKh1XqDNK074TNI0o2DgIRVCYA3Pe/979i1nXfsPAHnsD7wiA3YAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 9.4.6\"\n        title=\"\"\n        src=\"/CR4-DL/static/cbfd83396e666d3beb9c517362711944/00d43/figure9-4-6.png\"\n        srcset=\"/CR4-DL/static/cbfd83396e666d3beb9c517362711944/63868/figure9-4-6.png 250w,\n/CR4-DL/static/cbfd83396e666d3beb9c517362711944/0b533/figure9-4-6.png 500w,\n/CR4-DL/static/cbfd83396e666d3beb9c517362711944/00d43/figure9-4-6.png 1000w,\n/CR4-DL/static/cbfd83396e666d3beb9c517362711944/21482/figure9-4-6.png 1350w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<ul>\n<li>The network has feedback loops so the pattern of activation changes over time and eventually converges to a stable pattern.</li>\n<li>The connections are either excitatory or inhibitory.</li>\n<li>The goal of the model is to simulate the perceptual processes that underlie letter and word categorization such that the pattern of activation corresponds to perception.</li>\n<li>This is done using three layers of nodes: feature, letter, and word.</li>\n<li>The feature layer represents specific line segments at specific positions.</li>\n<li>E.g. All letters are composed of a subset of 12 possible segments with four positions each, resulting in 48 feature nodes.</li>\n<li>The letter level represents one of the 26 letters at one of four possible positions.</li>\n<li>Each letter nodes mutually inhibits each other to form a winner-take-all network.</li>\n<li>The highest level nodes represent four-letter words.</li>\n<li>Word nodes receive excitatory connections from letter nodes that contain its letters and receive inhibitory connections from letter nodes that don’t contain its letters.</li>\n<li>The letter nodes are position-dependent for the word nodes.</li>\n<li>E.g. The node for “A” inhibits the first position in the “BACK” node but excites the second position.</li>\n<li>The word network also has a winner-take-all architecture.</li>\n<li>To capture the word superiority effect, the IA model uses feedback connections from the word to letter level.</li>\n<li>E.g. For letters in words, the feedback from the presented word strongly activates the correct letter nodes. But for a random string of letters, no word node is activated so the letter nodes don’t benefit from any feedback.</li>\n<li>This difference in activation at the letter level allows the model to identify letters in words more quickly than in random strings.</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"part-iii-visual-dynamics\" style=\"position:relative;\"><a href=\"#part-iii-visual-dynamics\" aria-label=\"part iii visual dynamics permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Part III: Visual Dynamics</h1>\n<h2 id=\"chapter-10-perceiving-motion-and-events\" style=\"position:relative;\"><a href=\"#chapter-10-perceiving-motion-and-events\" aria-label=\"chapter 10 perceiving motion and events permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 10: Perceiving Motion and Events</h2>\n<p><strong>Section 10.1: Image Motion</strong></p>\n<ul>\n<li>Most objects in normal environments are stationary most of the time, but this makes the objects that do move important.</li>\n<li>The motion of an object is different from the motion of its retinal image when the eyes, head, and body move. If these are fixed, then it’s correct to equate object motion with retinal image motion.</li>\n<li>Image motion always depends on eye, head, body, and object motion.</li>\n<li>E.g. In a completely stationary environment, just moving your eyes moves the retinal image in the opposite direction. In contrast, moving your eyes to track a moving object moves your eyes without moving the retinal image.</li>\n<li>Computational problem of motion perception: how to go from dynamic optical events at the retina to the perception of moving objects in a stationary environment.</li>\n<li>The visual system appears to solve this problem in at least two steps\n<ul>\n<li>An early process of motion analysis using 2D image motion</li>\n<li>A late process of motion analysis using 3D objects moving in 3D space</li>\n</ul>\n</li>\n<li>Space-time diagram: a representation of image structure as it changes over time.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 671px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/c4edcbcba89334f296d2ac137a579087/d0e73/figure10-1-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 131.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAaCAIAAAA44esqAAAACXBIWXMAAAsTAAALEwEAmpwYAAADG0lEQVR42nWU6XLaMBSFec22T9FkhuRPJqS0tAFDgtlsy/uCdxvyhP0ksTUzZc4ISdbRvTrnSoO8KC4oyhJUdZ2kaRjFaZY5johiOnLYtA1ou/aMbpCXpUZRVkVVF1VV1k2UZvui6Po+TVP2osN2cRzfkLuul+TqAsh5WddNq8PSGQ4fgzBkKcyyqjRNgx0/k0HdNNl+HwRhHCfL5cr1fD8I4iRp2va/ZGgcFy6Az1Jw+DiSnu6fs+0vGKhUT2RNk6gbzT/tQqRe0vrD4TMZNF0fxkkUyYOhDcrT7vMc4ThCUeJFzkbM0x6OxyuZj1GS+kFo2U4YRY4QCGbbDsd2hMuJ15uN7Tgzw3A9D35V1fIAkDEGZf+8TlmNwgTEWOEK1u12FjR+prmyHRvNAOnA5ywncpJmM2NOQOR9eflBYcxmBgrblsNqQlMkcRJzbPL6nLYUjMzj2DDm8/kCLM0V0VbrNVgs3pgxDMM0TZv9hKhV2KtgxFcmIV2DZtLstkUsPFYWSNmxSut/VfvWp4tVuk+dh0p/7bPi/+uzJKvgV5PP/EuQ48cHLQWDz7d8dWak77okTXDrXEysPbZ9T3Bkf3tfjsc/v375hn5sdCmV05mTbI85fhiipCL4xnzx/DwaDh8eHh9GoxGavS+XOhcWXGubIsEbFEaeX5PJ97v7u7v7p6en6XRq2TbGYhIBARzygnzyWZELfDZXa9sR4/F4MvlNSZKbzrA/XBVSw4M+uX4MpMm6Ql+nxmq9EZ4nXFfDD+R9vEFIAF4cNpJkXSTaarbIJeSThNpItd3u9MN08U87d4qMVJovX56ylBVS6ftMtZTUCR0uKFbKIoGjLnajPB+8L83pbO64HoKRJ48OIhETBGFg2RaXhMJGS+qXr7SAr7QDW7jmekPJgp0laayj5YRCCDaCzDqGTNHhK5MowcxgZzubnWXZgg7gSlP6253FRsRxXW+z3qg5IYeex8WSWgohydQDOUsmPJZu5T+SOrJofFvxJNXVFDg+30EAWcjHMcryQj7t+JvnAOVo5ago9O1Xw0wboR+moiz/AoQB5ghZLDYFAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/c4edcbcba89334f296d2ac137a579087/d0e73/figure10-1-1.png\"\n        srcset=\"/CR4-DL/static/c4edcbcba89334f296d2ac137a579087/63868/figure10-1-1.png 250w,\n/CR4-DL/static/c4edcbcba89334f296d2ac137a579087/0b533/figure10-1-1.png 500w,\n/CR4-DL/static/c4edcbcba89334f296d2ac137a579087/d0e73/figure10-1-1.png 671w\"\n        sizes=\"(max-width: 671px) 100vw, 671px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Apparent motion: when a stationary object is perceived as moving.</li>\n<li>E.g. The rapid frame rate of movies provides the illusion of moving objects.</li>\n<li>The goal of perception isn’t to recover the velocity of images on the retina but the velocity of objects in the world.</li>\n<li>It seems unlikely that our brains store all of the possible 3D shapes of nonrigid objects that move.</li>\n<li>Instead, the brain may store the way in which objects move. Thus, motion may play a role in object classification.</li>\n<li>Not all motion events produce visual experiences of motion.</li>\n<li>E.g. The blades of a fan move too quickly to be perceived as moving. Instead, the blades look like an unmoving blurred surface. The moon moves too slowly to be experienced as moving.</li>\n<li>There’s a range of velocities that we perceive as motion and movement slower than this range is perceived as stationary, while movement faster than this range is perceived as spatially smeared across its trajectory.</li>\n<li>Experiments found that the visual system is more sensitive to motion of one object relative to another than it is to the motion of the same object relative to the observer.</li>\n<li>Motion is also subject to adaptation and aftereffects like other visual features.</li>\n<li>E.g. Prolonged viewing of constant motion diminishes its intensity over time, causing the motion to appear slowed down.</li>\n<li>Waterfall illusion: if you stare at a waterfall for awhile and then look at a stationary object, the object appears to move upwards.</li>\n<li>Where in the visual system do motion aftereffects occur?</li>\n<li>We can start by studying interocular transfer effects.</li>\n<li>Interocular transfer effects: the extend to which adaptation in one eye transfer to the other eye.</li>\n<li>If the effect transfers, this suggests that motion aftereffects occur in the brain and not the eye.</li>\n<li>Results support the hypothesis that motion aftereffects occur in the binocular nervous system, but different motion aftereffects can occur in both eyes independently.</li>\n<li>Motion contrast is analogous to the phenomenon of simultaneous lightness contrast where the perceived lightness of a gray central patch is affected by the lightness of its surrounding region.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 830px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/48835b3548832d765335cd2f0abfd2dd/715a3/figure10-1-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAACQElEQVR42i1Sh7KqMBT0g33PigLSBRtiQ5FQFWyEovcP7yI3s8OcCdnsnj1pPV+vx/N5u9/3hwPH8yzHLVdmck3zvHi+siyjeVEkSbpamaIoSbLsElJWVYlVVa1nloFpGPNOp9frDSYTsd3+L4ry8XjKi7Ioq4Nty7I6HDIsywHdbm++WBQN+UXzg33ErQwzVjUddb8/0DRdlmRKc4iriiYKIg5sNluYwi1TXfd8//Pz03q8Mm1qyIqmadOVaZmmhYLjJizLX65JHF8VRZMVZblcGcZMUVWWZUGezebV+w3bFDo8Lyiqttns1tZmu9vPZguQieddk0QQRF03YBgFy/Fry1Lh0DD+yDj/r91Bt+u1td8fJhOBHXNT3bg/HkmaCoLAMCNFUbfbnWmukdmQYYIwfH8+NTm53bu9gSBK0JckBed63f7p5CASRA3CaDRGL2gb/pE4gMBqZfSMzPwwOp6c+WKJFixrY9s2pfWQsDBLxzk757Mxm6Nt2z4hC8jWadfkjGKc8eXq+YEfBHEcgwgqUPPLmh/HF1gF8CigCWatTIsS8wQfyq5LgiCE4SRJGs+wB4RhRAimE7jEO59d7P8pn10SQfSa4pve7q/vm0GR3m4QQWZARim++AXgLuwDqFuYjbm2cAWeh+O4YRTDAsTxwuCSYHleGEUnx4Fy7TwIIR7FMdCq+/QDQnxs4YcP+CEWcb9GgwAH4svF/8YB/03nuLEmOxAlHshQrktCoiiCT4QENFYbYDP7jqD5S2n+C4LW7Z5Gxf2DAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1.5\"\n        title=\"\"\n        src=\"/CR4-DL/static/48835b3548832d765335cd2f0abfd2dd/715a3/figure10-1-5.png\"\n        srcset=\"/CR4-DL/static/48835b3548832d765335cd2f0abfd2dd/63868/figure10-1-5.png 250w,\n/CR4-DL/static/48835b3548832d765335cd2f0abfd2dd/0b533/figure10-1-5.png 500w,\n/CR4-DL/static/48835b3548832d765335cd2f0abfd2dd/715a3/figure10-1-5.png 830w\"\n        sizes=\"(max-width: 830px) 100vw, 830px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>So motion contrast may be caused by lateral inhibition in the visual system.</li>\n<li>One of the most powerful factors affecting apparent motion is the alternation rate.</li>\n<li>Alternation rate: how rapidly the displays are interchanged.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 562px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/fd227a829d13f2ffd175c2eef08fb933/6e88f/figure10-1-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 151.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAeCAIAAACjcKk8AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEEElEQVR42j2V53KlOBCFefOp2fLu/pmadANRIIIAESTCtef19mvhWlebKySdDqcD0Xmer9fLWus3P81zWVaFKpGqqpq2zfMiTbPO9KuXU/Zvt8fj8RzH0XkXHef5/vEBxvRD23UsVKnLqta6KSuNkmleVuenZV3WdbDT29//suk2vzgXYRfpB/anrjNtZ0zft61pLulM3ZrF+WX18+qnddVNO4x2dh6NYhlpDTectVPdtLoOotu66fIcV2q3basXsdP84+fvzgyiDvBlebAWgOlHwuv63gxj1w9caru+64Zc6ULpbT/tvNzucWuG+X/wEUjz+75uG8/9ONy2b8fJwm+b8/jsMOv3YzuO/XxfN7mACPj1/k7M8MkaD/ETSZJM180WANzrB5ukeZYX7D/jNOa0aQX88eeP1jXRjnaC87ppmrarqpqtHq2j3c8Ttjnqh/Hbt++cEiAMR5fPpAGRXTNAN0/usUANe/gMfjv2/Xz1o72CErflamdGazGBZf7hfJrJ7oJQGKyQeRUN5OjX7zvXQPIeoRuHIaYX8MwB2OvJDoLbJKLSzfE6SVWcpNTEJ9iE3BD2Rft+wud5LTZx9YB/VG/7jkD4jsN+X0g7qYIeBKurc9BOYigL6jnLcp7ruqIUmPObUhUkJ3JU3O6Ppuki+KDKsQ+93m/LsqiiIlrI4/ZxHBwRDy4QNoS8vf1jpJaFiwiXKAaI8d5LCMbgI0QQLZkPZPe71IYEQra+/vU24/F+SMx4RVfRTxADTzzJGemFc0yJX4E54oIICuPLl6+8uitmtLJL20lj2enCA0B4l+doCQEht9T2z183wBBGn0mReEm+FCmBwY1QHUi+GIbg63UN1jCzhvYUMAcgq0qvqyPyOM3ujIqYdGaP5xOhpEkBSNooSdXt9rzf43FapKvE6/cXPcAA8MfuQlakgZ2DCDgDyStXKQxoiOMU49PqoO2ztiHZha7CPeIMlRwaMHQl3LG+5gEOylQCzBg6gtvkhuljAz2SxnlGIEw4s1OlayofjV0/EgdHeCQz7BpDOEkyKIYxgCmgoizHyfayN7VtS/IAM2HgA42fYOFTWCURkrMwK86GUTgMgfb9qhAq6Qqb3rrcFnCSpFCqVMmiYurJ9C1hj+lAbTFl47APoYVikhumYqlrRZd1JsKG4rb8tYwOpRROsiZ5aAR2fzz5oR3Qzk6a5aqqMsaq1hHFCw620a61zvlGFAV9Boyvhm4aFT4DpF0y/4iDovx2v2MyCt8XdSGDbvRW0pVZFpBlkuJ2jUqm3+MZ88WhJeMkQWmEqxi5nERLsIkp4JWSIdgUhcqVyosSkemZZWleUIhFpSVmAHK15CwHz5MPCFV1jTPyb8LXiM9A348krxvHhsk32v8ASQmWmZS7a8UAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1.6\"\n        title=\"\"\n        src=\"/CR4-DL/static/fd227a829d13f2ffd175c2eef08fb933/6e88f/figure10-1-6.png\"\n        srcset=\"/CR4-DL/static/fd227a829d13f2ffd175c2eef08fb933/63868/figure10-1-6.png 250w,\n/CR4-DL/static/fd227a829d13f2ffd175c2eef08fb933/0b533/figure10-1-6.png 500w,\n/CR4-DL/static/fd227a829d13f2ffd175c2eef08fb933/6e88f/figure10-1-6.png 562w\"\n        sizes=\"(max-width: 562px) 100vw, 562px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Flicker fusion: when a flickering light appears to be continuously on.</li>\n<li>Flicker fusion occurs at a flicker rate around 60 times per second or faster.</li>\n<li>The existence of apparent motion implies that the visual system somehow determines which objects in one display go with which objects in the next display.</li>\n<li>Correspondence problem of apparent motion: how does the visual system match objects from one display to the next?</li>\n<li>This problem is analogous to the correspondence problem of stereoscopic vision.</li>\n<li>The correspondence problem shows up whenever two or more objects are present in apparent motion displays.</li>\n<li>The most important factor in solving the problem is the proximity or distance between corresponding elements.</li>\n<li>All else being equal, the closest elements are perceived as identical.</li>\n<li>This explains why wheels appear to be moving backwards at certain rotation speeds.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 670px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/dd06e52487c20e2c07b43aaa8cab450b/d67fd/figure10-1-9.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 120%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAIAAAB1KUohAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC/ElEQVR42lWUh3rkIAyE/f65TVzWBfeyLmDcNnnA+4FNLkf4iAwaaTSI9c7z+D2FEF3fY+3HaxiDA2Ps13VO07htGoM9z2JOe2pd962q658th1TrqrftvK5xGqWU7tCBX2O3rsuylFVlabyQUqm3tz9EXKSc5pkQxnM3i3dd17ZtZVE416IoMyEwNnd+HKvW87K0bTdO0wtpwXA0YK11FEVu93xe+IH8mQQaGdN0fLP7D0xmsjm/umnSTHDMJ3WyDo8HbN2Om7/Bp9YbbN3xYxzLsnJstUXCmaAu0D+wlfZFO45jVzPri/ZxPMg5TeyAdAAMF2U/vmkrpciMzlVVCjPy6/msmqYfeiCEJigUGBgwIsKxE0Ib8LquRVk2bRsGgRO27fosy7Re8SPFNM1pmrZd5wfhx4dfluVh0BYsV0W6YRjIjza+H3Rdx/VKJc/L5BR5zidGFEZJmi5ygfO2rUYwKVXTNISHcFXVIhPQhoLeN2qGAgRmWkRKOicvCoKSVmvl8Y0AeLdte7/fKajvB6gCHoYHXJQ2vSnlwmWLLKNDOQVpwDQiH5CnkvfbO4Hruv78+prnpa4b9l10dKIXIG+vVv2AT9f/9BDEKCyOE27byDtOg1nGfnigFq+taTt2tOX8yuzegLm6gwxrniNej7xwQcUwDKmItOhcN210v1dVBfF1lZ7pjPOnAahvJT8mKze/257jSSIquTF64j0GpDbgz8/n83m54Z4pPZPYgc6B77OmaeIHATmDMOTatBkW3PUdMiKPqXEc11VRPJOrIYpaFbeK8jwrVroAw6rBaxk9SrrdbgSnvZMkpZ4PP7jHcZplRVGQh90wiiiYtDjwyY3G8R1pPMQo7QCWWUBqeGZUGNsQWUbXCPqXlQkuSRN+PJgevcV5S2fbKHwWRS5yYaKkSU58+q4oMvMvJzMsTEQh0iQxYG7FHuU4uRDGzIuqrEwGO9jAAwy0OYuTuMhzDw/wcCYVGAxc4Wz6wLwNZe5JIZtkOs3mZZ74m6e/vo+htSeFkvIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1.9\"\n        title=\"\"\n        src=\"/CR4-DL/static/dd06e52487c20e2c07b43aaa8cab450b/d67fd/figure10-1-9.png\"\n        srcset=\"/CR4-DL/static/dd06e52487c20e2c07b43aaa8cab450b/63868/figure10-1-9.png 250w,\n/CR4-DL/static/dd06e52487c20e2c07b43aaa8cab450b/0b533/figure10-1-9.png 500w,\n/CR4-DL/static/dd06e52487c20e2c07b43aaa8cab450b/d67fd/figure10-1-9.png 670w\"\n        sizes=\"(max-width: 670px) 100vw, 670px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Surprisingly, factors other than proximity (such as size, orientation, and color) had less of an effect than proximity on apparent motion and the correspondence problem.</li>\n<li>Studies have shown that time and distance trade off against each other in solving the correspondence problem.</li>\n<li>E.g. At slow rates, correspondences over farther distances are preferred. At fast rates, correspondences over short distances are preferred.</li>\n<li>There appears to be two different motion processing systems, one for short-range motion and one for long-range motion.</li>\n<li>Short-range motion system: sensitive to short displacements and rapid alternation rates.</li>\n<li>Long-range motion system: sensitive to large displacements and slow alternation rates.</li>\n<li>However, this distinction has been criticized because each system is almost always studied using a specific stimuli, so we can’t be sure that differences between phenomena aren’t due to differences between the way a process responds to difference stimuli.</li>\n<li>The truth is probably a good deal more complex.</li>\n<li>Aperture problem: since neurons have small receptive fields, there’s local ambiguity in the direction and speed of motion in the field.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 556px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/ed22e336659d083dbac7c0dcb2de1221/96638/figure10-1-13.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 160.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAIAAACdAM/hAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEjElEQVR42mWV13LrRhBE+a++b7ICg8RMZJCggiUVAyKTLgNABMpf6DMAVXaVVyAKWGxPz/T0rmp5nmdZerkUjuNoumaPx1zjiTWdOrZtW7b1/DJ1phPHmYwn4+kzk1a0Xl8ul7woaoBTwN+XyWSim6aijvqDwXA4bLWahmHa9rjVaimqomryGwz6hmlEq6gAnOcVOLt8f8Ojakq93rh/eGi0mkT48+b21x+/+r1Bo/HIaDbrjWaD7KJIwFmWCficZryQrKISXuHq9nrz+fx4ig+Ho+f7/X5PlaE5U8e0zDBc5cWFfH/ARQGzbhiUO55w2VwwrNZrx5kyN3b4wixr9DAM/wWTtjBP5AOBuYii68ZQxohX6pS7ZZRgIwhD1gsYTrJHPb4DtsYojEzjiTOhPGac5ykYXu2JkBumHvyfma6omgaDVK6o3V7XYJhWTwrWRsoILXRDI6P/grNzmhaFpA241Xp8fBJtoZ4v3Nl8ToubrVa/32+323d3d5qmCRjK9FwjZwTL88KyLMLT3na7Q4j39w8/CF3Pe3t7p38dRrdDXoArwZIzYGHOQFvWVTDk0XT15fUlSZJjHD8/vyiaWmlm2YDVIAiyvIiTuGwV/swyZKKgH20tnjVRTMNbBGXyZ14DnOZZnCRXwSAnbfQwLRYhCia3qWIwHIiQBCnB8qyqvjBnJXNRlODUkqgqn4HRKhMeVkstRtl5vXogEQQDEsc/YKqvoqIWaj89tQaDget62ItdYpQO0Qxpu1rWLGCYxST5FTwcDW/vbuuN+v3D/fPLi+u6CMtSRVH4WpGz7fzAvzJXmwsw2dJnXUdYHZ/QYWz3/ff3bDbDHhRFzVTOcxAGlBnHJxEM5jRNAWs/gpXd0v56f399fVXLPokA3C1aoPp+BY6vaifnBG3Ee6bsiuoBY3IwCKFRRdSpXNfF2+f0fAVLzUksfTY0y6YfYgaEHamjyu1VtYDFYSU4EfAJwQQOM5uW9vYHvU4XyZ9w8tvbG1uz02lb5TbWSv9hT/qcVMwCLsQupSuGNzc37Q7Qp4/Pj81243kelG3wtniLEhRNwfBxcj6eRDC8mdM0jDEcjXB/tyet/vj4XK83nCVL172/v8dtlRYwczCd4uRwOFxrpoDSnrppW2ZZMBt/s93OZvN6vc7GuCvxlF06LCLT4/Eo4LzIT6cTB3IpiVGdRDyTwmDYHylKs9XsdLtyzrCrytPzfD6fEIzj/vIt+4tz25Q+6eW9cqKGn1gNXtWFE/NzX62itFJ7v98T4xTH681mvdp8/f69221/77l/bbY7Tk/K3u52fKWKTXk/HA9kejweap+fnzgpDCPP91x3eTwdWIcBeQ2jCLXm88Ua6bZbzmF8jttpFfZer1e1KAqXSzeMQl48z/V9n1dGtOLVXyyXaAu/6/l0CCgX33E+EWoku/RckLARiCgM2guY1R4UwhPwjPKLxYIFECyW/C0Ab0iG/11cFISS682KhGEgYbLgB1WVP5m5EjDwZXg1eIAFoQ/NChwpBH60IjxlM1kuK5nB776+9ofD/iC3/X7/DyTXQhxTX6OxAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1.13\"\n        title=\"\"\n        src=\"/CR4-DL/static/ed22e336659d083dbac7c0dcb2de1221/96638/figure10-1-13.png\"\n        srcset=\"/CR4-DL/static/ed22e336659d083dbac7c0dcb2de1221/63868/figure10-1-13.png 250w,\n/CR4-DL/static/ed22e336659d083dbac7c0dcb2de1221/0b533/figure10-1-13.png 500w,\n/CR4-DL/static/ed22e336659d083dbac7c0dcb2de1221/96638/figure10-1-13.png 556w\"\n        sizes=\"(max-width: 556px) 100vw, 556px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>In terms of physiology, almost nothing is known about the long-range system so we’ll focus on the short-range system.</li>\n<li>The short-range system is thought to work in almost all motion perception from continuous motion to apparent motion at short distances.</li>\n<li>Review of the magnocellular (M) and parvocellular (P) layers of the LGN.</li>\n<li>In contrast to P cells, M cells respond rapidly to changes in stimulation.</li>\n<li>The facts are consistent with the thought that magno cells represent an early stage of motion processing because magno cells project to layer V1. Layer V1 then projects to areas MT and MST, both of which appear to be heavily involved in processing visual motion information.</li>\n<li>The majority of V1 cells are motion-sensitive in one direction.</li>\n<li>When presented with plaid gratings, directionally-sensitive V1 cells respond selectively to the motion of one of the component gratings as if the other gratings weren’t present.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 606px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/18c9ebff30659aa334025b8d71b6e175/4d4a2/figure10-1-17.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 143.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAdCAIAAAAl5NuSAAAACXBIWXMAAAsTAAALEwEAmpwYAAADfklEQVR42mWU65bbKBCE9fKbH4lnPLauIIQkdL/Y8rzhfg2OE+9yMAcDRXdVF4qmeXn2ZZ2WJS+K08en0uWybvO6LttG3263umkv12uW530/HMdxu0uLnrB5Gad52+XQ+evCfF5W8CCZs7vf79c4trY+HseNFsDjvMzr1g8jsK7vGW3dMA7jtG7buu9slVXVj2PTtv0wjON4Pw6QxI9IlchEcF0v8ee5GwYwjXOMJMzYdh0jODDkNC/L4fHR7KmmeZEXCiS5/Tydhmmq6rpuGnJJs5yL2DqdPjjkXGebhhsej4eAESac5oQ25hLH3ThygkXubV1HUq7rPj4/c9+cc4dvpC1gsgLpkHKaYFi38B9C2kgFEa5WWg/DQDrbtr+BQ1WIQ5HOl0vTOaRet32/3QJt8GmWVZUNyKdgAvMlCfVEZOJvHrO/967roPr9/X38bk/Ogt+lMCQp/X9IOgq9Yv4XDFLqMU3iEP69I6m5JyHtL7AghTBIRlWWOKxQOlANdJiUxsRJopRu25bM3zgjFTG5k5LGSSqAbQ+5UAX6/fEolIL2O2cSXnHPhI2QtGmddOfEnr5UwQJsGUza99T+RTsKdeJQNw7El5cwz2JJ50KRGdGflzBOE7QpNfSDeALmdIIJC0WQJMt+/vqFQyCJyfAWhpNEhuGfHz+CvcqyXJZV7BlMAp6jhMVeXBMsxV1sERZGvIqvry9rrXiVRLzmf+zJUYzFLXA21gIIPhFG3ttVVSEL9nxVy4N/v3tuKbS+JglvWNTyhQ11JnKSJODx0XF/vWdeJ0j/0eFLAtvGS7V4VzxtgWz7zbWOFI/H4/ZS++PzfD5fdGkQDG8EtqStvWB8PWxd+4lDbV4d0843mEdxkiVprrVB78paYFlRoFmuVJoz8MwMXHRZ8iRBMkM2BAcfJXwRfUxT2dzDcBIm5TQxkzQNSBb5tMDZGLrhTi6KsoIIYEtA1zgBxmkiUmeuk/orRfLsslCW2OzZuDEqdMmJJMl88KqUVXO5xnCGPIv8hTOfAbENWpCRremsRIQlVfDEIBQAtkmFxapuwILnL8FpjNBCPThT8AiMBPXRSIqJEm3KXMhrX4UiTlMWiU9wVBBdSdLayIflpOaXerlSnwWnkb0otNye59BBCy4FJIoqofUULM3lr8imtMhr/FwLUlQwBucQjF1eXvh+rNv2LxpDUxedyD+4AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1.17\"\n        title=\"\"\n        src=\"/CR4-DL/static/18c9ebff30659aa334025b8d71b6e175/4d4a2/figure10-1-17.png\"\n        srcset=\"/CR4-DL/static/18c9ebff30659aa334025b8d71b6e175/63868/figure10-1-17.png 250w,\n/CR4-DL/static/18c9ebff30659aa334025b8d71b6e175/0b533/figure10-1-17.png 500w,\n/CR4-DL/static/18c9ebff30659aa334025b8d71b6e175/4d4a2/figure10-1-17.png 606w\"\n        sizes=\"(max-width: 606px) 100vw, 606px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>This differs from what people consciously experience, suggesting that the neural activity underlying conscious motion perception must lie farther along the motion system.</li>\n<li>Why are directionally-selective cells sensitive to direction?</li>\n<li>Somewhat surprisingly, the answer isn’t yet known with certainty because it’s difficult to determine the precise wiring of the visual system from physiological and anatomical techniques.</li>\n<li>Most computational theories of image motion are based on some version of a delay-and-compare scheme.</li>\n<li>Delay-and-compare scheme: compare what’s happening in one retinal region with what happened before in a nearby area while accounting for some time delay.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 538px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/13d16353caf5199dd9e91137688962da/9516f/figure10-1-18.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 166%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAhCAIAAABWXBxEAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD2klEQVR42o1VCXIaOxDlRknVzyES/xOkXP8eTuwkBtsBbDbD7PsKA7NqNhufLk8SYDs4qT/VpRGaft2vFzWdqqrK/UNKgqcgRRD6pmmso2WSplEUyYrkOG6WZ/TJ6SvNUkgHYMKhzEpRFND++vXbu3fvb2/v7u7GqqZ9+OfD2dlZ2z4kKcAFkzzN8k5d14QhKQUmQRB2L68n43s/CG3bGQ5H/f5dQco97Fl2YI7HWjcNwJPxzPOD9vGhquv5XBQEKVpv8py8AS5orM94UDIMSxBlWdE2ceK4sBPQUBkAzA/kOxUFEwYvD2LZDgz1ujfT6Txar+M4gYM0L/Liz7Rp5HWdF0QUZZzAQbiMvpxdWJaFT8hQ9pr5DsxQdbnPGSLH2rRN89CORtPJZFbWNa3S/wFz2cQxi6hMsgzvHFIUTMjbYPqq64odog8otEYWK/gsKJj8DcwKXhNSbZ+eQB6nIIvvTdsCVuydvwYzh9jQrGb5ZDo7Ofl3Npurqu553qdPH8/PvzTtA426OIq5ZJ4J6kxIGK5+/Oh9/nwqycrsfi4Iwunpf4IogggH/+6ZNyhhNgzT6veHlmVDyXW9QX+IDsOXgpA3aL/MMKlKdEX3sjcajeEKJr5dfIfbJE1YUOQV7d2t2oPxuW5qSVL6/cFweDsY3I7HE0qWwV6C6a16CT7Y/vlzgGxdXd/M58JgMPR9H2rZ3gTH764kf7gVxLzZxL3eta4b7eOj74dXVze6YfKaHcI+ArPgeZ8DiYCxedxuTdNaRVH52jOQO9rHXYlQcSu4ocl4ats2Bx8484u5i5kLy1bjuO75+YXn+dunbRAuLy+7kixz2pwt5A0wm37o2zxOEpRHVpT7+QKVwxh8meRncHkE5v6B6XWvMDTZDS/+COb37jdBzPMF3EucyzE4TlM6hnBj0JaHUh8yB/LVfs5wB3yGspHCSqVpuihJQRiiJIZhYkpG0Xq1imzHXS6XHn18kMcJJiFkuVr7fmCatuN4HdM0FwsBSAw9C1quh2PUGXNLVVX8VlR1sRAVVZMVFVMVn6Clabg+dgeqlm1D1aWGXV3XHZxYFvaGSW1iBcBgvKiYFhpO1XBmdlzXhQ5WqmBanu+Dg6pq2IMe/muAVxSFKVN68Ilrg5lOwegewzDYSuNwPQ9WwEXXdMSs4YdhgjlSANqgyijAnIpAOlSbahgOcw48DGFFV1ErlgOGMMFWHUZhgu41+HM6NHpdx/+Ts8uTjdrCInVObbkaCxirIIhwJ0oy9nylnhEYEouINFUDGNyQMIsFghfNs6yGKNEmXkVrdB7WMFwuV9EvTzkMcGZxPKcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1.18\"\n        title=\"\"\n        src=\"/CR4-DL/static/13d16353caf5199dd9e91137688962da/9516f/figure10-1-18.png\"\n        srcset=\"/CR4-DL/static/13d16353caf5199dd9e91137688962da/63868/figure10-1-18.png 250w,\n/CR4-DL/static/13d16353caf5199dd9e91137688962da/0b533/figure10-1-18.png 500w,\n/CR4-DL/static/13d16353caf5199dd9e91137688962da/9516f/figure10-1-18.png 538w\"\n        sizes=\"(max-width: 538px) 100vw, 538px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>We can incorporate a temporal delay between the outputs of two spatially separated receptors, causing the activity of both receptors to converge at the motion detector cell.</li>\n<li>The detector then sums the input of the direct unit with the delayed unit and the output reflects if the motion is in the preferred direction with the preferred time lag.</li>\n<li>A different type of motion detector circuit is based on edge detection.</li>\n<li>Motion detectors could be built by analyzing the change in illumination over time with an edge detector.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 824px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/6e3f5bd663e84c92ca6e8e304690c9d3/c1c45/figure10-1-19.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAACKUlEQVR42l1Ty3LTMBT1f9ElHeAHaFYdlh1WDcssoOl02NDEju34Jcu2pPgt+fWFPZIhMCRnHEm+55x7dW+sYRwBNWhkWR4nCSEkywuaZb2UXPBhHKZ5GvGdp/9grWTDH8B0Pd/z/ZeXn7ZtF4w1bTsv80oep2kFgtethZ+VLJWK4nglb7ffdrsdISnMl2WZ5hmha3bwkApQWFhX27brwyi2T6eT697ff/n44dPT03OvVCf7pm3Kuu6lAq3pZdP1bQ9Z9ZfctF0QhmDatnP7/vbm3c3d3QaV4/N6OByOdlnVcjDkXna9BPsfcte53tkxzg8PXzefN8ejDQchLo7t+EHQSmSh0WsY8rUGJOOfA5Cdk/v4uP3+Yx8npO06ZFQ3DTi18Wz/PFfyaMhDJ6UfhLajyb9eDwDjAudQp5QiTDvDwzARjOuwrk3GoigYSdMUheY5FifXIykFwjDy/TPq5kLI35lqUZ02cJUw23FeFijs98+u50EJ/DxnCUkLxkGDpwbSRhxmx7R+WiXMDMy4T0SLS7k2edDjgcWsnWGg0DZlYRIwjBgmpMS5qGoMFW645OJyKSsAh3iFPl+qigkE1GVZibLE1kIx5yBIKY0wnAmhKLdgkMO0EUphjpPEXASeRG8zmmd4hYXFOAdT/x9SnYK+IUoLzmECnSiKwUNmUULQuUKI1CjqGMYtREASPokuIEv1KVuF8AqiSATdRjlVg3Z3eKIETFvdtG+UzDiXw9fOfAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1.19\"\n        title=\"\"\n        src=\"/CR4-DL/static/6e3f5bd663e84c92ca6e8e304690c9d3/c1c45/figure10-1-19.png\"\n        srcset=\"/CR4-DL/static/6e3f5bd663e84c92ca6e8e304690c9d3/63868/figure10-1-19.png 250w,\n/CR4-DL/static/6e3f5bd663e84c92ca6e8e304690c9d3/0b533/figure10-1-19.png 500w,\n/CR4-DL/static/6e3f5bd663e84c92ca6e8e304690c9d3/c1c45/figure10-1-19.png 824w\"\n        sizes=\"(max-width: 824px) 100vw, 824px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Alternatively, we can use local spatial frequency filters to detect motion.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 825px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/f4e1c93fded0027f9d23246c1a29ab0b/d4c13/figure10-1-20.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 103.60000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAIAAADJt1n/AAAACXBIWXMAAAsTAAALEwEAmpwYAAADOElEQVR42j2UbVPbRhSF9bMSSDLTfOALDT+gmfRTCRnaZAglTTF5G3BKCgnC2AmiSFq9v2Is2ZJxmj+X52qn3dnZWa/Oufecu3dtNG3TzZa1nbd1XVuWVZRFO5/PZo2e0xmLrG3bKuU5jjO/mYM3AOnBZvF1MZ1OLy7+SdNUkyEIU0Yzv7lpmsbzPKUUh0KuKkk1GAwuL+0v59bR0fHJiVnVNVok23SWJAknYN6/75+wM804jheLBQDDdd2/j47evnu3vv7LnTt3t7d3/jo8DKMIJ5B9Pzi3rDdv3j579vT+/R9M87TX6xHiRpM9zx8Oh2tra8tLy5ubT4dnw93dXQ7regofh7bjHh5+WF5a2nj85MWLP1ZXV62LixpJTSPk4+OPv2/vDAaj337dXF66/fCnh2V5laYZADIHYYScH1cffPpo3rt779Gjn5XnXY3H15OJQezBYHhinq6vP75969bGxhPyUBXTHMRxEoYR/NevXiN46/nzlZWVg4P+cDQ6Oxte2raR5/np6dne3t7Ll3/u7/f7B/3e3qs0y6+vJ1wbX6Mo2d8/oJBbW9u9nsBGo88YpqgG6quqKsoS/VEUF0UZx6k2rCfiCZTnheuqorwCMJlUrfRFy1VV01rcZ3mOkbnc9lfa5f8OAUdtufZJVfHj32/fOkBHLvLcZ3iBOIziIIjYk0c3SV3P0hTlXG2K/zhJfT+khjhq5q1RliXGkoSmytIsy7OC/Xg8Rot02HSGnKIQtQD0Ckp8kRn31rmlXEW8rjGz8XjCPuCSuKUuJytairzkRPOjMGYadLnjOmEQosd2XeDcDcXDBfrZZFnBKprjhHuFkyaZ3H8QGNiVwWkU8ZskfBAQBhPxqaemkZkQnnB8jAqZD5SMt4IGOEw0J3ECFJoOJ0ZEXcA74aSLGBq2fUleyK5SKNMthSvgneacE2iJ1Dn4jyYYAuLZBkdaoDoTqxInPkLCQJjg4k6IYCRQ0mmMpGCKWndk1xWrslGKKvvdBjS9hTEQGscJKzJ5GIonTUbbdhDjuKoLJ2QGojq/oY5CTl1OXgUU/gycrtykVbowGhfI5URdvRJ9jlQahH8eHiOzqurvAK4QSxUTEosAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1.20\"\n        title=\"\"\n        src=\"/CR4-DL/static/f4e1c93fded0027f9d23246c1a29ab0b/d4c13/figure10-1-20.png\"\n        srcset=\"/CR4-DL/static/f4e1c93fded0027f9d23246c1a29ab0b/63868/figure10-1-20.png 250w,\n/CR4-DL/static/f4e1c93fded0027f9d23246c1a29ab0b/0b533/figure10-1-20.png 500w,\n/CR4-DL/static/f4e1c93fded0027f9d23246c1a29ab0b/d4c13/figure10-1-20.png 825w\"\n        sizes=\"(max-width: 825px) 100vw, 825px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>By using alternating regions of inhibition and excitation oriented at different angles, local spatial frequency detectors can be tuned to different speeds of motion.</li>\n<li>We would think that it’s easy to determine which theory of motion detection is correct simply by examining the corresponding neural circuitry, but this isn’t the case.</li>\n<li>This is because the motion-sensitive cells have many synapses into the brain, such as V1 or later, where it isn’t possible to trace out specific circuits neuron-by-neuron and synapse-by-synapse to test the theories.</li>\n<li>To substitute, we can build computational models and test them against psychophysical data obtained from humans while they performed behavioral tasks.</li>\n<li>But many of the computational models are consistent with data, making it difficult to judge which model is true without physiological evidence.</li>\n<li>E.g. Some of the models make identical predictions for all known psychophysical tests.</li>\n<li>If we extend the local region analysis of motion detection to the entire object, we run into the aperture problem because receptive fields are small.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 831px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/e67ef61b79b6bf9b287cc61ca7673dea/5b4a1/figure10-1-21.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB3UlEQVR42n2TaW/iMBCG8+crykKF1G5zOI7PBHL5SBzYv7dvDmi0H9Z6NR4cP+PxeIjCFMYxDOM2rLNccM5FnlOS54wLKE7Ssqqcc3YZ/VNRCGFctYxhDB5TCGlKjsdzllGlS6w47y1g58xTMzw+4WE+H87khjFJszQjOaWHwxHzvOgH6wfjEGIT/OiFrfILeTyeAIdpyin7+h1fb3XT9mYGwA//wk879b19e3uPk4wW3PsRpx3eT7iCULoDPoxmS2HWD/zib3X7cb58x2nbmV+nc06LcH8MYbILZv8Lz7wuq8+v78vlE5bQYrvtHl5Wfgq21/3xpyxvhORK67rtkO2mHY9rR6gKdq92X/m66bIsr5sGiaCKL7k9fH88QE73O+waAv61rgmhcZyicoiy8WPA4bvkfdQ0zdw0xkAdao0+8t4Yi9oa64xxcHr8NBavBYsdbW+huUmSJE3TjDFOKZpJI1ZRMCEkLKVMKU2LIskImrRgjBaYJV96Fk50vd2UUlVVoQ11WUqpuFj3USEl4zxFixZMVxV8QojAkCpfokRSIhIvy1LrEit4JJy/fEJMtTr4kyyBGIJiAS+P1l1gpcFX1RXJgBRCyTlVBh+OkJAC2fXWWMjg2m3Xz+rNX1e7UJt1FUUCAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1.21\"\n        title=\"\"\n        src=\"/CR4-DL/static/e67ef61b79b6bf9b287cc61ca7673dea/5b4a1/figure10-1-21.png\"\n        srcset=\"/CR4-DL/static/e67ef61b79b6bf9b287cc61ca7673dea/63868/figure10-1-21.png 250w,\n/CR4-DL/static/e67ef61b79b6bf9b287cc61ca7673dea/0b533/figure10-1-21.png 500w,\n/CR4-DL/static/e67ef61b79b6bf9b287cc61ca7673dea/5b4a1/figure10-1-21.png 831w\"\n        sizes=\"(max-width: 831px) 100vw, 831px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>How does the visual system integrate local motion?</li>\n<li>One elegant theory is if a straight edge moves across an aperture-like local receptive field, its precise direction and speed are ambiguous because the actual motion can be any vector along the constraint line.</li>\n<li>Constraint line: a line where all possible motion vectors exist.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 669px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/c06acf9f162c6ef2253c20c830c47f7c/99272/figure10-1-22.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 114.39999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAXCAIAAACEf/j0AAAACXBIWXMAAAsTAAALEwEAmpwYAAACpElEQVR42nWU2VLbMBRA/a1tf4DpC3Rn6EwhIUDxvsmyLdvx3vxhjyRI2erckRVZR3eX0+73T8U8/ThNcRzXtWrath+GcRwHLRPCp2merTgv4WEASbNse727+32vmk6IgmUYCyOv4b7rkQGlpVJhHItCfvn6LUrS9XBgjmYL8LKTeVkcgKPshxEpZMkO1XZnnz6/f/chSVPMUU2zrKtl5sWOr2BG4H4Y52X1/ODk5OPp6VkUxWVZG8u185Z8W3NeSDuHb7s9f3e3d0ma3bvubndTKwX2zGyLIfisAYLU69BMy4IhQRh1+17KsqwqSz7AR+wohKZpOxhZVkJK5nGSfv9xjvF/DgeL/VfzNC9W4CGZEL8sF+T86O3LaFtYtW2S5WVFeObHrIIvNtQ2T28HrFIN2jab7WZ7XdXK8DMKTZ1NBls5ht9LGJJqqZQq6/rX5RXlhVp2kOEojosCF1pOJNqEBUMQxxpsVhuS07ZdGMVhnGCCKApTrYr5xcVP1/WuNpi1pdr/wQjlKauaJFWmGdjh+X6nO2QG9oPA9Vy+0jM8YM/MtrVFnIQsL835aZq1Xcc+9lOhJgnLU9Ga0Wm64rE2y8oLQtKLTiPT02Y4ygPcjw/pHdg6z7inQ7quYLqPzCbb1UdrbZ41TNFmohC2ngqJY7iK3wgVYt1kBO721mc9MXU4OTT99c2tH0SeH7qeT4RzIXw/9IMwCMhRkmU5hckinqMp4YU69hXSISsIOtFPbfE5zXKyBaaNygU024HpClqDdTDgLM8dmpZz0lxwBPcGf2VVBSFqWdB0GEVshec0YBhWGQmEc+96XDrAWI7mKGHIacmIs0ypYABqU0sIgdCbTMm/w8F+iMP0bEyFEBiCQjwoEF5MWNEXqMmZvUnslTJO01/rsXR5Y2QixAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/c06acf9f162c6ef2253c20c830c47f7c/99272/figure10-1-22.png\"\n        srcset=\"/CR4-DL/static/c06acf9f162c6ef2253c20c830c47f7c/63868/figure10-1-22.png 250w,\n/CR4-DL/static/c06acf9f162c6ef2253c20c830c47f7c/0b533/figure10-1-22.png 500w,\n/CR4-DL/static/c06acf9f162c6ef2253c20c830c47f7c/99272/figure10-1-22.png 669w\"\n        sizes=\"(max-width: 669px) 100vw, 669px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>If the edges belong to a rigid object, then they must both be moving in the same direction.</li>\n<li>So the constraint line for all edges must intersect and contain exactly one vector in common, and this vector represents the motion of the object.</li>\n<li>Whether this theory is true or not is unknown.</li>\n</ul>\n<p><strong>Section 10.2: Object Motion</strong></p>\n<ul>\n<li>So far, we’ve only considered movement in 2D images but this is only the first step in motion perception.</li>\n<li>What we really care about is object motion in the 3D environment.</li>\n<li>Image motion must be interpreted to provide information about the motion of objects, which requires integrating image motion with eye movements and the distance to moving objects.</li>\n<li>Motion constancy: accurate perception of object motion despite changes in image motion due to viewing factors.</li>\n<li>E.g. Eye, head and bodily movements.</li>\n<li>How is the speed of an object perceived?</li>\n<li>It isn’t retinal speed because tracked objects are stationary on the retina but are perceived as moving.</li>\n<li>The perception of moving objects must be determined jointly by image motion and eye movements, resulting in approximate motion constancy.</li>\n<li>Velocity constancy: object speed, rather than image speed, is the major determining factor of perceived velocity.</li>\n<li>Experiments find that as long as the observer has good information about object distance, then they perceive its real-world velocity by presumably accounting for the distance.</li>\n<li>As with other forms of constancy, velocity constancy breaks down under extreme conditions.</li>\n<li>E.g. When the distance to the object varies a lot such as meters to kilometers.</li>\n<li>Three ways motion and depth are related\n<ul>\n<li>Relative motion parallax</li>\n<li>Motion gradients</li>\n<li>Accretion/deletion of texture</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 678px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/71f336d75112154f6831283dc2d013ef/38cea/figure10-2-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 117.19999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAXCAIAAACEf/j0AAAACXBIWXMAAAsTAAALEwEAmpwYAAACdUlEQVR42lWTi5KqMBBE+flb1uo+FIEkIKCQICC6+4n3JKNh15rCkExP9/SEZBgnies0z7eFmOZbjCtH13EMR7flGcv9LpFEJEm794/e8T9J0ny7LfdHXqiv/cENZE0RRtwfL7DHj1PdtF1v67ohOAYMoTGVUhqkKauu7ynHEVVXZgHfH9/zcoeaErGFuj2fL93j+wdkby2wl/4oO/SmtLl0fXWqyRZmeLKiOBxSyimt3eBEc+h8Sdx1FDyL/SH1Hc834QxtL9RCNqesBRZt82AJUPDgHEGB325LC9LnH3B0u7POVKcsL45Znh6z9nyhlnXD6VRjlVKGfZwXz1fm3549Y+L3HDXyZc5MUKaIgj+yPTioHV+axzmCF2sd/IiKRkj/iVjFs7eOYNRoe9vuWEvPvR0YwefXfrvb4Tn3Lba9MtOzpx39VZNCvAIGyYK27XD14Qa5fJCvbgN2wVmRTRILdghekUNOe+l4lUGuYBLPXf9vs1G6PNVNZA4qbMGgTSmKKBq9WJmj4WJVNI8dLOD+iH5eV3A0DNmnpi38PItCaRR6n91QNw2XDNkMn1Gzs8oGQ0/S86W3BAvp37vNkbtK5104cuH+eoHTnLx/fG7etpn/aFNlSshzvgBTFdrwZINrh9VleGKHLqvcO1C17TlRpkqzHFjptWnOeDuyMix4lj6bZvxTg9JVxQ6vjQeXngRmgVE1Vx4MJ00ChhkwtSggaaICGQnII2ZkOQceWSiphUIyWcBqAkFVN6YC6b8TivLNJ1DAQ4AkrdAlxbOwQ14W6oIU5VnQ4AmU9mCS9ulR8LFKeswZmA6EgiFYU1c2L50dp/k/mU1qAvtB2LEAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/71f336d75112154f6831283dc2d013ef/38cea/figure10-2-1.png\"\n        srcset=\"/CR4-DL/static/71f336d75112154f6831283dc2d013ef/63868/figure10-2-1.png 250w,\n/CR4-DL/static/71f336d75112154f6831283dc2d013ef/0b533/figure10-2-1.png 500w,\n/CR4-DL/static/71f336d75112154f6831283dc2d013ef/38cea/figure10-2-1.png 678w\"\n        sizes=\"(max-width: 678px) 100vw, 678px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Interestingly, the 2D motion of dots was interpreted as 3D motion by participants instead of motion on a flat plane.</li>\n<li>Whenever the pattern of 2D motion could be perceived as rigid motion in depth, it was.</li>\n<li>This suggests that something is special about rigidity in perceiving object motion.</li>\n<li>Rigidity heuristic: if there’s an interpretation where rigid motion can be perceived it will be.</li>\n<li>Kinetic depth effect (KDE): humans are sensitive to the rotation of a complex rigid object in depth.</li>\n<li>However, there are limitations of the KDE such as for smooth changes in object form.</li>\n<li>Vertices and endpoints appear to play an important role in the perception of rigid motion because they’re unambiguous except when viewed head-on.</li>\n<li>Since smooth curves don’t have unique points on them, recovering depth from them is ambiguous.</li>\n<li>This is just one example where ambiguity in solving the correspondence problem leads to unexpected perception of nonrigid motion.</li>\n<li>Two factors in understanding the perception of object motion\n<ul>\n<li>Rigidity of the object undergoing motion</li>\n<li>Availability of unique points to solve the correspondence problem</li>\n</ul>\n</li>\n<li>When unique points are continuously and unambiguously available, rigid motion is almost always perceived.</li>\n<li>When unique points aren’t available or are intermittently available, the solution to the correspondence problem is ambiguous. This results in different motion of the object at different times.</li>\n<li>Two findings of apparent rotation\n<ul>\n<li>The process underlying apparent rotation is an analog process that’s continuous and requires time to go through intermediate orientations.</li>\n<li>Apparent rotation must take place in an internal representation of 3D space rather than transformations of the 2D image.</li>\n</ul>\n</li>\n<li>It appears that the visual system is representing motion as it would occur in the real world and that there’s some maximal limit to internal spatial transformations.</li>\n<li>The preference for rigid motion only happens if there’s enough time for the analog process to complete the appropriate transformation internally.</li>\n<li>One interpretation of all these results is that the visual system has internalized the structure of motions that objects undergo in the real world to a degree that allows it to fill in the most likely motion in apparent motion displays.</li>\n<li>E.g. The visual system prefers rigid to nonrigid motion presumably because it’s experienced more motion due to rigid objects.</li>\n<li>Thus, the organism learns about the motion structure of the environment in the same way it learns to infer depth from 2D retinal images.</li>\n<li>However, challenging evidence comes from the visual system not internalizing all of the properties of motion such as momentum.</li>\n<li>A different interpretation based on the Gestalt principle of Pragnanz is that the percept is as simple as the conditions allow it.</li>\n<li>Review of common fate or the tendency to group units that move with the same velocity.</li>\n<li>E.g. A dog running behind a fence. We group the parts of the dog we see into one object.</li>\n<li>Common fate provides enough information to correctly perceive which regions are part of which objects and can destroy even the best natural camouflage.</li>\n<li>Induced motion: an illusion where a small stationary object is perceived as moving when it’s surrounded by a larger moving object.</li>\n<li>Two assumptions that explain induced motion\n<ul>\n<li>The visual system is more sensitive to relative motion between two objects than to absolute motion.</li>\n<li>The visual system uses a heuristic of assuming that larger or surrounding objects are stationary.</li>\n</ul>\n</li>\n<li>The induced motion illusion is broken when the surrounding environment is perceived as stationary.</li>\n<li>Anorthoscopic perception: the perception of an object through a small aperture.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 807px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/e7a8850b3a7738f06fbaefc3129aef0d/d2a60/figure10-2-21.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 98.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR42k2U2VLiUBCG85jOheUAmuBaA+rMndfsKCEkZN9AshkRH3C+c4KWXV2hc9J/L3/3QanepNR1o/X7e1GWURzbtm2tEds0zTRNPw6Hbx+hEqL8PAJZllUYRs/PL9PZbDadjSeT0Xg8GAx0XS+KAkgltRHlO0yjeVEkafr09KR1tXa7fX19ff9w3+12MdJNuv/4EOCqKqUqWMdg77XnBaSN47jX652engI4P79QVfXy8uru7o6gAixFgMtSaWIIqd9432y29Hl7e0e2npA+RlfTeHi+T2mkwh9eBPgd2e9Rkmd5zqllre/792RD1Qu136fsy5ubG6rCh85xpgTRM1W6nusHge04YRBCcpZncZLM58/T6UxfGrq+jOJkt9tttlvTsiAfZ9O06EJhDLwPBsPhcATJvu/neX44HMIoGo3G8/lLEARkOnx+7rJsNBwyBOKOxxMcRNmC5FyQzJNp0bbreRT/slhMJlOSLJcGJwyiEKRsMI6E0UYp+BZPYlMwrn///bu4UM/OfquqRs/w1W53HNcFBS/AwKNKAxPzYkf2e2qm8j+93snJr1ar3Wp1xKw0IZ7nve52AizYbsB1fZyWXBXA0HZ7cwvg4fGRKOQHf3V1vbZtCIfOL3AuwD83jlPXdfv9Plk77Y6maiRudzrMOYojJkTbUiX4e90qyYHEV+zJfD5nsWEVcoej0co0qRm3THaLUoJCnzBcf0mz59yhLMuSJHFdsbF02/DadFdWpdywQmE3+BwEwsP3A1aMYbA5Scrs0u12m6YbbKYglj5JGDsfMFCFwSx0fWWsDMOwbQePdXOTbW4yvzbnxmqFCRcrc7VY6Lxa9tpxHIVslmXxwaEGh1l6wIThea7jEoRUtjxBlobh+h79c05ghUWl3DAMBd5xOI2isIkIBsOT4hCLp3u8BeJ/xrIUArCl8q5VzeoU8skyoN+G1IwVhPPt6yvGLsv+A8yA2ThEw4ofAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.2.21\"\n        title=\"\"\n        src=\"/CR4-DL/static/e7a8850b3a7738f06fbaefc3129aef0d/d2a60/figure10-2-21.png\"\n        srcset=\"/CR4-DL/static/e7a8850b3a7738f06fbaefc3129aef0d/63868/figure10-2-21.png 250w,\n/CR4-DL/static/e7a8850b3a7738f06fbaefc3129aef0d/0b533/figure10-2-21.png 500w,\n/CR4-DL/static/e7a8850b3a7738f06fbaefc3129aef0d/d2a60/figure10-2-21.png 807w\"\n        sizes=\"(max-width: 807px) 100vw, 807px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>If the observer fixates on the aperture, the object’s contours are all presented to the same region on the retina but are spread out in time.</li>\n<li>So the visual system can extract contour information and integrate it over time to form a coherent object from the presented fragments.</li>\n</ul>\n<p><strong>Section 10.3: Self-Motion and Optical Flow</strong></p>\n<ul>\n<li>Retinal motion due to eye and head motion differs from object motion because it causes the entire visual field.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 832px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/331a7bddfceca7f0b5456875177aab1f/ef6b9/figure10-3-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 85.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAACFElEQVR42l2T63akIBCEff/f2SQ6KoKKoHJzXnC/ltlkzmY4SqCrurq6bVKOMcVcit8PNel13Tbn2e/HwcVxnsdx2m1brV1X3s7ajfOr8Hs2NzilXI4zaDMvRDhPBNEgoXB+H8ZRTZM25uvrm3cI8bqe5bp+wJk4MgOFoOsehGttzpBQAeERwhlC3w9QsAH5Di77IZnJsywrOL/vy7pyBdhaBwAhIkFNL3B5A58hotlTod2AUbWAY3LOnwjIGTBYo2dkv4Nfhn233TCqRz9Q4eYcsj8/v9quu+ks7ATwL1wgfzPnciEY2bhFVWaeMRl7uVrsSnRIaT9OzumEhP8HxifoSaIFuwDmzZXYTidCrLzsY0jIzhUcbipagWZ190TeamKBgW7fBU6XYUX8q1UVnDLDEOhNR7lYYmYyUDCwKt577xz+WVbAIdwr5Vc24LbtmAGBjwokDUM8VgvXZJTS/TDU+aN4NEPQXM98PS9sYLoAYDLL3zpZMeefPLwoL8hZeRk2jsOj7xkJKrSb57oimSoysJE5vxd7Bkmeu9DGmBvEfHz8wSRGEs3kZy6pvO0ebHBuGMZlXqQWM1MCh+09vBTVYKAYO2mZykk+DcrF8vop1C8CC+Z/pDzVdHdTwMZUhwmalCzucZiTBzmXVQzTYr7EaIO67hZFTIN9/NEV7rb7k6obXIUOeoJ4Vv95okPmLeeU8l/n9Zmjl8tG+QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.3.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/331a7bddfceca7f0b5456875177aab1f/ef6b9/figure10-3-1.png\"\n        srcset=\"/CR4-DL/static/331a7bddfceca7f0b5456875177aab1f/63868/figure10-3-1.png 250w,\n/CR4-DL/static/331a7bddfceca7f0b5456875177aab1f/0b533/figure10-3-1.png 500w,\n/CR4-DL/static/331a7bddfceca7f0b5456875177aab1f/ef6b9/figure10-3-1.png 832w\"\n        sizes=\"(max-width: 832px) 100vw, 832px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Optic flow: global patterns of retinal motion.</li>\n<li>We use optic flow to navigate through the world and control our actions within it.</li>\n<li>Induced self-motion: the perception of moving through the environment because of the pattern of optic flow on the retina.</li>\n<li>Sometimes, induced self-motion is illusory because your surroundings move.</li>\n<li>E.g. When sitting on a train that’s beside another train, if the other train moves, you may perceive that you’re the one moving.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 830px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/afd8e57483b385c5dfed826c6f4eb16c/715a3/figure10-3-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 79.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAACC0lEQVR42nWTiXKbMBCGefC4mbY2cU18cF86AIlDCGzwE/YXJJ7WTTU7Gknsx/67WllN27Zd13aq7xfTvVKdlKJtG0JImiaqV1LK8Toaw/THsAypVAuw77XWUtbH4wmYe7nsdvaP7z/PpzOljFIKzwd6W4a1kgiMoHoYi7J62Xy7XFzP8w/OyXGO+FeWE88PCGXTND3IFe4N3GvASuuuU/v9Lz8Iez1QVvCyQl6c8zwnRVF8Aa9hjehhqJvG94NKSGyx2G53QRCCz7K8KErA4zO8kCsMP2jmRQknx3nfbF5RT1nXiFyW5XPkNewjMlwRkHGOCtr2HtYpJaSMkxSy5/v8BfwZWcM1CCLG4Vm+H89RlHBeIJckzRjj0/x/GAbY8wLIvk3zxfXDKIb+JWcUm9/+lf0gIRs71/XSNMOd7XZv260NiDKGE0iYnyKjtz46Sw8w8Lb9hpqlWY7LDsIwimLAh4ODTO73+1+wHq/DYuP1tixGZJgTUlZVTihaDcVDCSohgM3LmD6HFUQx8oEfehBSQSJhzKiwkKJuWoFVXUsctS2aHE9BoSnNg+iss+v5fpikaRwn4BEzwcbUh6JacZKYkzTFnOV5lmUmCGNQhOuwjDbKYPBAPXEfuBX44RsI8PjLCqNJjbplYC2EsHCMUIRQ6DKVQ6t3H++zMU8Vm7Wi/TAMy4WgLOM6/wZ8HGPptCZe+gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.3.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/afd8e57483b385c5dfed826c6f4eb16c/715a3/figure10-3-2.png\"\n        srcset=\"/CR4-DL/static/afd8e57483b385c5dfed826c6f4eb16c/63868/figure10-3-2.png 250w,\n/CR4-DL/static/afd8e57483b385c5dfed826c6f4eb16c/0b533/figure10-3-2.png 500w,\n/CR4-DL/static/afd8e57483b385c5dfed826c6f4eb16c/715a3/figure10-3-2.png 830w\"\n        sizes=\"(max-width: 830px) 100vw, 830px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Almost all perception of self-motion within a visual environment can be thought of as visually induced self-motion.</li>\n<li>The visual system can detect slight changes in head position and orientation with great speed and accuracy, enabling much tighter control of balance than is possible without visual input.</li>\n<li>So visual information contributes to balancing the body.</li>\n<li>Eye rotations add a component of motion to the optic flow field, resulting in a complex combined optic flow.</li>\n<li>Given that the direction of self-motion can be determined visually, the other component required to specify one’s motion through the environment is speed.</li>\n<li>One hypothesis on how we perceive speed is the time to contact an object.</li>\n<li>E.g. The faster the image of the surface expands within your visual field, the shorter the time until you will contact the surface.</li>\n<li>VR displays are an existence proof of the adequacy of visual information to specify a 3D environment given the implicit assumptions the visual system makes when interpreting optical events.</li>\n</ul>\n<p><strong>Section 10.4: Understanding Events</strong></p>\n<ul>\n<li>People have a strong tendency to perceive moving points as though they are attached to a rigid rod moving in depth.</li>\n<li>One important component of motion events is their causal structure of how the motion of one object interacts with and affects another object.</li>\n<li>Launching effect: when the a moving object imparts momentum onto another object.</li>\n<li>Three conditions of the launching effect\n<ul>\n<li>Timing. The other object must move within 200 ms of being in contact.</li>\n<li>Direction. The other object must move off in approximately the same direction.</li>\n<li>Speed. The other object must move slower or at the same speed.</li>\n</ul>\n</li>\n<li>Only through the dynamics of interaction (collision and contact) does the mass of an object become a visible property.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 810px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/da6886f6b71579ec271cbd5c332a16e3/d7542/figure10-4-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 79.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAACbElEQVR42i1S2XKjMBDkz/eobPYpiW1iAxJIoPtEHPb/bWNv1ZRKiOmZnp6utn133hNCCSHW2n3fl3Vdty3NmVDaUeJDzKWkPMc0x5w7Qs+XWmod57kKKfuQpDRCGqmNtT7lMuHDemOD1BZ3ZR2+lbHahVGqSWo2TnisTuf6crkiGU9oobTxPhrr8Pvtz7txXjsvlOn52JD+99tfNomW9A3tUa76/DzX9Q3FGRfoiTYhJosOQtbfN1BW2gGMYKP4utST0sM49XxSxlXGBW2c0hZ4pZxUJuU8TmIUCgDktAQkQN9NUmEuoa3QbsSDj1VZt3lBrKWsy7pBAgw+sJHQ4dZ0tGe05/W1QZ9c1pDnV0CYkOYDnMsyl6UAv6zLtiGeFdf/dcualyXlJaYCgA3R+IATJSpIjcGsDZgT40GvPBcQxidCSDsMB2l0noTBdC5EoY/5cYFgJ6g9CY1p87yAMwRDxYGPUNuFdOxJ6+eMGf9+/PyFPZN++L421cfn6fvaoicfZZoXdIzpUBu9MXOaC0DAQGR3EPYfX6d+4ChNB149nYDa2IcFSfSHpQhlfBRYG5KQyieYCGprWAPrBQsupIux2h+P9f7Y7vft/lj3u3UePbGnrqMICugobi0Bn7Lu+RBvTWUJc4lzqbASLIZzgW6wJMAIkHc+gIJz4AUrRHlUlOgMtmgLn+Gszuf61rQD49crzhF0YX1C+470OPEOYTo60IEh2o7Cm8A3hDYtqZAwThPcD9jAGM5JCJABDFfGIMxxP0Q67rylPenZq1yFDNTvnxnwk9YGaMaPx9cFLnv6jAmlYayXSQ6f+PAPSB9lENy3SosAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 10.4.4\"\n        title=\"\"\n        src=\"/CR4-DL/static/da6886f6b71579ec271cbd5c332a16e3/d7542/figure10-4-4.png\"\n        srcset=\"/CR4-DL/static/da6886f6b71579ec271cbd5c332a16e3/63868/figure10-4-4.png 250w,\n/CR4-DL/static/da6886f6b71579ec271cbd5c332a16e3/0b533/figure10-4-4.png 500w,\n/CR4-DL/static/da6886f6b71579ec271cbd5c332a16e3/d7542/figure10-4-4.png 810w\"\n        sizes=\"(max-width: 810px) 100vw, 810px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Two heuristics to visually determine mass\n<ul>\n<li>Ricochet heuristic: when the incoming object ricochets or moves backwards at a higher velocity than the forward motion of the stationary object, the stationary object is heavier.</li>\n<li>Clobbering heuristic: when the stationary object moves off with a higher velocity, the incoming object is heavier.</li>\n</ul>\n</li>\n<li>Intuitive physics: the nature of people’s beliefs about simple physical events.</li>\n<li>The perceptual intuitions tapped by viewing simulations seems different from the high-level cognitive abilities shaped by formal training in physics.</li>\n</ul>\n<h2 id=\"chapter-11-visual-selection-eye-movements-and-attention\" style=\"position:relative;\"><a href=\"#chapter-11-visual-selection-eye-movements-and-attention\" aria-label=\"chapter 11 visual selection eye movements and attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 11: Visual Selection: Eye Movements and Attention</h2>\n<p><strong>Section 11.1: Eye Movements</strong></p>\n<ul>\n<li>We focus our visual resources more on some objects than on others, resulting in a more vivid visual experience of them.</li>\n<li>Two types of visual selection\n<ul>\n<li>Overt: external and observable.</li>\n<li>Covert: internal and unobservable.</li>\n</ul>\n</li>\n<li>The fact that our eyes are facing forwards is a form of selection because the human eye doesn’t register all available light information.</li>\n<li>Two functions of eye movements\n<ul>\n<li>Fixation: positioning objects of interest on the fovea.</li>\n<li>Tracking: fixating objects on the fovea despite object or head movements.</li>\n</ul>\n</li>\n<li>Moving the eyes instead of the head or entire body is more efficient because it’s faster, more precise, and uses less energy.</li>\n<li>Nystagmus: small involuntary movements of the eye.</li>\n<li>Nystagmus is important because if the eye is stationary, visual patterns disappear.</li>\n<li>E.g. The perception of stabilized images fades within seconds.</li>\n<li>The eye is not a biological camera that faithfully records static images because almost immediately after light is captured by photoreceptors, adaptation and motion processing take place.</li>\n<li>By the time information gets to the visual cortex, very few cells produce sustained responses to unchanging stimulation.</li>\n<li>Saccades: rapid and abrupt eye movements that bring new objects of interest to the fovea.</li>\n<li>Once a saccade is in progress, its destination is fixed.</li>\n<li>It’s interesting that we don’t perceive a moment of blurred vision while the eyes move.</li>\n<li>Saccadic suppression: visual blurring isn’t perceived during saccades because vision is suppressed during a saccade.</li>\n<li>Saccadic suppression hypotheses\n<ul>\n<li>Perhaps the image motion from saccades is so fast that it isn’t perceived, but experiments have found this to be false.</li>\n<li>Another explanation is that the image before a saccade masks the vision during a saccade.</li>\n</ul>\n</li>\n<li>Smooth pursuit eye movements: when the eyes track a moving object.</li>\n<li>By tracking an object and thus maintaining a stationary image on the retina, the visual system can extract maximum spatial information from the image.</li>\n<li>Pursuit movements serve the important function of helping to identify moving objects.</li>\n<li>Differences between saccades and smooth pursuit movements\n<ul>\n<li>Smoothness. Pursuit movements are typically smooth and continuous rather than jerky and abrupt.</li>\n<li>Feedback. Pursuit requires constant correction based on visual feedback.</li>\n<li>Speed. Pursuit movements are slow compared to saccades.</li>\n<li>Acuity. Both movements are clear for the focused object and blurred for unfocused objects.</li>\n</ul>\n</li>\n<li>Eye movements can differ depending on the target’s motion.</li>\n<li>E.g. Both eyes move in the same direction for side-to-side motion but move in opposite directions for depth motion.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 512px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/5ae18bc74406975a37957a8306ac6906/01e7c/figure11-1-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 170.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAiCAIAAADQyG7qAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC90lEQVR42o2V6XLqMAyF/fDcX6XtUEIWxxAgu9m6TN/vfraoa1Lu4tGYJOhIR/Kxrc6Xy+l8Zha7vL7udvu8KHb87PfGrNfrzWZTbbfb2E1MTcCC//j8zPNisXh5e38/nk5d3+/rmu+Y+DvI+awmMIw/3j8+Cq2zPAfM934YYCEOAYyp8BRH4ZVsbdfJX8M4Tv49HI8wugMmvD0c2rbrx2EYRvzENQbzegrgCXl4jtYSYufZAp74AL7JHAxw07YACCFgosQOgrwPBtZB2VpKJQoPkjkGSy03NctMHjDMUKBtPE+iC9hlnrTx9e2NJQUjq4LTpqqkWzG1A4PMsUgk1cPDvO8HAcN5Pn+Uym84+6EmfMiDNmmVgOGMVJBn6JNUewXHfDDpE4ZCRCfM1H+D/BP4ulRNu0rT5TKRbrn2BNjfwWQA71TtdX79+J9gSEpaNmOyWqVZpsvS+vZ6czBeUaCK2xgeqLnv+1+zWdM2I8PrBAMzWjE72uGOwlxvzicCz2a/2B30Slb18I08DC7iPfB1Gw4Dp0frRW59hdb+AP+sWTRTNw2G2iRnRPhfYKxpGjzQlv3iLHj2qgjhPlhUSU6ExYIjEicPP6bgn91i5hiBMwcoNddNK9oKzL/wHhwWKQgYtqItRM6pGzbDLXhUkxXGyMD2CGKqqiowvwOO01IwolomiZwKzJzBT8/PCCtWyHfNE20lyYqLQs4KrKq2i8Viv69RWVinwY1eST3hiIg3I0eCe+462oZabbTI9KIHfIm3jqfASQRDjgRunJeXJZm9z+n4tTHdrvB6U1xjbkm6joVlph5mOTSFXudf0QxEPBdHqq5ZwUY9PT0/zh+5C9mG9IbaVnQsTbXWbEaSZ2mWphkU4KJLTTvYp6vEOShWxRjDLcpLnuemNJoYGI6lKUsDgnuWLV0aQ1hCuJEXfFB8dY5lSXicsszl4T0vdOHx7jRwCC3UyIo/bjwrCtZ+kJ8QjppxY23IZojPDHngpPVENJGoqyzNbxgSO7hg8YVLAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 11.1.5\"\n        title=\"\"\n        src=\"/CR4-DL/static/5ae18bc74406975a37957a8306ac6906/01e7c/figure11-1-5.png\"\n        srcset=\"/CR4-DL/static/5ae18bc74406975a37957a8306ac6906/63868/figure11-1-5.png 250w,\n/CR4-DL/static/5ae18bc74406975a37957a8306ac6906/0b533/figure11-1-5.png 500w,\n/CR4-DL/static/5ae18bc74406975a37957a8306ac6906/01e7c/figure11-1-5.png 512w\"\n        sizes=\"(max-width: 512px) 100vw, 512px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Saccadic, pursuit, and vergence movements are used to keep the image fixed on the fovea when the head is still.</li>\n<li>Vestibular and optokinetic movements are used to keep the image fixed on the fovea when the head moves.</li>\n<li>Vestibular eye movements are extremely fast and accurate compared to pursuit movements.</li>\n<li>E.g. Focus on your finger while moving your head versus moving your finger.</li>\n<li>Your eyes must move to fixate on an object when you rotate your head.</li>\n<li>While vestibular movements are driven by kinesthetic signals from the inner ear, optokinetic movements are driven by optical translations of the entire visual field due to head motion.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 812px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/4fa746ac65bb7adb890af01756442145/63ec5/figure11-1-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABnUlEQVR42k2R65KdIBCEff+qJH/Wo4KKHFEEEVBfMB9uKmepLmq49HTPTHXddz7P87o251LOHAHHdJ4R5PyNI6b/CDHtR/ThqPh33vc4TWZZRqWathVlyRDjkdI3k0RwnA/hB/NDfrXte54hsK/WLus6qmndtvO6Y8qb929jJv2GfqQcfpLzdcl+mI3B/zAq0fcxJSGlnme/B00+s6BsnTPLys3md5huD//IECatcb6HYz8ONWmzrnAAOsV2ylhA/D0baB8yVaH5MPkZm6al8vr1WqyFiJTbd1ACv9vNfZTT09KnJZFqgd02ql1Wi8l5KeIUjCB1EXey17OhZ6SoGAzMlMu0ytieKgB8IXuqyOfFK8B5GVXK/oiu2AkVyR4n3nl8Bf84pGx2i0vs+WLYPs/ECK6b4wiqr7r+9fsPvR2UksNAb5l2J8RX/Wo70Q9jJyT940jAuStfR8zzXPG7H1hKac3GeKXsx1ENw8j8+N0KyeWrIZUkrks7u6YTxBU6aioERBgsiaAC+eSj84yHulZbWkirmCc3Ss+D0n8BnGfURCgsAasAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 11.1.6\"\n        title=\"\"\n        src=\"/CR4-DL/static/4fa746ac65bb7adb890af01756442145/63ec5/figure11-1-6.png\"\n        srcset=\"/CR4-DL/static/4fa746ac65bb7adb890af01756442145/63868/figure11-1-6.png 250w,\n/CR4-DL/static/4fa746ac65bb7adb890af01756442145/0b533/figure11-1-6.png 500w,\n/CR4-DL/static/4fa746ac65bb7adb890af01756442145/63ec5/figure11-1-6.png 812w\"\n        sizes=\"(max-width: 812px) 100vw, 812px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Optokinetic movements are like a smooth pursuit plus a saccade to reset the eye’s position.</li>\n<li>In terms of neurophysiology, the different types of eye movements are controlled by different neural centers.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 811px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/31008b0adedb48ad6519f54bab1c2f7f/fd28b/figure11-1-7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 98.8%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAADJUlEQVR42kVUiW6bQBD1f6ZSm1Rq2sSJlKRp0lQ+YmM78YEPvCw3u9ywBuz2B/sWWy0awbLM25l5b4ZW1lx5nnPOPc/DPZVvhaqqijJinGdFgY0drrLaleU/K6uqhQ9pBgfhM9btdqlpprmI04wa5mK58niQ4hMPlqvVRtsGYSR2u0IIIY+qWmlewPJC4Ij5QrU9n4WR5bi6aVHTtlyPBSHj4XS2mM5V23ELsYOzQOiqluCsAWNXOs1Vw7JN29F0qq43xDC1LfEYj5I0gWchqnpf1XWDPYILAbzrenAaKqPXgWK7nuMxBMdCXa7bN7e6YZZ1TQ1LXa62hCDSCZyLneszx3bDKFlr28FA4WHkMr4hOiI//ni6vPz29PyCTUeetdIpPaWNhEEPKnUcF8Qg+PtsPn6bIvnZYkGocXV9fX5+0W7fYF3W+3p/2B9+H5M/ReZBaNnORiMgDLDJ+7TT63c6vW6/f3V1fXb24eLiM9Ep48F6o63WG9O0GqkKcWQP3CdZrhHq81AeYTvv0/nd3cPT88+Pn86/XH6lppWkGbogBnlpKiODKiCx5fm+KCvwRHRjqxuEmlsdYtuvg+H9w3dlNAYvPuPT6QxpAnmKDCQhFDTYtguRwTDkDeIkzvKhMm63b4ej8UJdukgHnYOF64OtRuesgCKUmuie1VobKpP1ZtsEp53e6/PLL5ASJqlGdHSIxxhqXq7WjPH94SAJA1XgYKPJ3fuHR+isjN56/QE452GM9ohlA4tcoB1ySg3oDJZB/P+0CdGdJltYlKY8isG5MpqMJ29ICsQcW7reQ6k/TZNVLegEnsAZsgJ3OAttiGpTscNUmI6L3i7kMEiDm2jGCwVD6hYw4BAEJkmCCHD1gzCMk6MFkUw7jGLI808kGNZwbs3nC5iOUUTpLi4PVVHDIFInC2uUI++6Dqot28Yr6ERHwh2RPYA09JbtYg4gLCRBJ2Iq4ex4Ps7AwYDJ/mUMK79BAt+icugwPw7eYVIl0wIYD3zCSCMDw4DwHuAI3hwi4+HRQiqGnDwfW/CWMW3JAu7HWiAuakFsZBDKP0l5pL0Qu7/6fNx98Qo1ogAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 11.1.7\"\n        title=\"\"\n        src=\"/CR4-DL/static/31008b0adedb48ad6519f54bab1c2f7f/fd28b/figure11-1-7.png\"\n        srcset=\"/CR4-DL/static/31008b0adedb48ad6519f54bab1c2f7f/63868/figure11-1-7.png 250w,\n/CR4-DL/static/31008b0adedb48ad6519f54bab1c2f7f/0b533/figure11-1-7.png 500w,\n/CR4-DL/static/31008b0adedb48ad6519f54bab1c2f7f/fd28b/figure11-1-7.png 811w\"\n        sizes=\"(max-width: 811px) 100vw, 811px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Review of the three pairs of extraocular muscles for left/right, up/down, and clockwise/counterclockwise movement.</li>\n<li>The control of voluntary saccades is traced to the frontal eye fields located in the frontal cortex, which project directly to the brain stem gaze centers and indirectly to them through the superior colliculus.</li>\n<li>Smooth pursuit movements require constant visual feedback using information from areas MT and MST.</li>\n<li>Vergence movements originate in the binocular disparity channels of area V2 and project to the midbrain region of the oculomotor nucleus.</li>\n<li>The vestibular-ocular reflex is driven by a three-neuron reflex arc from the output of hair cells in the vestibular system to the vestibular nuclei to the oculomotor neurons.</li>\n<li>The control of extraocular muscles is distributed in different parts of the brain that specialize in different kinds of information.</li>\n<li>People explore a complex visual scene by making a sequence of saccades on different parts of the scene.</li>\n</ul>\n<p><strong>Section 11.2: Visual Attention</strong></p>\n<ul>\n<li>The visual system doesn’t passively process all the information available within the image.</li>\n<li>Instead, we selectively attend to different aspects of it.</li>\n<li>E.g. Entire scene, selected objects, specific object part, or a property of a particular object.</li>\n<li>Attention: our ability to engage flexible strategies for processing different information within the visual field.</li>\n<li>Overt eye movements determine what information is available to the visual system, while covert attention determines what subset of this information gets fully processed.</li>\n<li>Two properties of attention\n<ul>\n<li>Capacity: amount of perceptual resources available for a given task or process.</li>\n<li>Selectivity: amount of attention given to different subsets of visual information.</li>\n</ul>\n</li>\n<li>Covert attentional changes seem to be like movements of an internal eye, the “mind’s eye”, that can sample different locations of the retinal image or afterimage.</li>\n<li>Spatial selection is only one aspect of visual attention as attention can also work to perceive different properties or features of the same object.</li>\n<li>This demonstrates the property selection case of attention.</li>\n<li>Why do we have the ability to selectively attend to different aspects of visual information?</li>\n<li>One answer is to prevent the visual system from being overloaded.</li>\n<li>Attention is likely a means of selecting the most relevant information for further processing to achieve the organism’s goals.</li>\n<li>Paradox of intelligent selection: how does the visual system choose important information without first processing all the information to determine what’s important?</li>\n<li>This is a paradox because if attention operates early in the visual system, then it’s unclear how the attentional system can determine what’s important. However, if attention operates late in the visual system, then the advantage of selective attention is lost because most of the irrelevant information has already been processed to perform the selection.</li>\n<li>Selective attention to important information is possible by using heuristics based on either innate principles or heuristics learned through experience.</li>\n<li>E.g. It’s evolutionary advantageous to attend to moving objects.</li>\n<li>This attentional heuristic might even be hard-wired at birth through natural selection.</li>\n<li>Experiments in auditory attention using a shadowing task found that gross sensory features could be perceived without attention, but specific features weren’t perceived unless attention was diverted.</li>\n<li>Review of Broadbent’s filter theory and the cocktail party phenomenon.</li>\n<li>The explanation that best matches the evidence is that selective attention operates in both early and late auditory processing.</li>\n<li>Initial selection is based on gross physical properties where early selection only attenuates/reduces signals from unattended channels rather than blocking them completely.</li>\n<li>E.g. Voices over music and random sounds.</li>\n<li>Late selection is based on salient stimuli.</li>\n<li>E.g. Your name or moving objects.</li>\n<li>Review of inattentional blindness.</li>\n<li>Results from the inattention paradigm suggest that some form of late selection occurs in both visual and auditory attention.</li>\n<li>Attentional blink: perception of a second target item is greatly reduced if it’s presented within half a second of the first target item.</li>\n<li>The attentional blink is typically studied using a rapid serial visual presentation (RSVP) paradigm where participants are shown a rapid sequence of visual stimuli.</li>\n<li>E.g. If the second target is presented within 200-500 ms of the first target, people likely miss the second one.</li>\n<li>It appears that the attentional blink, like inattentional blindness, occurs at a late stage of selection.</li>\n<li>Change blindness also supports the hypothesis that lack of attention to an object causes failure to perceive it.</li>\n<li>Change blindness: people’s poor performance in detecting changes to unattended objects.</li>\n<li>We can test change blindness by alternating between two scenes that are identical except for one object or feature.</li>\n<li>If the two pictures are alternated back and forth, the task is extremely easy. But if there’s a brief blank scene inserted between the alternation, the task becomes extremely difficult.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 500px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/d37266af10f1c1e5229016151f30eec3/0b533/figure11-2-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 173.60000000000002%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,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'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 11.2.5\"\n        title=\"\"\n        src=\"/CR4-DL/static/d37266af10f1c1e5229016151f30eec3/0b533/figure11-2-5.png\"\n        srcset=\"/CR4-DL/static/d37266af10f1c1e5229016151f30eec3/63868/figure11-2-5.png 250w,\n/CR4-DL/static/d37266af10f1c1e5229016151f30eec3/0b533/figure11-2-5.png 500w\"\n        sizes=\"(max-width: 500px) 100vw, 500px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The main result is that we sometimes fail to see things that are clearly visible.</li>\n<li>An alternative hypothesis is that we aren’t blind to unattended stimuli, but rather we forget unattended stimuli. This hypothesis argues that we have inattentional amnesia rather than inattentional blindness.</li>\n<li>Three forms of unintentional inattention\n<ul>\n<li>Inattentional paradigm: lack of expectation.</li>\n<li>Attentional blink: lack of resources.</li>\n<li>Change blindness: some distraction.</li>\n</ul>\n</li>\n<li>But what about objects that are intentionally ignored?</li>\n<li>One experiment found that shape wasn’t perceived unless it was attended to.</li>\n<li>It seems that attention doesn’t only facilitate the processing of the attended object, but it also inhibits the processing of ignored objects as supported by the negative priming effect.</li>\n<li>Negative priming effect: when the presentation of an object is intentionally ignored, it slows recognition of the object in later trials.</li>\n<li>Attention is a double-edged sword where dedicating more visual resources to one object, location, or property comes at the expense of fewer resources for unattended stimuli. This can be a problem if the unattended stimuli are important such as predators.</li>\n<li>The involuntary summoning of attention to new and salient stimuli feels different from the voluntary and effortful process of directing attention.</li>\n<li>Two types of attention cues\n<ul>\n<li>Push cues: cues that act on voluntary attention shifts.\n<ul>\n<li>E.g. An arrow pointing where to look.</li>\n</ul>\n</li>\n<li>Pull cues: cues that act on involuntary attention shifts.\n<ul>\n<li>E.g. An unexpected flashing item.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Differences between push and pull cues\n<ul>\n<li>Pull cues produce benefits without costs, while push cues produce both benefits and costs.</li>\n<li>Pull cues work faster.</li>\n<li>Pull cues can’t be ignored but push cues can be ignored.</li>\n</ul>\n</li>\n<li>Experiments on attentional cuing clearly show that attention has measurable effects on tasks.</li>\n<li>How do we shift our attention?</li>\n<li>Three steps of shifting attention\n<ul>\n<li>Disengagement. The first step is to disengage from the currently attended object.</li>\n<li>Movement. Once disengaged, attention is free to move towards the new stimulus.</li>\n<li>Engagement. After reaching the stimulus, attention must be reengaged.</li>\n</ul>\n</li>\n<li>Evidence from neuropsychology suggests that these three steps are controlled by three different brain centers.</li>\n<li>E.g. Patients with parietal lobe damage have difficulties with disengaging their attention from objects. Patients with superior colliculus damage have difficulty moving their attention. And patients with thalamus damage appear to have difficulty engaging their attention.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1000px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/8dd902d85373021b659e1e727c748cd0/58354/figure11-2-12.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 59.199999999999996%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABw0lEQVR42k2S2XbaMBCG/Yw8RblMcpM6FyUnwbTQU0xMvIH3VRJeIC/YT3ZPGp1BjOT59I9mZNRti7VCD6UUblVVeVFgWZY1TXO93YZx7LquLKswDKUU4zj2fd/1vVG3ohESE1LVTTNhZZbn+E3bxknyY7UqynK8XtXlgsNmPwyQl64zGvEfhknSNM0ygpIkhQdYLBbL5RKHZVlVM/kJa1KqC0wQhucoCk8n53iM4pgkScSyrMPhLYoiTuTTTGIcp2EyV13/5jgvL6+O4/zZ703TXFvW90fz13br+X5d134QkDAAghjOP1jnrFRV1+bT03q95p74KLuuyy1g0LcPB2ZSI/kZlkppuJWqFZKtqqp3u9+2bb+77t627+7uLWvjet52t2OmZsTAzLJSSmOWlZNxE6QQWT0/f9Njudn85JT7hwdKNQzDnDORGpFialWrSz3x+koM2g5GqUiEEpzO534q76yJCQUiDJ2zJi+fxUAfER4GTfB9n/rfPj7Y+tqkSVwaXhCeKRGvKcv1j8HbynM6Nx/HEn9+c+kUQTgOs3F0Pef47vkBJUHHD0K6QlVZFEXBm6XsiBMaxwn/2uKYnmd59heOuMTwK3ijKwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 11.2.12\"\n        title=\"\"\n        src=\"/CR4-DL/static/8dd902d85373021b659e1e727c748cd0/00d43/figure11-2-12.png\"\n        srcset=\"/CR4-DL/static/8dd902d85373021b659e1e727c748cd0/63868/figure11-2-12.png 250w,\n/CR4-DL/static/8dd902d85373021b659e1e727c748cd0/0b533/figure11-2-12.png 500w,\n/CR4-DL/static/8dd902d85373021b659e1e727c748cd0/00d43/figure11-2-12.png 1000w,\n/CR4-DL/static/8dd902d85373021b659e1e727c748cd0/58354/figure11-2-12.png 1396w\"\n        sizes=\"(max-width: 1000px) 100vw, 1000px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Using the metaphor that “attention is like an internal eye” runs into the problem of infinite regress.</li>\n<li>E.g. If attention is like an internal eye, then does the internal eye have its own internal eye? And so on?</li>\n<li>One crucial aspect of attention captured by the internal eye metaphor is the selection of one region over another to concentrate processing on, and the ability to move from one region to another.</li>\n<li>So, attention is like a spotlight.</li>\n<li>Predictions of the “attention as a spotlight” metaphor\n<ul>\n<li>Rate of motion. One experimenter estimated the rate of motion to shift attention at 8 ms per degree of visual angle.</li>\n<li>Trajectory. When a spotlight moves, it should highlight objects along the path.</li>\n<li>Size. The spotlight may vary in size.</li>\n<li>Unity. The spotlight can’t be divided into two or more regions.</li>\n</ul>\n</li>\n<li>An alternative metaphor for attention is like a zoom lens.</li>\n<li>No notes on the zoom lens metaphor because I dislike metaphors and at this point we’re just recreating the eye.</li>\n<li>So far we’ve covered position-based theories of attention but attention can also highlight objects, their parts, and their features.</li>\n<li>E.g. Finding the red letter in a sea of green letters.</li>\n<li>Some evidence suggests that there’s a cost when switching from one object to another that can’t be attributed to distance. This supports an object-based theory of attention.</li>\n<li>The presumption that object- and space-based theories of attention are mutually exclusive may not be true and that attention may operate at multiple levels.</li>\n<li>E.g. At early visual processing stages, a space-based attention makes sense because low-level representations don’t contain objects. But at late processing stages, object-based attention makes sense.</li>\n<li>Both hypotheses might be correct, just at different levels of the visual system.</li>\n<li>Stroop effect: when the color of a color word doesn’t match the word, it interferes with naming the color.</li>\n<li>E.g. Black (matches) versus red (doesn’t match).</li>\n<li>Interestingly, the reverse direction of the Stroop task (reading the word instead of naming the color) can be done just as quickly regardless of the word’s color.</li>\n<li>One important consideration of the Stroop task is that it may rely more on the link between color and sound of the color name than to attention.</li>\n<li>E.g. When participants instead pressed buttons to indicate color rather than saying the color names, the Stroop interference was significantly reduced.</li>\n<li>Three types of object properties\n<ul>\n<li>Separable dimensions: pairs of dimensions that can be selectively attended to without interference.\n<ul>\n<li>E.g. Color and shape of an object.</li>\n</ul>\n</li>\n<li>Integral dimensions: pairs of dimensions that can’t be selectively attended to without perceiving the other.\n<ul>\n<li>E.g. Saturation and lightness of a color.</li>\n</ul>\n</li>\n<li>Asymmetrically integral dimensions: pairs of dimensions where one property is separable but the reverse isn’t true.\n<ul>\n<li>E.g. In the Stroop task, color naming is strongly influenced by word identity, but word naming isn’t strongly influenced by color identity.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Two modes of attention\n<ul>\n<li>Distributed: when participants expect a target at any location.</li>\n<li>Focused: when participants expect a target at a specific location.</li>\n</ul>\n</li>\n<li>A key difference between these two modes is that distributed attention uses parallel processing while focused attention uses serial processing.</li>\n<li>The phenomenon of visual pop-out is used to support the idea that distributed parallel attention exists.</li>\n<li>Visual pop-out: when an object stands out in a scene because it’s different.</li>\n<li>It seems like the different item calls attention to itself.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 831px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/a4ba2a2d50b0b4736487d64c5dbe52f2/5b4a1/figure11-2-19.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 57.199999999999996%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABoElEQVR42lWS6ZajIBCFfe6Z051Zj8at45KYRBGXNqKiCJp+vrlGexnO/VFAfdyiQJvmWSollVyGkpieTtH3b0++5xvG3tCNMAwPB//P779iHPth+CoN2WqaHvwmSjPLcnRdB/Zj99Pcm7btpint+n4QAgx/aIHn/+FRqjwvfD84R+fLJXYc17Yc13nRdWOAs4DEBovxw3mVUtNsmvbz007XzfPlalqOYzthEB5PEbIHIaH1iAWev8BrCUVZhuERno7tEpKapvm8+5XSfBjlpo1/h1eBvL+9BUF48HxU7Xk+hmW7wTFKSIqyP+BV2vuFNx4trW7MffH2e0s3TJLmsPW8IMsKWIlRQZ8wrKb5jptD8/0OvmYNIbQoqzgmSULjOC2KMi9egY1yElItwhGj1MrytWaMNU3bdXVdc95zzgVa2g8d75dlzpHQth1C1rRN22J6Y01VMw1fIgxPsMqyPI6TLC9ISiHYppSi4YTSa5zQZT2LzlfE16UiPHyuVdUN5klCatbiEzQd7zieUcAR34C1Xct7xF0/PIIeu2sCH8Q/hEypMModZmoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 11.2.19\"\n        title=\"\"\n        src=\"/CR4-DL/static/a4ba2a2d50b0b4736487d64c5dbe52f2/5b4a1/figure11-2-19.png\"\n        srcset=\"/CR4-DL/static/a4ba2a2d50b0b4736487d64c5dbe52f2/63868/figure11-2-19.png 250w,\n/CR4-DL/static/a4ba2a2d50b0b4736487d64c5dbe52f2/0b533/figure11-2-19.png 500w,\n/CR4-DL/static/a4ba2a2d50b0b4736487d64c5dbe52f2/5b4a1/figure11-2-19.png 831w\"\n        sizes=\"(max-width: 831px) 100vw, 831px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>The existence of visual pop-out implies that the visual system can detect certain object properties in parallel over the entire visual field without requiring a sequential search through individual elements.</li>\n<li>Interestingly, visual pop-out isn’t a symmetrical phenomenon meaning the time to find an object with a feature versus the time to find an object with the absent feature aren’t the same.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 666px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/aaa135f5243869856babfcc8f4f9e216/ace37/figure11-2-22.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 121.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAIAAAB1KUohAAAACXBIWXMAAAsTAAALEwEAmpwYAAADRklEQVR42j2UiZraMAyE8/4tBRZIyOkrl3NfsK/X32bbfF7WcSTNaCQ52Pf9eL20KeM4zfKirhtjyiIXUfRUSr9eb07YH8ehNV/KtrVhGFlr9+MI+Nu2vW5aY6o0LfJcGF0KoaQkYEXgsqqezyTPi0LI8/l6vlzZ267b9j0AmX8fZDzxKcsKTE5w3vajbppnnOSFfIRRIUSaZlKaruudM7AryHVTVjW0hdT4V2UdRbGUik+AEOh2e5xOFyHV6XTGoOuGZV1/nKu6KYQqCpUkudFVlhVQFUJCu24soZMkI9z1eiMQlnXd/tDGuWm7OM5ItWnt4x6JQvKqlEEw4l4vX+E9ImIYxl/Xe5FLazsvmHsONJRKN41t286xsn0SZ03T4jyOU9O2WhlsqqpGEWz6fqBGwbptrHleOEBzkFvbgYYPQVnLsvbj0A9jPwyjf4bBJeyc+UbaHFMSCq1MSVbIrnXJp+N4EaVtbJ7lSODq57+iNpUPiEHFCbr5dvnoB9vX6+VedxeX3/f39/v9xoKHzcaDYNM8E5IazITZdnDI51N8iPFVKgWpzWvT9/2yLPM8k7+jTd2rqoIRdmTgk/iIeLBFcHhhABKYuBE6jhPWOM2By0Nrmu+/syPgUF0KiOxa2pR4rutKwWf/0H2uVEKgjQZ/mhxxHPEjjpeWjFCwxH/oR6jS9kpqVvyMx2kMoijCWSnlgZGHqlrY/v51kkIlaU7bFoWkPQkHZzD4hQuvQekCa4p0uz/IIElTJoQ6/flzzrKcNlTaMFCE9mK4yntyrhwBTCkatU3SjKkABFjjhiyhnzCjZ2g+ZoZfbJyBNgwv+ME0TaS6bCsvEId21w/wRwEamA0467rDlj021JKTvieF3iFTknGaqBbky7JO0xzyjAGWq+8ZwpEC4C4FIT/4eAV1XUMbHAaL3ra2B4TwlJFFqiz6lL1H/fkdxhmzIAzD8/nC1YFaxOb20F4hROa6cd2uDNNG/rk/VK67KwYOlwCphX+oJ1pRcHdZ+ZYUfg44xwHaXCxUgcVUcsqc0iQyS5l7J3HOvHhnGg5Dt3EXEy1opKOmMOb9c8nhHxSYuEfiSQh8cOi8rCwuMJ/hNHgVBifDMv3T4i9lWqqnaVAO+gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 11.2.22\"\n        title=\"\"\n        src=\"/CR4-DL/static/aaa135f5243869856babfcc8f4f9e216/ace37/figure11-2-22.png\"\n        srcset=\"/CR4-DL/static/aaa135f5243869856babfcc8f4f9e216/63868/figure11-2-22.png 250w,\n/CR4-DL/static/aaa135f5243869856babfcc8f4f9e216/0b533/figure11-2-22.png 500w,\n/CR4-DL/static/aaa135f5243869856babfcc8f4f9e216/ace37/figure11-2-22.png 666w\"\n        sizes=\"(max-width: 666px) 100vw, 666px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>There is no feature map for the absence of a feature.</li>\n<li>Physiological evidence in the form of retinotopic feature maps support the idea that the visual system divides input information into many distinct subsystems that analyze different properties.</li>\n<li>Review of the binding problem (how different visual properties are integrated into coherent perception).</li>\n<li>One proposal to solve the binding problem is that feature binding occurs by selectively attending to specific locations in the visual field. This results in the integration of all features at that location in all feature maps.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 666px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/9994c348232b28b7b18a47285992cf8a/ace37/figure11-2-23.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 122.39999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAIAAAB1KUohAAAACXBIWXMAAAsTAAALEwEAmpwYAAADYklEQVR42l2Uh3LbOhBF+dvJOM9RxhEli4xV2EAQLGBvKu8LcwA5k5lgIJACttzde0FnXpZpnpd1WdbrejXz9rhrXX98HE+ni5SZObjdbvc7R0+7aV5ms3t1WKZlmdeVgz9BrmVZikT4ni9EOgwjO/izYslciLKuT+e1G4am6/Ki0nVT142u68slYO7fD8fTOU4EB2WlF5tgnGdVFHXbWudljUUqlbL/bwbi/U6gj+PZ9z+SWIzTcn/8Xzdt1w/jNOOc5XlelsaZX9sPddPILPP8X1Gc9ABp2rLUyoIxDtNM8e8Hz3X3QkgCYQMQx1ZYZUpVWtOGcZqGccyLAofr7c5s266qNKHpK7CmaaK6oqyUUk5Rlvxnxb8oyqZtcWY1/Vyvi2nMSh46FydJHCdA7rqOQEEQOmEYcUxyIO12e9fdEQh/KjfjYRgCDrxtNj883399/U7ocRwTIZwoirTWSuWe5ycidXf7tuvKqkqlTBKRJEkqZFXqr19eCIqBEIIGqTyHDieMIpBRsMzUdrsLw5jAoGL99u2/t7ct7Vmu6/F42u/f395+EpS+w3YQBA5JGJ7Vw8HzkQdAiqIYhuHHZgPmru/P50uayiiKd/s9+WknyQjnQCtifH3duK47m7EEYXg+nwmMEV2jChCeL5fG9kmkKe8gzzJlqHrqNs8LyOQMLNBAYyEZfxQWhBHFQ1iaZjBiRb588vyU7v3xYAs+6QK18UKTmRADPWwSYrt1ff+XRbRAsLM+x58QTDr5fjhgDfNSSl3pDmVOE1CRMFUYFczzv85MDtI0BSTwKKRpGwijBN003KRpmc0VtGn/OlMAsFESCb2D9/Ly0pl0A63OixIWafXzPo4252QqGs2VfF53rg2KIzMyiOMYzipdob/P3ioFBC7vZPX/mflmbrnRIlvUw5b9btxQXpZlSAIs5Ef3zH4cZz4JltJpnhyikq3vwWgGu6xIF1g8nsjNGW0fBrQF2wyceHEOBw/RAy+O6Avcah7wxEjsADAnmcqFRcF9AFQYBhg4nGVWEubzUJiLSbfV58hxtjpJ4Aw3ZipTPhiEK8rCQWXEIxWmCJiuwDM9w8daJ5SdyoyIbLKGcXRBLugulY4xZU9KrJ9u3AqSGlPS2QLgyXTbOEtOT6eTylXX978BcxKGrSfEvCkAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 11.2.23\"\n        title=\"\"\n        src=\"/CR4-DL/static/9994c348232b28b7b18a47285992cf8a/ace37/figure11-2-23.png\"\n        srcset=\"/CR4-DL/static/9994c348232b28b7b18a47285992cf8a/63868/figure11-2-23.png 250w,\n/CR4-DL/static/9994c348232b28b7b18a47285992cf8a/0b533/figure11-2-23.png 500w,\n/CR4-DL/static/9994c348232b28b7b18a47285992cf8a/ace37/figure11-2-23.png 666w\"\n        sizes=\"(max-width: 666px) 100vw, 666px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Conjunction search: objects pop-out if they differ by elementary features, but not for objects that differ by multiple elementary features.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 824px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/9169be69bdef78fe7e681586d2b72031/c1c45/figure11-2-24.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACD0lEQVR42i1SiZKaUBDkU5NUbSp7iC7gicd6gLfBrTUu930YXfCpbFB/Lk3M1PgcZrpn+g1QjuPwfJPjyvBOp9Pt9j7iWNf14XA4mUzHuU0kabknZDab1+v1Vqs9Go3K5cp6vaZM0+Q4jqZpkJ+eCo8Pj9vdTpZlURzCYYDO5vP9nqANxtRqtQbfYBgWGMq2LUQMwzw/MzRdbDQav7dbWVEEQQBtNB53e72fkpQk+35fAKzX61er1Vqt/n8yNLAsW6lUi8USHGRVVZvNVrvdmUynoiguFoskSTAfGl9eumD2+wNN0yjdMO5/PMC/fvl2d/cdKqJoA0loz/M8Zg8GA5AJIYIgFgp0uVxlWa5UKq1WKyraROv1u/wur1a/4JL0GsdxGIaKqhqGYcFs23Vdcjjg1HVDUTTDMEEJo5BK0zTLslOaYp/n8zk7Z8fjEUnEx9MJe7per3+yLP38BOxyuQCG6uV6QZIK/ACXxJXCMPL9YLfbfcDiOAHvcCAHskeAP8SE5PkkwQ81PFJvb6vl8hWSVFXDnjzPu8mD3jDC9TemZSmKis/BDwLbtj3fVzUNSddx8aqQd+FgggCy7bjAeR6OEB193wckCEOgUcoBtoMilFKapuuajhQ0YxOGma8Iw92bBMNABWS0wCNOlDEJhkZ4zxYQ0AMEFmuD6XlOjgtumvKclYc3s/9BsZo9IX8B8NKtF5UUSFoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 11.2.24\"\n        title=\"\"\n        src=\"/CR4-DL/static/9169be69bdef78fe7e681586d2b72031/c1c45/figure11-2-24.png\"\n        srcset=\"/CR4-DL/static/9169be69bdef78fe7e681586d2b72031/63868/figure11-2-24.png 250w,\n/CR4-DL/static/9169be69bdef78fe7e681586d2b72031/0b533/figure11-2-24.png 500w,\n/CR4-DL/static/9169be69bdef78fe7e681586d2b72031/c1c45/figure11-2-24.png 824w\"\n        sizes=\"(max-width: 824px) 100vw, 824px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Objects that differ by multiple features can only be found using serial search and attending to both features simultaneously.</li>\n<li>One counter against the feature integration theory is the finding that conjoined features can sometimes be detected in parallel rather than serial search.</li>\n<li>E.g. Color and direction of motion.</li>\n<li>This goes against the theory’s claim that attention is necessary to conjoin features.</li>\n<li>How does the visual system cope with changes in visual information?</li>\n<li>E.g. If an object changes its location, we don’t see it as a new object at a new location but as the same object at a new location.</li>\n<li>Review of attention-impairing neurological conditions such as unilateral neglect.</li>\n<li>The primary symptom of unilateral neglect is that patients fail to notice objects on the contralateral/opposite side of the world.</li>\n<li>Unilateral neglect isn’t just a sensory problem as it also applies to the visual imagery of imagined or remembered scenes known to the patient before their brain injury.</li>\n<li>It’s hypothesized that the neglect results from the inability to disengage attention from objects in the good side to objects in the bad side.</li>\n<li>Neglect affects the high levels of visual perception and not low-level sensory input.</li>\n<li>Balint’s syndrome: the inability to see anything except for one fixated object.</li>\n<li>Balint’s syndrome supports the theory that attention can be object-based.</li>\n<li>Perhaps Balint’s syndrome is due to the inability to disengage attention, freezing perception on the currently attended object.</li>\n<li>In terms of physiology, the right parietal lobe is especially important in moving attention from one object to another.</li>\n<li>PET studies find that the right parietal lobe is involved when attending to both halves of the visual field whereas the left parietal lobe is involved only when attending to the right visual field.</li>\n<li>Does attention follow eye movements, or do eye movements follow attention? Or perhaps there’s a complex relation between them?</li>\n<li>Given that the similarities between eye movements and attention are so compelling, it’s tempting to say that both are identical.</li>\n<li>However, we know that attentional shifts occur more quickly than saccadic eye movements and that attentional shifts don’t require eye movements.</li>\n<li>The current dominant view is that eye movements normally follow attentional movements.</li>\n<li>Exceptions are covert attentional shifts where the eye movement that would normally follow an attentional shift is inhibited.</li>\n<li>According to this theory, attention drives saccadic eye movements to the directed objects by either being summoned to a salient event (pull cue) or by internal direction to an important location (push cue).</li>\n<li>This also suggests that if an eye movement is made to a location, then attention will precede that movement and therefore can’t be sent to a different direction.</li>\n</ul>\n<h2 id=\"chapter-12-visual-memory-and-imagery\" style=\"position:relative;\"><a href=\"#chapter-12-visual-memory-and-imagery\" aria-label=\"chapter 12 visual memory and imagery permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 12: Visual Memory and Imagery</h2>\n<ul>\n<li>As visual information gets processed through different stages, what happens after perception is achieved?</li>\n<li>Everyday experience and experiments show that perceptions produce significant learning of and memory for visual information.</li>\n<li>Salient visual memories can be very long-lasting.</li>\n<li>E.g. Your childhood bedroom or your first love.</li>\n</ul>\n<p><strong>Section 12.1: Visual Memory</strong></p>\n<ul>\n<li>The most obvious residual effects of previous visual experiences are visual memories.</li>\n<li>Such memories allow us to reexperience visual information after time.</li>\n<li>Some visual memories are more fleeting than others.</li>\n<li>Visual memory: the preservation of visual information after the optical source of that information is gone.</li>\n<li>Three types of memory\n<ul>\n<li>Sensory information stores (SIS): brief buffer stores for modality-specific information.</li>\n<li>Short-term memory (STM): limited-capacity memory for information that’s currently being processed.</li>\n<li>Long-term memory (LTM): large-capacity and temporally extended storage for knowledge of general facts, personal events, and skills.</li>\n</ul>\n</li>\n<li>Five important memory characteristics\n<ul>\n<li>Duration</li>\n<li>Content</li>\n<li>Loss</li>\n<li>Capacity</li>\n<li>Maintenance</li>\n</ul>\n</li>\n<li>A memory can be lost due to decay or due to interference from other memories.</li>\n<li>Iconic memory\n<ul>\n<li>Duration\n<ul>\n<li>About a second depending on the exact conditions.</li>\n</ul>\n</li>\n<li>Content\n<ul>\n<li>Spatial and color cues increased performance on an experiment but category cues didn’t.</li>\n</ul>\n</li>\n<li>Loss\n<ul>\n<li>Due to either decay or interference.</li>\n<li>E.g. Using a mask about 100 ms after the target display.</li>\n</ul>\n</li>\n<li>Maintenance\n<ul>\n<li>Memory is lost either by decay or by interference from subsequent items.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Masking: when visual displays presented close in time to a target display degrade the target’s perception.</li>\n<li>The fundamental question of masking is: how do two temporally contiguous displays interact with each other perceptually? The answer is extremely complex.</li>\n<li>E.g. Masking depends on spatial structure, temporal delay, relative brightness, etc.</li>\n<li>Visual short-term memory bridges the gap between iconic and long-term visual memory.</li>\n<li>Review of the recency and primacy effect as evidence for short- and long-term memory.</li>\n<li>Conceptual understanding of coherent pictures and sentences takes place very quickly, but leave no memory trace unless some form of further attentive process is performed.</li>\n<li>Visual short-term memory appears to be involved in integrating visual information across saccades and other brief interruptions of visual information, but it may also be involved in the creation and manipulation of visual images.</li>\n<li>Three types of LTM\n<ul>\n<li>Semantic: general knowledge of generic concepts.</li>\n<li>Procedural: general knowledge about generic skills.</li>\n<li>Episodic: objects and events experienced as part of your personal history.</li>\n</ul>\n</li>\n<li>No notes on the difference between recall and recognition.</li>\n<li>Unlike poor auditory recall, poor visual recall may be due to the inability to draw and reproduce the memory rather than limitations of the memory.</li>\n<li>It’s also difficult to measure how similar a reproduced visual memory is to the actual image.</li>\n<li>Instead, we can use a recognition paradigm that avoids drawing and provides a way to measure correct versus incorrect responses.</li>\n<li>Experiments find that visual recognition memory is excellent even when recall is poor.</li>\n<li>E.g. A random meaningless shape tested a month later couldn’t be reproduced but could be recognized.</li>\n<li>Memory dynamics: the idea that memories may change systematically in time.</li>\n<li>The evidence for memory dynamics is weak at best and there are many methodological problems with such experiments.</li>\n</ul>\n<p><strong>Section 12.2: Visual Imagery</strong></p>\n<ul>\n<li>Visual imagery: processes involved in generating, examining, and manipulating visual images.</li>\n<li>Because visual images are private, they aren’t directly accessible to scientific study.</li>\n<li>Zenon Pylyshyn’s critique of mental images\n<ul>\n<li>Image retrieval. To retrieve an image, we don’t look through all of our stored images using our mind’s eye. Instead, images are interpreted so that they can be accessed by some kind of symbolic description.</li>\n<li>Interpretation of images. If images are like pictures, then they’re uninterpreted.</li>\n<li>Indeterminate information. Images can contain less information than a normal picture but are incompletely in ways unlike the picture.</li>\n</ul>\n</li>\n<li>Instead of images, Pylyshyn suggested that images should be considered as descriptions constructed of abstract propositions.</li>\n<li>No notes on analog versus propositional representations of visual images.</li>\n<li>Many theories of visual imagery assume that imagery shares many processes with normal visual perception.</li>\n<li>The general hypothesis is that imagery and perception are similar except for the source and direction of information processing.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 817px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/6b594af5d28d191c42c57a5dc48f8583/98314/figure12-2-8.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.60000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAACT0lEQVR42i1Ti3KbMBDkg9uknYmdtEnwEzAGgR+AAAmweQnzEM7kB7t4yjDyWbrdvb0Tiuf7m+3WdQ+1EMTB41qWHUUsjhmxHYe4QUDzvCDEwYvkfhhixpAWUKqEUWzZxHWPTdvyNDX3eOyqFkmaHY9nQlxKw6KoDu7xcDidz15za3EEojCMFETz17e9TXopKyGMnalpxiBH0dxMc69pel3VwzgahrnZanme3+9fILUswnmixCx5ev5NnEM3DAD/efv7/v7Rdn0tGgTgLR/g3c5U1eX1mg+DhKPlcsU4V8KYfapLlIqMy+UKsLpY9YOE8nq9MfcWWCawsZvN5oB9fX3bxGGMNc1N8QP6/PxL03U53vO81PXder2FMrx9fKrAQxlcnuejkCTJ4C67XNG2sqzQMIYk9Aw+86JcrTeH42mQEmB1sfxUF3XddP1wzQu0AAlwhxXgoiwVGsVTwywC5TS7vLzMKY3kOLZdp2sG+oT6b22HaeVFIWBByrKq/iufzv6Pn0+6sZNyBBg1YySIu75fLBaz2asQN1RRFOWtbUUzgau6Bjt2plF5Po0Yxy7+YwyMp4gBtm0S0FDgAnQ9fKZphiIAg/L0TspeQKOIpxlLEsYTiMASHIII8RSgY7VAKmJcHrjAghLgSzmezpbtoOeeH5w9H6OPotjzAtzQMIyDIESfKaKY0yBEQsx4iIyYoQvT9YQ2VsAwd1xG6COZJwnuMH/E2AQY+xOIo8oMR1l2QbcjnIAMHwB7HE0LTwDGD2JgwA34Q2IihTBo2677BzD6+HX6XDiDAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 12.2.8\"\n        title=\"\"\n        src=\"/CR4-DL/static/6b594af5d28d191c42c57a5dc48f8583/98314/figure12-2-8.png\"\n        srcset=\"/CR4-DL/static/6b594af5d28d191c42c57a5dc48f8583/63868/figure12-2-8.png 250w,\n/CR4-DL/static/6b594af5d28d191c42c57a5dc48f8583/0b533/figure12-2-8.png 500w,\n/CR4-DL/static/6b594af5d28d191c42c57a5dc48f8583/98314/figure12-2-8.png 817w\"\n        sizes=\"(max-width: 817px) 100vw, 817px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>One experiment found that imagery interferes with perception in the same modality more than it does with perception in a different modality.</li>\n<li>If imagery and perception share common neural mechanisms, we would expect damage that impairs perception to also impair imagery, and this is the case.</li>\n<li>E.g. Agnosic patients that can’t recognize certain objects can’t imagine them either. The same applies to patients with deficits in color perception and patients with unilateral neglect.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 815px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/2f8b3b21e7b96d3bb842fd42db460f0d/ef916/figure12-2-10.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABo0lEQVR42jWSWXODMAyE+eVtn5r2oUcyEGNjc/mAHCRAOm1/Xj9DOuMxGku7Wq1Ipmkep3mabzEYJ+LTeWit6w/H9ebwErre+WCd974jKOuGl2S4XEnG/HCBYr59EVL//fNLkEtVNzbbC2B1017HaT3WBxe6BCZTViSa1vJRhaYhrPGlsWQ+t7vnzcvb+wdZrcvdLhVCKa3vYG1K8NRxV1WT5zCYzeY1TfdIOERdQ9cffOhIPDw+QVfRygfAgVbOeWtdnHy+oZZStHHDiATEwwsG8XrRCTh2xifqLtdxXOfBsNOASQzPvfokcgmMAINwlJr7zPN8u04RNi62c87nC1pou97jPztgxDbW1a3VpnK+S5gKPg59iKPtS0zProtbOi7mrytY+4PnhP6YbLc7nGQQTErTLM6D50pLVdR1SxAtlEoVhjkLU7I2OgupeEyAMRJ6MAY8UokJoBBCGhO3wIFLLV5JpflDctgLk8QNmYpn0twUwFUsdbQlxXpI8USaB/aY7gWB1CZBFYXrnmkO6t52oQBWaOgUGHSmmSBGc5ZLXdV/0OXJOKCnDqcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 12.2.10\"\n        title=\"\"\n        src=\"/CR4-DL/static/2f8b3b21e7b96d3bb842fd42db460f0d/ef916/figure12-2-10.png\"\n        srcset=\"/CR4-DL/static/2f8b3b21e7b96d3bb842fd42db460f0d/63868/figure12-2-10.png 250w,\n/CR4-DL/static/2f8b3b21e7b96d3bb842fd42db460f0d/0b533/figure12-2-10.png 500w,\n/CR4-DL/static/2f8b3b21e7b96d3bb842fd42db460f0d/ef916/figure12-2-10.png 815w\"\n        sizes=\"(max-width: 815px) 100vw, 815px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Interestingly, PET studies found high activation in both the temporal and parietal lobes when participants were asked to imagine walking along a route and observe landmarks.</li>\n<li>In almost every PET, fMRI, and ERP visual imagery study, they’ve found that imagery tasks produce activation in visual regions of the brain including the occipital cortex.</li>\n</ul>\n<h2 id=\"chapter-13-visual-awareness\" style=\"position:relative;\"><a href=\"#chapter-13-visual-awareness\" aria-label=\"chapter 13 visual awareness permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Chapter 13: Visual Awareness</h2>\n<ul>\n<li>None of the previous 12 chapters have covered the most fascinating aspect of vision: the conscious phenomenological component of it.</li>\n<li>Visual awareness is what it “feels like” for a sighted organism to see.</li>\n<li>Many of the processes involved in vision are unconscious; they’re inaccessible to consciousness.</li>\n<li>If they weren’t, previous scientists would have presumably solved all the problems of visual perception, attention, and memory long ago by simply introspecting about their own conscious experiences.</li>\n<li>This explains why most people are surprised to discover the enormous complexity of vision.</li>\n<li>It’s startling just how much of visual processing is unconscious.</li>\n<li>E.g. Our visual experiences derive from receptor activity, but this activity itself isn’t conscious.</li>\n<li>Awareness appears to come from somewhere further along a complex chain of neural information processing.</li>\n<li>What we experience visually conforms not to the firing of retinal receptors, but to some higher level of neural activity.</li>\n<li>Explanatory gap: the gap between explaining how neurons give rise to the properties of consciousness.</li>\n</ul>\n<p><strong>Section 13.1: Philosophical Foundations</strong></p>\n<ul>\n<li>No notes on the mind-body problem and dualism.</li>\n<li>Emergent properties: attributes that don’t arise in ordinary matter unless it reaches a certain level or type of complexity.</li>\n<li>Imagine that neurons glow slightly as they fire in a brain and that this glowing is somehow related to conscious experiences.</li>\n<li>The pattern of glowing is clearly caused by the firing of neurons in the brain, but the neural glow doesn’t cause neurons to fire any differently than if they didn’t glow.</li>\n<li>Therefore, causation runs in only one direction, from neural firing to light.</li>\n<li>We come to know about other people’s mental states through their behavior, but this doesn’t mean that the nature of these mental states is inherently behavioral.</li>\n<li>Multiple realizable: many different physical implementations can serve the same function provided they causally connect inputs and outputs in the same way through internal states.</li>\n<li>E.g. There are many different ways of building a thermostat.</li>\n<li>Criteria for consciousness\n<ul>\n<li>Behavioral similarity\n<ul>\n<li>Other people act similar to how I act when I have conscious experience.</li>\n<li>E.g. When other people say “Ouch!” after stubbing their toe, you also say “Ouch!” when you stub your toe.</li>\n</ul>\n</li>\n<li>Physical similarity\n<ul>\n<li>Other people, and animals to a lesser degree, are similar to me in their basic biological and physical structure.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Both criteria are required to believe that another entity has consciousness.</li>\n<li>E.g. A comatose patient is physically similar but not behaviorally similar.</li>\n<li>But even both only allow us to infer consciousness; we can’t be certain.</li>\n<li>E.g. How can we know whether my color experiences are the same as yours?</li>\n<li>Inverted spectrum argument: suppose that our color experiences are identical except that your spectrum is inverted.</li>\n<li>E.g. What I see as red is what you see as green.</li>\n<li>The claim of the inverted spectrum argument is that no one could tell you and I have different color experiences.</li>\n<li>The basic version of this argument doesn’t work because flipping hues is only one dimension of color and we could detect differences in the other two dimensions.</li>\n<li>So, a more sophisticated version of the inverted spectrum is required.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 600px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/ca1bcf755f99f83c3e21c90f9f7b0b25/0a47e/figure13-1-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 141.6%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAIAAADuuAg3AAAACXBIWXMAAAsTAAALEwEAmpwYAAADUUlEQVR42m2UiXKbMBCGefd2esykyaS+MIcAgQFzGbABp0/Xb6XUsTPV7MjSav/998LO5TKdLxeEw+2MTPNcVfXr60qepg89Mp7PCFrnv0gLaNo2zbKyPHan053Z5XZ2rJtPXjkM4+B5fpLoum7cvReEqm4aPFp5AN/wHIZxJObscHh6+rXZ7PK8SNNMni7nh9A+mM26eSFUrdNhGF9eXtfrLUrcPaR2tuBxhOqdn9/Lpe26vedFUYwL9uOxmpbZWn8qigOyh8J4sIsTD6e+P+S5UvE0zfeYW3bszjAMfT+AR3BiopBA5mWh2tute19OG7AR4RawXYIH+y9tdmoWBGF2yB/6fL4r2Dtzb8K+Fe/O7p4ZL9M8feTcm0XALatrrQv7RtoU7x6PXdN27xP2Dj71GFPYzXZLS+gnQs46zdbrDQfpE4zzTM9XqzUHHM3z4vQmbJiP5dEMY2lu0rNjVYGnVTWl6zqo0KTZgZkjqK47CTNl4hvYbXc80B6mKtEaO7qNO8O2qeoawOvvFQEWZblzXfSO7U4YhO7O3e89naZoIQlDRZ8Yb0YtL4rlelVR7PuB6+6ZVljQEHYfxzGMBPPly1c/CK5vb7whfEzfvn33/GC+LjSMuCD/8eMnJLZ+Dkm6rpskSZ7nq/W6blsy/PX8Eqm4qqrfqxUBI8/PLzBD4PshV9t5k/M4Nk0bhiEubIfIhLYRXhwn1JYQKGLTNMRLgrZndMs59SdknmdyY57omSn2eP3zRmZKRfP1Kq0ex2VZKIe39+ZJFr2SnKkHDDqhWLjOANd1nZnKW2F8DrJyCDBCUxQFDRRm/iJ4AMabBTdGo7W4Y8dXUZSIFBKvWUaCNMxZrSnThvKEgTqyyiNZEUWWysAUeQEjMOGFsCwhTU07GQQnCIPdbhcGAR7JmWL6dDMIYv6+tI7N4koMieSVhUolopeLEyex7wVxlChmJ479QJB44YwCYVrYUSWyNGeYaY1MGDMEEgvuOGLnmSJbO3a8YG1DsEr5zkO+84NDjIhSCjgBi0Gi8cVByYpYbATMTlQI2RMklXMkJZMnYMlNE5JgDD7mKoRaG0eKKlAak0uIO0ccqwhyLEiBMwmzi1LyxF4bb5HnyWcDBgiT2w/DX9a4VldnxmkLAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 13.1.1\"\n        title=\"\"\n        src=\"/CR4-DL/static/ca1bcf755f99f83c3e21c90f9f7b0b25/0a47e/figure13-1-1.png\"\n        srcset=\"/CR4-DL/static/ca1bcf755f99f83c3e21c90f9f7b0b25/63868/figure13-1-1.png 250w,\n/CR4-DL/static/ca1bcf755f99f83c3e21c90f9f7b0b25/0b533/figure13-1-1.png 500w,\n/CR4-DL/static/ca1bcf755f99f83c3e21c90f9f7b0b25/0a47e/figure13-1-1.png 600w\"\n        sizes=\"(max-width: 600px) 100vw, 600px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>It’s the sameness of the physical spectra that ultimately causes colors to be named consistently across people and not the sameness of the private experiences.</li>\n<li>Pseudo-normal color perception\n<ul>\n<li>Red-green reversal is the most plausible form of color transformation because there’s a plausible biological argument for it.</li>\n<li>Normal trichromats have three different pigments in their three cone types.</li>\n<li>E.g. Short (S), medium (M), and long (L) cones.</li>\n<li>Some people are red-green color blind because they have a gene that causes their long-wavelength (L) cones to have the same pigment as their medium-wavelength (M) cones.</li>\n<li>Other people have a different form of red-green color blindness because they have a different gene that causes their M cones to have the same pigment as their L cones.</li>\n<li>In both cases, people with these genetic defects lose the ability to experience both red and green because the visual system codes both colors by taking the difference between the outputs of these two cone types.</li>\n<li>But suppose that someone had the genes for both forms of red-green color blindness.</li>\n<li>E.g. Their L cones have M pigment and their M cones have L pigment.</li>\n<li>Then such doubly colorblind individuals would not be red-green color blind at all, but instead be red-green reversed trichromats.</li>\n<li>But nobody has ever detected such as person.</li>\n</ul>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 559px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/69e914de21f99bf4818ded281c8f864d/a65ce/figure13-1-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 149.2%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAeCAIAAACjcKk8AAAACXBIWXMAAAsTAAALEwEAmpwYAAADr0lEQVR42mWV61LbSBCF9aqp/QVJtiohqQ2pylKBBFs2BmzrrtHVlnWxLdkLD7jfzNgqsTvVUYSYM3369JnG2JTVMKq6yVZrPwhnsyfLdh3HXS6t+XyxtKy6aXZq7Xd7vYz/gDdVXdYNR4QijpNUiCgUUZav6mZLNNst4O3utAwN6DE6+Hhz85P4cvX13bs/fv2+P/7zQuatRMrEMnZkrpvyjNTHa7ztuIvlcjQa//j7xrKd9nAAqnLCt9VhyJxVJYNTVFQKT4ZSHUHoH/nXDJASXP4frIKtVD02J+ZkKkS8aztJeoDswdCuK6nHjqjlE2X2YFDb83w023eHnrNew8w1Iq3WBbEuNjxJneV5nCQp/61Qu5Fs5NqBhMJuCI7TzPX8pWXTUl5knUjatuuiCMKQ/Lbt2rYTBKHqscqMLUrQzZYMab4KQpGkKR1W/Lf6Sf4syyMRwYHN7XkZinBJhaY5ub8fX1y8/3l7d3X1tVUKQRS2d3e/v/11/fnT1cXFpTmZvL6+7gfgCpGQxnV9kA+zx/vRmJIaZSnoP88X45F5e3t3ff3dcZyXlxctm6EdUqre7NsuimK2plnGD3xTstOmDp0pOwiC4/HYK25szuBSWW06fTDHk+fnBRjtCt1eFKHjk8m0KIquO2g83q51q9GsKCv8JKJYq91owRSlKE5I63sByh0OBy24oRP2xpb1N1s8A1W5oesUwQ53UAQASaPtvX0mTALsEUWJLC0UPGgYHWbhlDiO0zTjltKq04HDmqENGEAY0tE4CHgkYciNjmST5Yo5aLMpuzPa6KXSzNWNbXlngJhKoenDDJEOxyMYqu2RZ7C+GKf7VGMptOUav7/88PHjn5eXHygEoJoipzapmveS9nD69IGlo5hb5dHeoizV6BiC1STRDtO0+4A/ajO6ZrNH+ON9UHIASfQJ/DbzWzy7yPn0NMetqA2kOaVudcMk+DS0aLLyw3lKyt9CmxnM2OVFOVRPPs7pTplFnNAeFEqyvNiUsODJMOBJ59RgWHPt5K2lHOU7olb+Mx6fnkdjkys1Xy657vQWbZmblOr6gecF8jaoYbBYLFSrBSaNRJznueEHwvF8/jgwqLA0PAlEJnjB6voLv7Isy/d9Vy65D9MYjDcGHJmlJf2AQcMp7AaKxXhhaGu3Yjbbtn3Pw2eWZXOQAUwOJ8e1LO65x1Y5FTwysCkCwXfJQQiYe65POUIt3G7wmVTg2fQ2lWTr+yRxNHmGsByjeLGSPmya5l/Fj3RLBMtnWwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 13.1.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/69e914de21f99bf4818ded281c8f864d/a65ce/figure13-1-2.png\"\n        srcset=\"/CR4-DL/static/69e914de21f99bf4818ded281c8f864d/63868/figure13-1-2.png 250w,\n/CR4-DL/static/69e914de21f99bf4818ded281c8f864d/0b533/figure13-1-2.png 500w,\n/CR4-DL/static/69e914de21f99bf4818ded281c8f864d/a65ce/figure13-1-2.png 559w\"\n        sizes=\"(max-width: 559px) 100vw, 559px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<ul>\n<li>Let’s return back to the problem of how to tell whether a given creature is conscious or not.</li>\n<li>Phenomenological experience is key.</li>\n<li>Such first-person knowledge or subjective knowledge is available only to the self.</li>\n<li>This is a problem for the scientific method because it requires facts to be: objective, available to anyone, and repeatable.</li>\n<li>Rather than observing consciousness itself, scientists are forced to observe the correlates of consciousness, the “shadows of consciousness” as it were.</li>\n<li>Two types of shadows that are possible to study: behavior and physiology.</li>\n<li>The behavioral criteria of consciousness is closely related with the Turing test since the test only judges the entity based on it’s behavior.</li>\n<li>Suppose, for the sake of argument, that neuroscientists discover some critical feature of neural activity that underlies consciousness.</li>\n<li>E.g. Maybe all neural activity in some particular layer of cerebral cortex gives rise to consciousness, or maybe neural circuits mediated by some particular neurotransmitter, or neurons that fire at a specific temporal frequency.</li>\n<li>Could we define consciousness objectively in terms of that form of neural activity?</li>\n<li>It would be difficult, but not impossible.</li>\n<li>If we knew all of the biological events that occur in a human brain, we still couldn’t provide a biological account of consciousness because we don’t have a way to tell which brain events are conscious and which ones aren’t.</li>\n<li>But we can by taking a causal approach, which is manipulating the events and observing their outcome on consciousness.</li>\n<li>Correlational theories of consciousness: what type of physiological activity takes place when conscious experiences are occurring that fail to take place when they’re not.</li>\n<li>These theories can only claim that this feature of brain activity is associated with consciousness but don’t explain why that association exists.</li>\n<li>Correlational theories fail to bridge the explanatory gap.</li>\n<li>Unfortunately, no examples of a causal theory of consciousness exist.</li>\n<li>To the author’s knowledge, nobody has ever suggested a theory that the scientific community regards as giving a remotely plausible causal account of how consciousness arises or why it has the particular qualities it does.</li>\n<li>This doesn’t mean that such a theory is impossible in principle, but that no serious candidate has been proposed in the past several thousand years.</li>\n</ul>\n<p><strong>Section 13.2: Neuropsychology of Visual Awareness</strong></p>\n<ul>\n<li>Is it possible for a complete perceptual analysis to occur without visual awareness?</li>\n<li>There are several neurological conditions where visual abilities are dissociated from visual awareness.</li>\n<li>E.g. Split-brain patients, blindsight, hemispatial neglect, Balint’s syndrome, and prosopagnosia.</li>\n<li>No notes on patients with these conditions.</li>\n</ul>\n<p><strong>Section 13.3: Visual Awareness in Normal Observers</strong></p>\n<ul>\n<li>No notes on perceptual defense.</li>\n<li>Subliminal perception: the ability to register and process information presented below the threshold of awareness.</li>\n<li>Experiments suggest a “gray zone” of perception where subjects believe they’re randomly guessing but actually perform better than chance.</li>\n<li>E.g. Patients with blindsight that perform better than random chance but don’t experience vision.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 804px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/CR4-DL/static/ad8c097f88f88c53fc2d94d640eb7532/27b7a/figure13-3-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.80000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAACKklEQVR42lVS23aUMBTlK+rzfIL6pL7r8g/qqpcXbd+7/Ifa2hnmgm1ngCQQCAQYSBgu41r239wJ9sFDYJ0k+9z2xum67nA4jOMQBIHWGtuqquqmqeva9/2IMRwe/rcpBF+nezJKGY7Kslws3J83c0IofCkLBA/D0PU94H3fD9amEAcBOOKcozL2KIjK+30dhsRduPcPD0KINBU50sgCJq2Z0L532radym63O/jIlWVAlnHMX7968+3rBRLd3d3Pb+er5Wq1WrvLFe5t10/BjDJ/52vdYrnu8vr6piqrd2/fX15+//P4OI5HJFVKTeBhPNqubTAu0iRF27hFsFKGoSAIXzx/+fnTFyFyNMVYxHlCCAlDmsuit2M7lsyW2baV1nXd4DpJkjzPZrPZ2dlHlKWEet4vb+Oh8/Xaa5TqDpawRjVKK0yJYChECFvMXRC+2XgnJ89OTz+gbbQzyWNHNU5rXcN2a+9izhEMXGv5L8vq/Pzi6urH8fhbT7NZ++f2Qz+MDqbgPLZyZJmUkKmQZbWv7eRd06iigN5GPGmlqqyS5t3XjodRPA9MBAHZ7YIoin0/wILOlEb2y/AXpGm63fqgDRJSxkKMx5gTxzGlBD2bFSeAAY3fC9wKkQHNokikGc6RCBiRChSIoognicNMmhCFqE0LBKEMOoHwGPvInCARCoLRMAwNiqNGghSYmQCC5kKjIZoPZC6hn7Km8TSTqxrwqaw2yjCotf4LQawbfQ59IawAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Figure 13.3.2\"\n        title=\"\"\n        src=\"/CR4-DL/static/ad8c097f88f88c53fc2d94d640eb7532/27b7a/figure13-3-2.png\"\n        srcset=\"/CR4-DL/static/ad8c097f88f88c53fc2d94d640eb7532/63868/figure13-3-2.png 250w,\n/CR4-DL/static/ad8c097f88f88c53fc2d94d640eb7532/0b533/figure13-3-2.png 500w,\n/CR4-DL/static/ad8c097f88f88c53fc2d94d640eb7532/27b7a/figure13-3-2.png 804w\"\n        sizes=\"(max-width: 804px) 100vw, 804px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><strong>Section 13.4: Theories of Consciousness</strong></p>\n<ul>\n<li>Neural firing alone is insufficient to explain consciousness because all neural information processing involves the firing of neurons, but only a small subset of this processing appears to be conscious.</li>\n<li>E.g. The firing of retinal receptors isn’t conscious.</li>\n<li>Two types of theories of consciousness\n<ul>\n<li>Functional approach: the functional role of certain types of activation determine what’s consciousness and what’s not.</li>\n<li>Physiological approach: that certain biological properties underlie consciousness.</li>\n</ul>\n</li>\n<li>Functional theories are independent of the specific neural hardware they’re implemented on and are compatible with multiple realizability.</li>\n<li>Consciousness seems most naturally identified with short-term memory (STM) because items in STM are experienced unlike items in iconic and long-term memory (LTM).</li>\n<li>The contents of LTM are generally not conscious but can be brought to consciousness by retrieval operations.</li>\n<li>That information from LTM is brought into STM to be experienced matches our intuition that STM is identified with consciousness.</li>\n<li>However, arguing that information in STM is conscious doesn’t solve the question of why information in STM is conscious.</li>\n<li>E.g. Why is STM special? Does it have a special structure? Function? Location? We don’t know.</li>\n<li>Three qualitative states of information\n<ul>\n<li>Conscious: items in STM that are the most highly activated.</li>\n<li>Inactive: items in LTM that haven’t been accessed recently.</li>\n<li>Activated: activated but unconscious items.\n<ul>\n<li>Previously conscious: items that were recently conscious but have decayed.</li>\n<li>Primed: items that were recently activated but not enough to reach awareness.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Functional theories of consciousness\n<ul>\n<li>Short-term memory</li>\n<li>Attention</li>\n<li>Working memory\n<ul>\n<li>E.g. Global workspace theory and multiple drafts theory.</li>\n<li>Conscious experience is like the broadcasting of information.</li>\n<li>There is no single unified stream of consciousness, rather, consciousness comes from temporary coalitions among specialized distributed processes working at different locations at different times.</li>\n</ul>\n</li>\n<li>2.5-D sketch</li>\n</ul>\n</li>\n<li>Biological theories of consciousness\n<ul>\n<li>Activation thresholds\n<ul>\n<li>The most prevalent assumption is that any neural activity that is “something enough” produces consciousness.</li>\n<li>E.g. Neural activity that’s strong enough or lasts long enough.</li>\n<li>Consciousness is just a matter of having enough of the crucial property and any neural activity that meets this criterion will be conscious.</li>\n<li>One intuitive version of threshold activation theories is that the firing rate must be above some minimum level.</li>\n<li>This doesn’t hold against the evidence that most neurons have about the same maximal firing rate.</li>\n<li>Perhaps the duration of neural activity is what makes it conscious.</li>\n<li>E.g. In masking experiments, the duration of the target stimulus is shortened by the mask, preventing neurons handling target neural activity from settling into a steady state.</li>\n<li>Consciousness generally reflects the results of processing rather than the processes themselves.</li>\n<li>E.g. We’re conscious of an object’s category but not the process of classification.</li>\n<li>The fact that we’re conscious of results rather than processes follows directly from stable-state theories because processes are in a dynamic flux of activation that lead to a stable state.</li>\n</ul>\n</li>\n<li>Cortical hypothesis\n<ul>\n<li>The hypothesis that consciousness arises from activity in the cortex.</li>\n<li>There is plenty of evidence to support this hypothesis.</li>\n<li>Virtually all cases where visual experience was impaired were caused by damage to areas of the cortex involved in vision.</li>\n<li>E.g. Damage to primary visual cortex results in phenomenal blindness, damage to the right posterior parietal region results in hemispatial neglect, and damage to the inferotemporal cortex results in agnosia.</li>\n<li>However, we need to be careful about inferring consciousness from lesions.</li>\n<li>The correct inference is that if damage to brain structure S destroys awareness of feature F, then awareness of F must take place along some pathway that involves S.</li>\n<li>This is because lesioning S disrupts neural processing not only in S, but also any brain structure that receives input from S.</li>\n<li>The cortical hypothesis is too vague a constraint on consciousness to be useful.</li>\n</ul>\n</li>\n<li>Crick/Koch conjecture\n<ul>\n<li>Review of the neural correlates of consciousness (NCC).</li>\n</ul>\n</li>\n<li>Extended reticular-thalamic activating system (ERTAS)\n<ul>\n<li>No notes for this theory.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>The current biological theory of life is a good example of how a causal theory can fill an explanatory gap in science.</li>\n<li>E.g. The work by Gregor Mendel teased out the relationship between parent and child without observing the physical basis of the entities that carried inheritance (genes). It would take the work by Crick and Watson to discover the physical basis of life: DNA.</li>\n<li>All levels of theorizing were important in the historical development of a scientific theory of life.</li>\n<li>There’s an objective relational superstructure that holds between individual experiences for every person with normal trichromatic vision, but this doesn’t mean that individual experiences are identical.</li>\n<li>E.g. The color space holds for all trichromats, but this doesn’t mean all trichromats experience the same color.</li>\n<li>These relations constrain experiences in important ways, but they don’t fix the experiences to be unique.</li>\n</ul>","tableOfContents":"<ul>\n<li>\n<p><a href=\"#part-i-foundations\">Part I: Foundations</a></p>\n<ul>\n<li><a href=\"#chapter-1-an-introduction-to-vision-science\">Chapter 1: An Introduction to Vision Science</a></li>\n<li><a href=\"#chapter-2-theoretical-approaches-to-vision\">Chapter 2: Theoretical Approaches to Vision</a></li>\n<li><a href=\"#chapter-3-color-vision-a-microcosm-of-vision-science\">Chapter 3: Color Vision: A Microcosm of Vision Science</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#part-ii-spatial-vision\">Part II: Spatial Vision</a></p>\n<ul>\n<li><a href=\"#chapter-4-processing-image-structure\">Chapter 4: Processing Image Structure</a></li>\n<li><a href=\"#chapter-5-perceiving-surfaces-oriented-in-depth\">Chapter 5: Perceiving Surfaces Oriented in Depth</a></li>\n<li><a href=\"#chapter-6-organizing-objects-and-scenes\">Chapter 6: Organizing Objects and Scenes</a></li>\n<li><a href=\"#chapter-7-perceiving-object-properties-and-parts\">Chapter 7: Perceiving Object Properties and Parts</a></li>\n<li><a href=\"#chapter-8-representing-shapes-and-structure\">Chapter 8: Representing Shapes and Structure</a></li>\n<li><a href=\"#chapter-9-perceiving-function-and-category\">Chapter 9: Perceiving Function and Category</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#part-iii-visual-dynamics\">Part III: Visual Dynamics</a></p>\n<ul>\n<li><a href=\"#chapter-10-perceiving-motion-and-events\">Chapter 10: Perceiving Motion and Events</a></li>\n<li><a href=\"#chapter-11-visual-selection-eye-movements-and-attention\">Chapter 11: Visual Selection: Eye Movements and Attention</a></li>\n<li><a href=\"#chapter-12-visual-memory-and-imagery\">Chapter 12: Visual Memory and Imagery</a></li>\n<li><a href=\"#chapter-13-visual-awareness\">Chapter 13: Visual Awareness</a></li>\n</ul>\n</li>\n</ul>","timeToRead":131,"frontmatter":{"title":"Vision Science: Photons to Phenomenology","date":"November 27, 2022","book_authors":"Stephen E. Palmer","categories":["Textbooks"]}}},"pageContext":{"slug":"/textbooks/vision-science-photons-to-phenomenology/","previous":{"fields":{"slug":"/papers/msc-neuroscience-notes/"},"frontmatter":{"title":"MSc Neuroscience Paper Notes","layout":"post"}},"next":{"fields":{"slug":"/textbooks/the-cambridge-handbook-of-expertise-and-expert-performance/"},"frontmatter":{"title":"The Cambridge Handbook of Expertise and Expert Performance","layout":"post"}}}},"staticQueryHashes":["1031357278","1878747374","3666494887"],"slicesMap":{}}