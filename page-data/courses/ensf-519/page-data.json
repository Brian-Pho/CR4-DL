{"componentChunkName":"component---src-templates-post-template-tsx","path":"/courses/ensf-519/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"l1---l7-python-pandas-numpy-introduction\" style=\"position:relative;\"><a href=\"#l1---l7-python-pandas-numpy-introduction\" aria-label=\"l1   l7 python pandas numpy introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L1 - L7: Python, Pandas, Numpy Introduction</h2>\n<ul>\n<li>No notes because it was review or uninteresting</li>\n</ul>\n<h2 id=\"l8-introduction-to-ml\" style=\"position:relative;\"><a href=\"#l8-introduction-to-ml\" aria-label=\"l8 introduction to ml permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L8: Introduction to ML</h2>\n<ul>\n<li>k-Nearest Neighbors\n<ul>\n<li>Classifies point based on “k” nearest neighbors</li>\n<li>k is always an odd number to prevent ties</li>\n<li>Higher k = Largest distance for search</li>\n<li>Pros\n<ul>\n<li>Easy to understand</li>\n<li>Reasonable performance given simplicity</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>Poor performance on sparse datasets</li>\n<li>Can be slow on large datasets</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Generalization\n<ul>\n<li>Underfit: predictions are not specific enough</li>\n<li>Overfit: predictions are too closely modeled on the data</li>\n</ul>\n</li>\n<li>Classification vs Regression\n<ul>\n<li>Classification: discrete (type/class/label)</li>\n<li>Regression: continuous (target/response/value)</li>\n</ul>\n</li>\n<li>k-Neighbors Regression\n<ul>\n<li>Average the k nearest neighbors to the input to predict its target</li>\n</ul>\n</li>\n<li>Evaluating regression\n<ul>\n<li>R^2 (Coefficient of determination)\n<ul>\n<li>R^2 = Explained variance / Total variance</li>\n<li>Between 0 and 1</li>\n<li>1 = Perfect prediction</li>\n<li>0 = Only predicts the mean of the data</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Linear Regression Models\n<ul>\n<li>Linear regression: fits line to data\n<ul>\n<li>Minimizes the mean square error</li>\n<li>Pros\n<ul>\n<li>Simple</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>Prone to underfitting</li>\n<li>No way to control model complexity because no parameters</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"l9-regression\" style=\"position:relative;\"><a href=\"#l9-regression\" aria-label=\"l9 regression permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L9: Regression</h2>\n<ul>\n<li>Ridge regression: linear regression but with weight penalty</li>\n<li>Regularization\n<ul>\n<li>Regularization: explicitly restricting a model to prevent overfitting</li>\n<li>Alpha parameter: controls the tradeoff between simplicity and performance of a model</li>\n<li>Tries to make the weight matrix close to zero</li>\n<li>Each feature should have as little effect on the out while still predicting well</li>\n<li>Regularization becomes less important as we have more data</li>\n<li>L1 Regularization (Lasso): penalizes the absolute size of the weight</li>\n<li>L2 Regularization (Ridge): penalizes the squared size of the weight</li>\n</ul>\n</li>\n<li>Linear Classifiers\n<ul>\n<li>Classification vs regression = decision boundary vs prediction values</li>\n<li>Logistic regression: predicts the probability of the input in a class (between 0 and 1)</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"l10-svm-and-clustering\" style=\"position:relative;\"><a href=\"#l10-svm-and-clustering\" aria-label=\"l10 svm and clustering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L10: SVM and Clustering</h2>\n<ul>\n<li>Linear Support Vector Machines (lSVC): decision boundary is the support vector</li>\n<li>One-vs-rest: binary relevance, build a binary classifier for each class</li>\n<li>Clustering\n<ul>\n<li>Clustering: grouping data into partitions</li>\n<li>Points in a cluster should be similar, points in a different cluster should be different</li>\n</ul>\n</li>\n<li>k-Means Clustering\n<ul>\n<li>Finds cluster centers that represent the data</li>\n<li>Steps</li>\n</ul>\n<ol>\n<li>Assign each point to the closest cluster center</li>\n<li>Update the cluster centers to the mean of the points assigned to it</li>\n</ol>\n<ul>\n<li>Pros\n<ul>\n<li>Fast</li>\n<li>Easy to understand and implement</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>Susceptible to falling into local minimums</li>\n<li>We don’t know how many cluster centers to give it</li>\n<li>Assumes all clusters have the same diameter</li>\n<li>Assumes all clusters are omni-directional</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Agglomerative Clustering\n<ul>\n<li>Assigns each point as its own cluster and merges clusters until the number of desired clusters is reached</li>\n<li>Merging function\n<ul>\n<li>Ward: minimize variance between clusters</li>\n<li>Average: smallest average distance between cluster’s points</li>\n<li>Complete: smallest maximum distance between cluster’s points</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"l11-clustering\" style=\"position:relative;\"><a href=\"#l11-clustering\" aria-label=\"l11 clustering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L11: Clustering</h2>\n<ul>\n<li>Density-Based Spatial Clustering of Applications with Noise (DBSCAN): identifies points in crowded/dense regions of the feature space\n<ul>\n<li>Core sample: points in a dense region</li>\n<li>If there are at least min_samples many data points within a distance of eps to a given data point, then that data point is classified as a core sample</li>\n<li>Algorithm</li>\n</ul>\n<ol>\n<li>Pick random sample</li>\n<li>Check if there are min number of samples within distance of eps</li>\n<li>If no, then label sample as noise</li>\n<li>If yes, then label sample as core sample and assigned a new cluster label</li>\n<li>All samples within the distance of the core sample are also given the cluster label</li>\n<li>If the samples are core samples, all neighbors are visited recursively</li>\n</ol>\n<ul>\n<li>Three kinds of points</li>\n</ul>\n<ol>\n<li>Noise</li>\n<li>Core samples</li>\n<li>Boundary points: samples within eps distance of core samples</li>\n</ol>\n<ul>\n<li>Increasing eps means more points in a cluster but fewer cluster</li>\n<li>Increasing min samples means more noise points but fewer core points</li>\n<li>Eps implicitly controls the number of clusters</li>\n<li>Min samples controls the minimum cluster size</li>\n</ul>\n</li>\n<li>Evaluating Clustering Algorithms\n<ul>\n<li>Without ground truth\n<ul>\n<li>Can’t use accuracy</li>\n<li>Silhouette score: computes how similar a point is to its own cluster compared to other clusters</li>\n</ul>\n</li>\n<li>With ground truth\n<ul>\n<li>Adjusted rand index (ARI): computes measure of agreement/disagreement between and within clusters</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"l12-preprocessing--dimensionality-reduction\" style=\"position:relative;\"><a href=\"#l12-preprocessing--dimensionality-reduction\" aria-label=\"l12 preprocessing  dimensionality reduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L12: Preprocessing &#x26; Dimensionality Reduction</h2>\n<ul>\n<li>Scaling\n<ul>\n<li>Standard: mean = 0 and variance = 1</li>\n<li>Robust: median = 0 and quartile = 1</li>\n<li>MinMax: values between 0 and 1</li>\n</ul>\n</li>\n<li>Normalizer: projects data onto a circle</li>\n<li>Fit the scaler on the training data, apply it to both the training and testing data</li>\n<li>Never fit the scaler on the testing data because it will perform an improper scaling and leaks data from the test set into the training set</li>\n<li>Dimensionality Reduction\n<ul>\n<li>Reduce the data dimension by compressing it</li>\n<li>Principle Component Analysis (PCA): decomposes dataset into dimensions that explain the maximum amount of variance\n<ul>\n<li>Eigenvector with the largest eigenvalue is the principle component</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"l13-textual-data\" style=\"position:relative;\"><a href=\"#l13-textual-data\" aria-label=\"l13 textual data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L13: Textual Data</h2>\n<ul>\n<li>One-hot encoding: associating a unique integer index to each unique word</li>\n<li>Corpus = Dataset</li>\n<li>Information Retrieval (IR)</li>\n<li>Natural Language Processing (NLP)</li>\n<li>Text Data as a Bag of Words\n<ul>\n<li>Discard document structure (paragraphs, chapters, sentences)</li>\n<li>Only count how often each word appears in each text in the corpus</li>\n</ul>\n</li>\n<li>Tokenization: splitting a document into words/tokens</li>\n<li>Vocabulary building: collect vocabulary of all words</li>\n<li>Encoding: counting how often a word appears</li>\n</ul>\n<h2 id=\"l14-tfidf\" style=\"position:relative;\"><a href=\"#l14-tfidf\" aria-label=\"l14 tfidf permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L14: TFIDF</h2>\n<ul>\n<li>Count vectorizer = tokenization</li>\n<li>Improve tokenization\n<ul>\n<li>Add lower bound on how many times a word appears</li>\n</ul>\n</li>\n<li>Normalization\n<ul>\n<li>Spelling</li>\n<li>Stemming</li>\n<li>Lemmatization</li>\n<li>Lower/Upper case</li>\n<li>Stop words: discarding words that appear to frequently to be informative (E.g. “a”, “the”, “i”)</li>\n</ul>\n</li>\n<li>Term frequency–inverse document frequency (TF–IDF): give high weight to any term that appears often in a particular document, but not in many documents\n<ul>\n<li>TF-IDF = TF * IDF</li>\n<li>Term frequency (TF): the number of times a word appears in a document</li>\n<li>Inverse document frequency (IDF): total documents / number of documents that the term appears in</li>\n<li>TF-IDF gives more weight to words the distinguish documents</li>\n<li>Low TF-IDF = words appears in many documents</li>\n<li>High TF-IDF = word appears in select documents often</li>\n</ul>\n</li>\n<li>n-Grams: overlapping sequence of words that preserves some order</li>\n<li>Latent Dirichlet Allocation (LDA): finds groups of words/topics</li>\n</ul>\n<p>that appear frequently together\n- Clustering algorithm\n- Each document is a “mixture” of a subset of the topics</p>\n<h2 id=\"l15-naive-bayes\" style=\"position:relative;\"><a href=\"#l15-naive-bayes\" aria-label=\"l15 naive bayes permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L15: Naive Bayes</h2>\n<ul>\n<li>Two types of classifiers\n<ul>\n<li>Discriminative: learn <code class=\"language-text\">p(y|X)</code>, learn output given input</li>\n<li>Generative: learn <code class=\"language-text\">p(y)</code>, learn <code class=\"language-text\">p(X|y)</code>, create input given class</li>\n<li>Discriminative learns the decision boundary</li>\n<li>Generative: learns the distribution of the input data</li>\n</ul>\n</li>\n<li>Naive Bayes\n<ul>\n<li>Generative classifier</li>\n<li>Uses class probability and data point given class probability to predict class given data point</li>\n<li>Bayes theorem used to flip conditional probability</li>\n<li>Different naive bayes classifiers depending on assumed data point given class probability distribution</li>\n<li>Pros\n<ul>\n<li>When assumptions match data</li>\n<li>Good for well-separated data, high dimensional data</li>\n<li>Fast and easily interpretable</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>Poor when a lot of features equals zero</li>\n<li>Correlation between attributes can not be captured</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"l16-decision-trees\" style=\"position:relative;\"><a href=\"#l16-decision-trees\" aria-label=\"l16 decision trees permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L16: Decision Trees</h2>\n<ul>\n<li>Decision tree (DT): a tree where each node is a decision</li>\n<li>Performs both classification and regression</li>\n<li>Questions are used to make a decision at each node</li>\n<li>Minimize impurity\n<ul>\n<li>Impurity: Gini-index and cross-entropy</li>\n</ul>\n</li>\n<li>DTs learn the decision boundary by recursively partitioning the space in a manner that maximizes the information gain</li>\n<li>Reducing overfitting\n<ul>\n<li>Limit maximum depth</li>\n<li>Limit number of leaf nodes</li>\n<li>Set min number of points needed to satisfy condition before splitting</li>\n</ul>\n</li>\n<li>Regression tree: DTs but for regression\n<ul>\n<li>Impurity: mean square error or mean absolute error</li>\n<li>Predict mean</li>\n</ul>\n</li>\n<li>Ensembles\n<ul>\n<li>Ensembles: combining different models to achieve better performance (Frankenstein models)</li>\n<li>Voting classifier: uses votes from each model to classify data\n<ul>\n<li>Hard vote: uses discrete votes</li>\n<li>Soft vote: uses probabilities</li>\n</ul>\n</li>\n<li>Averaging methods\n<ul>\n<li>Build several ensembles from random subsets of the original dataset and aggregate their predictions</li>\n<li>Bagging: random subsets of the samples with replacement</li>\n<li>Bootstrap Aggregation (Bagging): generic way to build different models</li>\n<li>Random forests: a collection of decision tree trained on different subsets of samples and features</li>\n</ul>\n</li>\n<li>Boosting methods\n<ul>\n<li>Boosting: refers to a family of algorithms which converts weak learner to strong learners</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"l17-model-evaluation\" style=\"position:relative;\"><a href=\"#l17-model-evaluation\" aria-label=\"l17 model evaluation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L17: Model Evaluation</h2>\n<ul>\n<li>Model evaluation\n<ul>\n<li>Cross validation\n<ul>\n<li>The data is split repeatedly and multiple models are trained</li>\n<li>k-fold cross-validation: split data into k partitions and train k times</li>\n</ul>\n</li>\n<li>Grid search\n<ul>\n<li>Trying all possible combinations of the parameters of interest</li>\n<li>Done before cross validation</li>\n</ul>\n</li>\n<li>Evaluation metrics\n<ul>\n<li>Accuracy: (TP + TN) / Total. Fraction of correctly classified examples</li>\n<li>Business metrics</li>\n<li>False positives/negatives</li>\n<li>Confusion matrices</li>\n<li>Precision: TP / (TP + FP). Used to limit false positives</li>\n<li>Recall: TP / (TP + FN). Used to limit false negatives</li>\n<li>f-measure: combination of precision and recall</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"l18-feature-engineering\" style=\"position:relative;\"><a href=\"#l18-feature-engineering\" aria-label=\"l18 feature engineering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>L18: Feature Engineering</h2>\n<ul>\n<li>Feature engineering\n<ul>\n<li>Use expert knowledge</li>\n<li>Domain expertise: specific knowledge in a domain</li>\n<li>Data representation\n<ul>\n<li>One-hot encoding: turn data into a vector where 1 represents its existence</li>\n<li>Binning: turn continuous data into discrete data by categorizing it</li>\n<li>Interactions: add the original feature to the binned data</li>\n<li>Polynomials: use polynomial regression</li>\n<li>Transformations: modify the data using some math function</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>","tableOfContents":"<ul>\n<li><a href=\"#l1---l7-python-pandas-numpy-introduction\">L1 - L7: Python, Pandas, Numpy Introduction</a></li>\n<li><a href=\"#l8-introduction-to-ml\">L8: Introduction to ML</a></li>\n<li><a href=\"#l9-regression\">L9: Regression</a></li>\n<li><a href=\"#l10-svm-and-clustering\">L10: SVM and Clustering</a></li>\n<li><a href=\"#l11-clustering\">L11: Clustering</a></li>\n<li><a href=\"#l12-preprocessing--dimensionality-reduction\">L12: Preprocessing &#x26; Dimensionality Reduction</a></li>\n<li><a href=\"#l13-textual-data\">L13: Textual Data</a></li>\n<li><a href=\"#l14-tfidf\">L14: TFIDF</a></li>\n<li><a href=\"#l15-naive-bayes\">L15: Naive Bayes</a></li>\n<li><a href=\"#l16-decision-trees\">L16: Decision Trees</a></li>\n<li><a href=\"#l17-model-evaluation\">L17: Model Evaluation</a></li>\n<li><a href=\"#l18-feature-engineering\">L18: Feature Engineering</a></li>\n</ul>","timeToRead":5,"frontmatter":{"title":"ENSF 519: Applied Data Science","date":"September 01, 2019","book_authors":null,"categories":["Courses"]}}},"pageContext":{"slug":"/courses/ensf-519/","previous":{"fields":{"slug":"/courses/seng-511/"},"frontmatter":{"title":"SENG 511: Software Process and Project Management ","layout":"post"}},"next":{"fields":{"slug":"/courses/cpsc-413/"},"frontmatter":{"title":"CPSC 413: Design & Analysis of Algorithms I","layout":"post"}}}},"staticQueryHashes":["1031357278","1878747374","3666494887"],"slicesMap":{}}